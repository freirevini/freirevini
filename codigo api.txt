# -*- coding: utf-8 -*-
import os
import re
import json
import logging
import pathlib
import datetime as dt
from typing import List, Dict, Optional, Tuple

import win32com.client as win32
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

# =========================
# CONFIGURAÇÕES
# =========================
# Datas de TESTE
DATA_INICIO = "01/03/2025"
DATA_FIM    = "07/03/2025"
# PRODUÇÃO (use os últimos 3 dias):
# _hoje = dt.datetime.today()
# DATA_INICIO, DATA_FIM = (_hoje - dt.timedelta(days=3)).strftime("%d/%m/%Y"), _hoje.strftime("%d/%m/%Y")

REMETENTE_FILTRO = "informe@mkcompliance.com.br"
ASSUNTO_FIXO = "[EXT] - Normativos Divulgados em "
OUTLOOK_FOLDER_PATH = r"Caixa de Entrada\Normas MK Compliance"

PASTA_DESTINO = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Normativo_Download_Outlook"
ARQUIVO_EXCEL_EMAILS = os.path.join(PASTA_DESTINO, "Normativos_Email_Consolidado.xlsx")
ARQUIVO_EXCEL_METAS  = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\3_BaseMK\BaseMkNormasParaAvaliar.xlsx"

LOG_LEVEL = logging.INFO
HTTP_TIMEOUT = 45

logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")

# =========================
# HELPERS
# =========================
def ensure_dir(path: str | pathlib.Path) -> None:
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)

def N(txt: str) -> str:
    import unicodedata
    txt = str(txt or "")
    txt = "".join(c for c in unicodedata.normalize("NFKD", txt) if not unicodedata.combining(c))
    return re.sub(r"\s+", " ", txt.upper().strip())

def only_digits(s: str) -> str:
    return re.sub(r"\D", "", str(s or ""))

def parse_date_flexible(s: str) -> Optional[dt.date]:
    if not s:
        return None
    s = str(s).strip()
    try:
        return dtparser.parse(s, dayfirst=True).date()
    except Exception:
        pass
    m = re.search(r"(\d{1,2})\s+de\s+([a-zç\.]+)\s+de\s+(\d{4})", s, flags=re.IGNORECASE)
    if m:
        d = int(m.group(1))
        mes_raw = m.group(2).replace(".", "").lower()
        mapa = {
            "jan":1,"janeiro":1,"fev":2,"fevereiro":2,"mar":3,"marco":3,"março":3,"abr":4,"abril":4,
            "mai":5,"maio":5,"jun":6,"junho":6,"jul":7,"julho":7,"ago":8,"agosto":8,"set":9,"setembro":9,
            "out":10,"outubro":10,"nov":11,"novembro":11,"dez":12,"dezembro":12
        }
        mes = mapa.get(mes_raw, 0)
        y = int(m.group(3))
        if 1 <= int(mes) <= 12:
            return dt.date(y, int(mes), d)
    return None

def sanitize_filename(s: str) -> str:
    s = re.sub(r"[^\w\-. ]", "_", s, flags=re.UNICODE)
    s = re.sub(r"[_ ]{2,}", "_", s).strip("._ ")
    return s[:180]

def build_output_name(orgao: str, tipo: str, numero: str, data_ato: Optional[dt.date], fallback: Optional[str]) -> str:
    d = data_ato.strftime("%Y-%m-%d") if isinstance(data_ato, dt.date) else (fallback or dt.date.today().isoformat())
    base = f"{d}_{orgao}_{tipo}_{only_digits(numero)}.pdf"
    return sanitize_filename(base)

# =========================
# OUTLOOK
# =========================
def open_outlook_folder_by_path(path_str: str):
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    parts = [p for p in path_str.split("\\") if p.strip()]
    if not parts:
        return ns.GetDefaultFolder(6)  # Inbox

    root = None
    first = parts[0]
    for i in range(1, ns.Folders.Count + 1):
        store = ns.Folders.Item(i)
        if str(store.Name).strip().lower() == first.strip().lower():
            root = store
            parts = parts[1:]
            break

    if root is None:
        root = ns.GetDefaultFolder(6)
        if first.strip().lower() in ["inbox", "caixa de entrada"]:
            parts = parts[1:]

    folder = root
    for p in parts:
        folder = folder.Folders.Item(p)
    return folder

def restrict_by_date_and_subject(items, data_inicio_dt: dt.datetime, data_fim_dt: dt.datetime) -> list:
    di = data_inicio_dt.strftime("%m/%d/%Y")
    df = data_fim_dt.strftime("%m/%d/%Y")
    filtro_data = f"[ReceivedTime] >= '{di} 00:00 AM' AND [ReceivedTime] <= '{df} 11:59 PM'"
    candidates = list(items.Restrict(filtro_data))

    out = []
    for mail in candidates:
        try:
            subj = str(getattr(mail, "Subject", "") or "")
            from_addr = str(getattr(mail, "SenderEmailAddress", "") or "")
            if REMETENTE_FILTRO.lower() not in from_addr.lower():
                continue
            if not subj.startswith(ASSUNTO_FIXO):
                continue
            data_str = subj.replace(ASSUNTO_FIXO, "").strip()
            try:
                data_no_assunto = dt.datetime.strptime(data_str, "%d/%m/%Y")
            except Exception:
                continue
            if data_inicio_dt <= data_no_assunto <= data_fim_dt:
                out.append(mail)
        except Exception:
            continue
    return out

# =========================
# TABELA DO E-MAIL -> DATAFRAME CANÔNICO
# =========================
COLS_CANON = ["Tipo", "Número", "Data do Ato", "Órgão", "Ementa", "Vigência"]

def map_headers_to_canonic(headers_norm: list[str]) -> Dict[int, str]:
    wanted = {
        "Órgão": {N("Órgão"), N("Órgão regulador"), N("Orgao"), N("Orgao Regulador"), N("Orgão Regulador"), N("Orgão Responsável")},
        "Tipo": {N("Tipo"), N("Tipo da Norma"), N("Espécie"), N("Especie")},
        "Número": {N("Número"), N("Numero"), N("Nº"), N("No"), N("N."), N("N")},
        "Data do Ato": {N("Data do Ato"), N("Data do Normativo"), N("Data"), N("Publicação"), N("Data Publicação")},
        "Ementa": {N("Ementa"), N("Assunto"), N("Resumo"), N("Descrição"), N("Descricao")},
        "Vigência": {N("Vigência"), N("Vigencia")}
    }
    idx_to_key = {}
    for i, h in enumerate(headers_norm):
        for canon, variants in wanted.items():
            if h in variants:
                idx_to_key[i] = canon
                break
    return idx_to_key

FORWARD_MARKERS = re.compile(
    r"(mensagem encaminhada|encaminhada|enc:|fw:|forwarded message|-----+ original message -----+)",
    re.I
)

def _table_to_df_if_valid(table) -> Optional[pd.DataFrame]:
    trs = table.find_all("tr")
    if not trs:
        return None

    ths = table.find_all("th")
    if ths:
        headers = [N(th.get_text(" ", strip=True)) for th in ths]
        data_trs = [tr for tr in trs if tr.find_all("td")]
    else:
        first_tds = trs[0].find_all(["td", "th"])
        if not first_tds:
            return None
        headers = [N(td.get_text(" ", strip=True)) for td in first_tds]
        data_trs = trs[1:]

    idx_to_key = map_headers_to_canonic(headers)
    keys_present = set(idx_to_key.values())

    # Precisa pelo menos Número + (Tipo ou Órgão) e >= 2 linhas
    if not (("Número" in keys_present) and (("Tipo" in keys_present) or ("Órgão" in keys_present))):
        return None

    rows = []
    for tr in data_trs:
        tds = tr.find_all("td")
        if not tds:
            continue
        row = {k: "" for k in COLS_CANON}
        for idx, canon in idx_to_key.items():
            if idx < len(tds):
                row[canon] = tds[idx].get_text(" ", strip=True)
        if row["Número"]:
            rows.append(row)

    if len(rows) < 2:
        return None

    df = pd.DataFrame.from_records(rows, columns=COLS_CANON)
    # limpeza básica
    for c in COLS_CANON:
        df[c] = df[c].fillna("").astype(str).str.replace("\xa0", " ", regex=False).str.strip()

    # elimina linhas cujo "Número" não contenha dígitos (evita cabeçalhos “disfarçados”)
    df = df[df["Número"].map(lambda x: only_digits(x) != "")]
    if df.empty:
        return None

    return df.reset_index(drop=True)

def extract_df_from_email_html(html: str) -> pd.DataFrame:
    """
    Extrai SOMENTE a melhor tabela do HTML:
      - ignora tudo que estiver DEPOIS de marcadores de encaminhamento/rodapé;
      - escolhe por score: (colunas relevantes, %linhas com Número válido, #linhas).
    """
    if not html:
        return pd.DataFrame(columns=COLS_CANON)

    # Corta o HTML no primeiro marcador de encaminhamento para ignorar descrição/forward
    cut_html = html
    m = FORWARD_MARKERS.search(html)
    if m:
        cut_html = html[:m.start()]

    soup = BeautifulSoup(cut_html, "lxml")
    candidates: list[tuple[Tuple[int, float, int], pd.DataFrame]] = []

    for table in soup.find_all("table"):
        df = _table_to_df_if_valid(table)
        if df is None:
            continue
        # score: mais colunas relevantes + maior % de linhas com número + mais linhas
        cols_found = sum(c in df.columns and df[c].notna().any() for c in ["Tipo","Número","Data do Ato","Órgão"])
        valid_ratio = (df["Número"].map(lambda x: only_digits(x) != "").mean()) if len(df) else 0.0
        score = (cols_found, float(valid_ratio), len(df))
        candidates.append((score, df))

    if not candidates:
        return pd.DataFrame(columns=COLS_CANON)

    candidates.sort(key=lambda t: t[0], reverse=True)
    best_df = candidates[0][1].copy()
    return best_df[COLS_CANON].reset_index(drop=True)

def add_normalized_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out["N_TIPO"] = out["Tipo"].map(N)
    out["N_NUM"] = out["Número"].map(only_digits)
    out["N_ORGAO"] = out["Órgão"].map(N)
    out["DATA_ATO_DT"] = out["Data do Ato"].map(parse_date_flexible)
    return out

# =========================
# METAS
# =========================
def load_metas_df(path: str) -> pd.DataFrame:
    df = pd.read_excel(path)
    colmap = {N(c): c for c in df.columns}
    def find(*cands):
        for c in cands:
            if N(c) in colmap:
                return colmap[N(c)]
        return None

    c_org = find("Órgão regulador","Órgão","Orgao","Orgão Regulador","Orgao Regulador")
    c_tipo= find("Tipo","Tipo da Norma","Espécie","Especie")
    c_num = find("Número","Numero","Nº","No","N.","N")
    c_data= find("Data do Normativo","Data do Ato","Data","Publicação","Data Publicação")

    miss = [n for n,v in [("Órgão",c_org),("Tipo",c_tipo),("Número",c_num),("Data",c_data)] if v is None]
    if miss:
        raise RuntimeError(f"Colunas ausentes na planilha de metas: {miss}")

    metas = pd.DataFrame({
        "Órgão": df[c_org].astype(str),
        "Tipo":  df[c_tipo].astype(str),
        "Número":df[c_num].astype(str),
        "Data do Ato": df[c_data].astype(str),
    })
    metas = add_normalized_cols(metas)
    return metas

# =========================
# MATCHING
# =========================
def row_matches_target(email_row: pd.Series, meta_row: pd.Series) -> bool:
    if str(email_row["N_NUM"]) != str(meta_row["N_NUM"]):
        return False
    e_org = email_row["N_ORGAO"]; t_org = meta_row["N_ORGAO"]
    e_tipo= email_row["N_TIPO"];  t_tipo= meta_row["N_TIPO"]
    org_ok = any(alias in e_org for alias in [t_org, "BANCO CENTRAL", "BANCO CENTRAL DO BRASIL", "BCB", "BACEN", "CVM", "ANBIMA", "CMN"])
    if not org_ok:
        return False
    tipo_ok = (t_tipo in e_tipo) or (e_tipo in t_tipo) or (t_tipo.split()[0] in e_tipo if t_tipo else False)
    if not tipo_ok:
        return False
    return True

# =========================
# PDF
# =========================
def find_pdf_attachments(mail, number_digits: str) -> List[Tuple[str, object]]:
    found = []
    try:
        cnt = int(getattr(mail.Attachments, "Count", 0))
        for i in range(1, cnt + 1):
            att = mail.Attachments.Item(i)
            name = str(getattr(att, "FileName", "") or "")
            if name.lower().endswith(".pdf") and number_digits in N(name):
                found.append((name, att))
    except Exception as e:
        logging.warning(f"Erro ao ler anexos: {e}")
    return found

def find_pdf_links_in_html(html: str, number_digits: str) -> List[str]:
    links = []
    if not html:
        return links
    soup = BeautifulSoup(html, "lxml")
    for a in soup.find_all("a"):
        href = (a.get("href") or "").strip()
        if href.lower().endswith(".pdf") and number_digits in N(href):
            links.append(href)
    return links

def download_link(url: str, dest_path: str) -> bool:
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return False
        with open(dest_path, "wb") as f:
            f.write(r.content)
        return True
    except Exception as e:
        logging.warning(f"Falha no download {url}: {e}")
        return False

# =========================
# PIPELINE
# =========================
def main():
    ensure_dir(PASTA_DESTINO)

    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    folder = open_outlook_folder_by_path(OUTLOOK_FOLDER_PATH)
    items = folder.Items
    items.Sort("[ReceivedTime]", True)

    di = dt.datetime.strptime(DATA_INICIO, "%d/%m/%Y")
    df_ = dt.datetime.strptime(DATA_FIM, "%d/%m/%Y")
    mails = restrict_by_date_and_subject(items, di, df_)
    logging.info(f"E-mails elegíveis: {len(mails)}")

    email_rows_all: list[pd.DataFrame] = []
    mail_index: dict[str, object] = {}

    for mail in mails:
        entry_id = str(getattr(mail, "EntryID", ""))
        subj = str(getattr(mail, "Subject", "") or "")
        recd = getattr(mail, "ReceivedTime", None)
        html = str(getattr(mail, "HTMLBody", "") or "")

        df_email = extract_df_from_email_html(html)
        if df_email.empty:
            logging.info(f"[SKIP] Sem tabela reconhecida em: {subj}")
            continue

        df_email = add_normalized_cols(df_email)
        # DEDUPE dentro do próprio e-mail por (N_NUM, N_TIPO, N_ORGAO)
        df_email = df_email.drop_duplicates(subset=["N_NUM", "N_TIPO", "N_ORGAO"], keep="first").reset_index(drop=True)

        df_email["EMAIL_ENTRY_ID"] = entry_id
        df_email["EMAIL_SUBJECT"] = subj
        df_email["EMAIL_RECEIVED"] = recd.strftime("%Y-%m-%d %H:%M:%S") if isinstance(recd, dt.datetime) else ""

        email_rows_all.append(df_email)
        mail_index[entry_id] = mail
        logging.info(f"[OK] Tabela extraída: {subj}  (linhas: {len(df_email)})")

    if not email_rows_all:
        logging.warning("Nenhuma tabela de e-mail extraída. Encerrando.")
        return

    emails_df = pd.concat(email_rows_all, ignore_index=True)

    # Salva a consolidação (auditoria)
    try:
        emails_df.to_excel(ARQUIVO_EXCEL_EMAILS, index=False)
        logging.info(f"Consolidado salvo em: {ARQUIVO_EXCEL_EMAILS}")
    except Exception as e:
        logging.warning(f"Falha ao salvar Excel consolidado: {e}")

    # Metas
    metas_df = load_metas_df(ARQUIVO_EXCEL_METAS)
    logging.info(f"Metas carregadas: {len(metas_df)}")

    # Join por N_NUM
    base_join = emails_df.merge(
        metas_df,
        how="inner",
        left_on="N_NUM",
        right_on="N_NUM",
        suffixes=("_EMAIL", "_META")
    )
    if base_join.empty:
        logging.info("Nenhum número coincidente entre e-mails e metas.")
        return

    # Validação órgão/tipo
    valid_mask = []
    for _, row in base_join.iterrows():
        valid_mask.append(row_matches_target(
            email_row=row[["N_NUM","N_ORGAO_EMAIL","N_TIPO_EMAIL"]].rename(
                {"N_ORGAO_EMAIL":"N_ORGAO","N_TIPO_EMAIL":"N_TIPO"}
            ),
            meta_row=row[["N_NUM","N_ORGAO_META","N_TIPO_META"]].rename(
                {"N_ORGAO_META":"N_ORGAO","N_TIPO_META":"N_TIPO"}
            )
        ))
    matched = base_join[valid_mask].copy()
    if matched.empty:
        logging.info("Nenhum match validado por órgão/tipo.")
        return

    logging.info(f"Matches validados: {len(matched)}")

    # Download PDFs com dedupe por (entry_id, nnum) e por (orgao, tipo, numero)
    saved_keys = set()
    saved_by_triplet = set()
    results: list[dict] = []

    for _, r in matched.iterrows():
        entry_id = r["EMAIL_ENTRY_ID"]
        mail = mail_index.get(entry_id)
        if mail is None:
            continue

        nnum = str(r["N_NUM"])
        key_mail_num = (entry_id, nnum)
        triplet = (str(r["N_ORGAO_META"]), str(r["N_TIPO_META"]), nnum)

        # evita duplicidade dentro do mesmo e-mail e também entre e-mails
        if key_mail_num in saved_keys or triplet in saved_by_triplet:
            continue

        orgao_meta = str(r["Órgão_META"])
        tipo_meta  = str(r["Tipo_META"])
        numero_vis = str(r["Número_META"] or r["Número_EMAIL"])
        data_ato_dt = r.get("DATA_ATO_DT_META")
        if isinstance(data_ato_dt, pd.Timestamp):
            data_ato_dt = data_ato_dt.date()

        saved = []
        # anexos
        for name, att in find_pdf_attachments(mail, nnum):
            outname = build_output_name(orgao_meta, tipo_meta, numero_vis, data_ato_dt, None)
            dest = os.path.join(PASTA_DESTINO, outname)
            k = 1
            while os.path.exists(dest):
                dest = os.path.join(PASTA_DESTINO, sanitize_filename(outname.replace(".pdf", f"_{k}.pdf")))
                k += 1
            att.SaveAsFile(dest)
            saved.append(dest)

        # links
        if not saved:
            html = str(getattr(mail, "HTMLBody", "") or "")
            for url in find_pdf_links_in_html(html, nnum):
                outname = build_output_name(orgao_meta, tipo_meta, numero_vis, data_ato_dt, None)
                dest = os.path.join(PASTA_DESTINO, outname)
                k = 1
                while os.path.exists(dest):
                    dest = os.path.join(PASTA_DESTINO, sanitize_filename(outname.replace(".pdf", f"_{k}.pdf")))
                    k += 1
                if download_link(url, dest):
                    saved.append(dest)

        # marcou como salvo (mesmo que vazio, para não tentar outra vez neste run)
        saved_keys.add(key_mail_num)
        if saved:
            saved_by_triplet.add(triplet)

        results.append({
            "email_entry_id": entry_id,
            "email_subject": r["EMAIL_SUBJECT"],
            "email_received": r["EMAIL_RECEIVED"],
            "orgao_meta": orgao_meta,
            "tipo_meta": tipo_meta,
            "numero": nnum,
            "arquivos_salvos": ";".join(saved),
            "status": "BAIXADO" if saved else "NAO_ENCONTRADO_PDF"
        })

    # Log CSV
    log_path = os.path.join(PASTA_DESTINO, "log_execucao.csv")
    try:
        df_log = pd.DataFrame(results)
        if os.path.exists(log_path):
            df_exist = pd.read_csv(log_path, encoding="utf-8-sig")
            df_log = pd.concat([df_exist, df_log], ignore_index=True)
        df_log.to_csv(log_path, index=False, encoding="utf-8-sig")
        logging.info(f"Log salvo em: {log_path}")
    except Exception as e:
        logging.warning(f"Falha ao salvar log: {e}")

    logging.info("Processo concluído.")

# =========================
# RUN
# =========================
if __name__ == "__main__":
    main()
