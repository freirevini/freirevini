# -*- coding: utf-8 -*-
import os
import re
import json
import logging
import pathlib
import datetime as dt
from typing import List, Dict, Optional, Tuple

import win32com.client as win32
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

# =========================
# CONFIG
# =========================
# TESTE – ajuste o intervalo:
DATA_INICIO = "01/03/2025"
DATA_FIM    = "07/03/2025"

# PRODUÇÃO (últimos 3 dias):
# _hoje = dt.datetime.today()
# DATA_INICIO, DATA_FIM = (_hoje - dt.timedelta(days=3)).strftime("%d/%m/%Y"), _hoje.strftime("%d/%m/%Y")

REMETENTE_FILTRO = "informe@mkcompliance.com.br"
ASSUNTO_FIXO = "[EXT] - Normativos Divulgados em "
OUTLOOK_FOLDER_PATH = r"Caixa de Entrada\Normas MK Compliance"

PASTA_DESTINO = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Normativo_Download_Outlook"
ARQUIVO_EXCEL_METAS  = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\3_BaseMK\BaseMkNormasParaAvaliar.xlsx"
STATE_JSON           = os.path.join(PASTA_DESTINO, "state.json")
LOG_CSV              = os.path.join(PASTA_DESTINO, "log_execucao.csv")

# Excel FINAL (único) – mesmo caminho do arquivo de metas
EXCEL_FINAL = os.path.join(
    os.path.dirname(ARQUIVO_EXCEL_METAS),
    pathlib.Path(ARQUIVO_EXCEL_METAS).stem + "_com_PDF.xlsx"
)

HTTP_TIMEOUT = 45
LOG_LEVEL = logging.INFO
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")


# =========================
# HELPERS
# =========================
def ensure_dir(path: str | pathlib.Path) -> None:
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)

def N(txt: str) -> str:
    import unicodedata
    txt = str(txt or "")
    txt = "".join(c for c in unicodedata.normalize("NFKD", txt) if not unicodedata.combining(c))
    return re.sub(r"\s+", " ", txt.upper().strip())

def only_digits(s: str) -> str:
    return re.sub(r"\D", "", str(s or ""))

def parse_date_flexible(s: str) -> Optional[dt.date]:
    if not s:
        return None
    s = str(s).strip()
    try:
        return dtparser.parse(s, dayfirst=True).date()
    except Exception:
        pass
    m = re.search(r"(\d{1,2})\s+de\s+([a-zç\.]+)\s+de\s+(\d{4})", s, flags=re.IGNORECASE)
    if m:
        d = int(m.group(1))
        mes_raw = m.group(2).replace(".", "").lower()
        mapa = {
            "jan":1,"janeiro":1,"fev":2,"fevereiro":2,"mar":3,"marco":3,"março":3,"abr":4,"abril":4,
            "mai":5,"maio":5,"jun":6,"junho":6,"jul":7,"julho":7,"ago":8,"agosto":8,"set":9,"setembro":9,
            "out":10,"outubro":10,"nov":11,"novembro":11,"dez":12,"dezembro":12
        }
        mes = mapa.get(mes_raw, 0)
        y = int(m.group(3))
        if 1 <= int(mes) <= 12:
            return dt.date(y, int(mes), d)
    return None

def sanitize_filename(s: str) -> str:
    s = re.sub(r"[^\w\-. ]", "_", s, flags=re.UNICODE)
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"_+", "_", s)
    return s.strip("._")[:180]

def build_output_name_orgao_tipo_num(orgao: str, tipo: str, numero: str) -> str:
    base = f"{orgao}_{tipo}_{only_digits(numero)}"
    return sanitize_filename(base) + ".pdf"

def load_state() -> Dict:
    try:
        if os.path.exists(STATE_JSON):
            with open(STATE_JSON, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {}

def save_state(state: Dict) -> None:
    ensure_dir(os.path.dirname(STATE_JSON))
    with open(STATE_JSON, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def triplet_key(orgao: str, tipo: str, nnum: str) -> str:
    return f"{N(orgao)}|{N(tipo)}|{nnum}"

def state_has_triplet(state: Dict, orgao: str, tipo: str, nnum: str) -> bool:
    key = triplet_key(orgao, tipo, nnum)
    return key in state.setdefault("downloaded_triplets", [])

def state_add_triplet(state: Dict, orgao: str, tipo: str, nnum: str) -> None:
    key = triplet_key(orgao, tipo, nnum)
    arr = state.setdefault("downloaded_triplets", [])
    if key not in arr:
        arr.append(key)


# =========================
# ALIASES & ACRÔNIMOS
# =========================
ALIASES_ORGAO_RAW = {
    "BANCO NACIONAL DE DESENVOLVIMENTO ECONOMICO E SOCIAL": ["BNDES"],
    "MINISTERIO DAS CIDADES": ["MCID", "MINISTERIO DA CIDADE"],
    "MINISTERIO DA FAZENDA": ["MF", "FAZENDA"],
    "MINISTERIO DA ECONOMIA": ["ME", "ECONOMIA"],
    "BANCO CENTRAL DO BRASIL": ["BCB", "BACEN", "BANCO CENTRAL"],
    "CONSELHO MONETARIO NACIONAL": ["CMN"]
}
ALIASES_ORGAO = {N(k): [N(v) for v in vs] for k, vs in ALIASES_ORGAO_RAW.items()}
STOPWORDS_SIGLA = {"DE","DA","DO","DAS","DOS","E","DOU","DOU-SE"}

def acronym_norm(s: str) -> str:
    toks = [t for t in re.split(r"[^\w]+", N(s)) if t and t not in STOPWORDS_SIGLA]
    return "".join(t[0] for t in toks)[:8]

def org_aliases(org: str) -> List[str]:
    orgN = N(org)
    out = {orgN, acronym_norm(orgN)}
    for key, vals in ALIASES_ORGAO.items():
        if key in orgN or orgN in key:
            out.add(key)
            out.update(vals)
    return [a for a in out if a]

def org_match_loose(email_org: str, meta_org: str) -> bool:
    e, m = N(email_org), N(meta_org)
    if m in e or e in m:
        return True
    al = set(org_aliases(meta_org))
    al.add(acronym_norm(email_org))
    return any(a and a in e for a in al)

def tipo_main_token(tipo: str) -> str:
    t = N(tipo)
    return re.split(r"[^\w]+", t)[0] if t else ""


# =========================
# OUTLOOK
# =========================
def open_outlook_folder_by_path(path_str: str):
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    parts = [p for p in path_str.split("\\") if p.strip()]
    if not parts:
        return ns.GetDefaultFolder(6)  # Inbox

    root = None
    first = parts[0]
    for i in range(1, ns.Folders.Count + 1):
        store = ns.Folders.Item(i)
        if str(store.Name).strip().lower() == first.strip().lower():
            root = store
            parts = parts[1:]
            break

    if root is None:
        root = ns.GetDefaultFolder(6)
        if first.strip().lower() in ["inbox", "caixa de entrada"]:
            parts = parts[1:]

    folder = root
    for p in parts:
        folder = folder.Folders.Item(p)
    return folder

def restrict_by_date_and_subject(items, data_inicio_dt: dt.datetime, data_fim_dt: dt.datetime) -> list:
    di = data_inicio_dt.strftime("%m/%d/%Y")
    df = data_fim_dt.strftime("%m/%d/%Y")
    filtro_data = f"[ReceivedTime] >= '{di} 00:00 AM' AND [ReceivedTime] <= '{df} 11:59 PM'"
    candidates = list(items.Restrict(filtro_data))

    out = []
    for mail in candidates:
        try:
            subj = str(getattr(mail, "Subject", "") or "")
            from_addr = str(getattr(mail, "SenderEmailAddress", "") or "")
            if REMETENTE_FILTRO.lower() not in from_addr.lower():
                continue
            if not subj.startswith(ASSUNTO_FIXO):
                continue
            data_str = subj.replace(ASSUNTO_FIXO, "").strip()
            try:
                data_no_assunto = dt.datetime.strptime(data_str, "%d/%m/%Y")
            except Exception:
                continue
            if data_inicio_dt <= data_no_assunto <= data_fim_dt:
                out.append(mail)
        except Exception:
            continue
    return out


# =========================
# EMAIL HTML -> DATAFRAME CANÔNICO
# =========================
COLS_CANON = ["Tipo", "Número", "Data do Ato", "Órgão", "Ementa", "Vigência"]

HEADER_VARIANTS = {
    "Órgão": {N("Órgão"), N("Órgão regulador"), N("Orgao"), N("Orgao Regulador"), N("Orgão Regulador"),
              N("Órgão Responsável"), N("Orgão Responsável")},
    "Tipo": {N("Tipo"), N("Tipo da Norma"), N("Espécie"), N("Especie")},
    "Número": {N("Número"), N("Numero"), N("Nº"), N("No"), N("N."), N("N")},
    "Data do Ato": {N("Data do Ato"), N("Data do Normativo"), N("Data"), N("Publicação"), N("Data Publicação")},
    "Ementa": {N("Ementa"), N("Assunto"), N("Resumo"), N("Descrição"), N("Descricao")},
    "Vigência": {N("Vigência"), N("Vigencia")},
}

FORWARD_MARKERS = re.compile(
    r"(mensagem encaminhada|encaminhada|enc:|fw:|forwarded message|-----+ original message -----+)",
    re.I
)

def _row_cells(tr) -> list[str]:
    tds = tr.find_all(["td", "th"])
    return [td.get_text(" ", strip=True) for td in tds]

def _header_mapping(cells_norm: list[str]) -> Dict[int, str]:
    idx_to_key = {}
    for i, txt in enumerate(cells_norm):
        for canon, variants in HEADER_VARIANTS.items():
            if txt in variants and canon not in idx_to_key.values():
                idx_to_key[i] = canon
                break
    return idx_to_key

def _scan_table_for_header(table) -> Optional[Tuple[Dict[int, str], int, list]]:
    trs = table.find_all("tr")
    if not trs:
        return None
    best = None
    for idx, tr in enumerate(trs):
        cells = _row_cells(tr)
        if not cells:
            continue
        cells_norm = [N(c) for c in cells]
        mapping = _header_mapping(cells_norm)
        keys = set(mapping.values())
        if ("Número" in keys) and (("Tipo" in keys) or ("Órgão" in keys)) and len(keys) >= 4:
            score = (len(keys),)
            if (best is None) or (score > best[0]):
                best = (score, mapping, idx, trs)
    if best is None:
        return None
    _, mapping, idx, trs = best
    return mapping, idx, trs

def extract_df_from_email_html(html: str) -> pd.DataFrame:
    if not html:
        return pd.DataFrame(columns=COLS_CANON)

    cut_html = html
    m = FORWARD_MARKERS.search(html)
    if m:
        cut_html = html[:m.start()]

    soup = BeautifulSoup(cut_html, "lxml")
    candidates: list[tuple[Tuple[int, int], pd.DataFrame]] = []

    for table in soup.find_all("table"):
        header_scan = _scan_table_for_header(table)
        if not header_scan:
            continue
        idx_to_key, idx_header, trs = header_scan

        records = []
        for tr in trs[idx_header+1:]:
            cells = _row_cells(tr)
            if not cells:
                continue
            row = {k: "" for k in COLS_CANON}
            for i, canon in idx_to_key.items():
                if i < len(cells):
                    row[canon] = cells[i]
            if only_digits(row.get("Número", "")):
                records.append(row)

        if not records:
            continue

        df_tmp = pd.DataFrame.from_records(records, columns=COLS_CANON)
        for c in COLS_CANON:
            df_tmp[c] = df_tmp[c].fillna("").astype(str).str.replace("\xa0", " ", regex=False).str.strip()

        mask_headerish = df_tmp["Número"].map(lambda x: N(x) in HEADER_VARIANTS["Número"])
        df_tmp = df_tmp[~mask_headerish].copy()

        # Mantemos todas as linhas (mesmo Número pode existir para órgãos/tipos diferentes)
        useful_cols = sum(df_tmp[c].notna().any() for c in ["Tipo","Número","Data do Ato","Órgão"])
        score = (useful_cols, len(df_tmp))
        candidates.append((score, df_tmp))

    if not candidates:
        return pd.DataFrame(columns=COLS_CANON)

    candidates.sort(key=lambda t: t[0], reverse=True)
    best_df = candidates[0][1].copy()
    return best_df[COLS_CANON].reset_index(drop=True)

def add_normalized_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out["N_TIPO"] = out["Tipo"].map(N)
    out["N_NUM"] = out["Número"].map(only_digits)
    out["N_ORGAO"] = out["Órgão"].map(N)
    out["DATA_ATO_DT"] = out["Data do Ato"].map(parse_date_flexible)
    return out


# =========================
# METAS (EXCEL)
# =========================
def load_metas_df(path: str) -> pd.DataFrame:
    df = pd.read_excel(path, dtype=str).fillna("")
    # DEBUG opcional:
    # print("\n--- Preview METAS (bruto) ---"); print(df.head(10))

    expected_cols = ["Órgão regulador", "Tipo", "Número"]
    for col in expected_cols:
        if col not in df.columns:
            raise RuntimeError(f"Coluna ausente em metas: {col}")

    # Data do ato (opcional)
    data_col = ""
    if any(df.columns.str.lower().str.contains("data")):
        data_col = df.columns[df.columns.str.lower().str.contains("data")][0]

    metas = pd.DataFrame({
        "Órgão": df["Órgão regulador"].astype(str),
        "Tipo":   df["Tipo"].astype(str),
        "Número": df["Número"].astype(str),
        "Data do Ato": df[data_col].astype(str) if data_col else ""
    })

    metas = add_normalized_cols(metas)
    metas["PDF"] = ""      # será preenchido com 'SIM'
    metas["Caminho"] = ""  # caminho do arquivo salvo

    # DEBUG opcional:
    # print("\n--- Preview METAS (normalizado) ---"); print(metas.head(10))

    return metas


# =========================
# MATCHING
# =========================
def row_matches_target(email_row: pd.Series, meta_row: pd.Series) -> bool:
    if str(email_row["N_NUM"]) != str(meta_row["N_NUM"]):
        return False
    if not org_match_loose(email_row["N_ORGAO"], meta_row["N_ORGAO"]):
        return False
    e_tok, t_tok = tipo_main_token(email_row["N_TIPO"]), tipo_main_token(meta_row["N_TIPO"])
    return (t_tok == e_tok) or (t_tok and t_tok in email_row["N_TIPO"]) or (e_tok and e_tok in meta_row["N_TIPO"])


# =========================
# PDF DOWNLOAD
# =========================
def score_attachment_for_meta(filename: str, orgao: str, tipo: str, nnum: str, require_number: bool) -> Optional[int]:
    nameN = N(filename)
    has_num = nnum in nameN if nnum else False
    if require_number and not has_num:
        return None
    score = 0
    if has_num:
        score += 100
    first_tipo = tipo_main_token(tipo)
    if first_tipo and first_tipo in nameN:
        score += 20
    for alias in org_aliases(orgao):
        if alias and alias in nameN:
            score += 10
    if nnum and nnum in nameN:
        score += 5
    return score

def find_all_pdf_attachments(mail) -> List[Tuple[str, object]]:
    out = []
    try:
        cnt = int(getattr(mail, "Attachments").Count)
        for i in range(1, cnt + 1):
            att = mail.Attachments.Item(i)
            name = str(getattr(att, "FileName", "") or "")
            if name.lower().endswith(".pdf"):
                out.append((name, att))
    except Exception as e:
        logging.warning(f"Erro ao ler anexos: {e}")
    return out

def find_pdf_links_in_html(html: str, number_digits: str) -> List[str]:
    links = []
    if not html:
        return links
    soup = BeautifulSoup(html, "lxml")
    for a in soup.find_all("a"):
        href = (a.get("href") or "").strip()
        if href.lower().endswith(".pdf") and number_digits in N(href):
            links.append(href)
    return links

def download_link(url: str, dest_path: str) -> bool:
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return False
        with open(dest_path, "wb") as f:
            f.write(r.content)
        return True
    except Exception as e:
        logging.warning(f"Falha no download {url}: {e}")
        return False


# =========================
# PIPELINE
# =========================
def main():
    ensure_dir(PASTA_DESTINO)
    state = load_state()

    # 1) E-mails elegíveis
    folder = open_outlook_folder_by_path(OUTLOOK_FOLDER_PATH)
    items = folder.Items
    items.Sort("[ReceivedTime]", True)

    di = dt.datetime.strptime(DATA_INICIO, "%d/%m/%Y")
    df_ = dt.datetime.strptime(DATA_FIM, "%d/%m/%Y")
    mails = restrict_by_date_and_subject(items, di, df_)
    logging.info(f"E-mails elegíveis: {len(mails)}")

    email_dfs: List[pd.DataFrame] = []
    mail_index: Dict[str, object] = {}
    email_df_map: Dict[str, pd.DataFrame] = {}

    for mail in mails:
        entry_id = str(getattr(mail, "EntryID", "")) or ""
        subj = str(getattr(mail, "Subject", "") or "")
        recd = getattr(mail, "ReceivedTime", None)
        html = str(getattr(mail, "HTMLBody", "") or "")

        df_email = extract_df_from_email_html(html)
        if df_email.empty:
            logging.info(f"[SKIP] Sem tabela reconhecida em: {subj}")
            continue

        df_email = add_normalized_cols(df_email)
        # Garante que os campos de e-mail existam:
        df_email["EMAIL_ENTRY_ID"]  = entry_id
        df_email["EMAIL_SUBJECT"]   = subj
        df_email["EMAIL_RECEIVED"]  = recd.strftime("%Y-%m-%d %H:%M:%S") if isinstance(recd, dt.datetime) else ""

        email_dfs.append(df_email)
        email_df_map[entry_id] = df_email.copy()
        mail_index[entry_id] = mail

    if not email_dfs:
        logging.warning("Nenhuma tabela de e-mail extraída. Encerrando.")
        metas_empty = load_metas_df(ARQUIVO_EXCEL_METAS)
        metas_empty[["Órgão","Tipo","Número","PDF","Caminho"]].to_excel(EXCEL_FINAL, index=False)
        logging.info(f"Planilha gerada: {EXCEL_FINAL}")
        return

    emails_df_inmem = pd.concat(email_dfs, ignore_index=True)
    # DEBUG opcional:
    # print("\n--- Preview EMAILS (normalizado) ---"); print(emails_df_inmem.head(20))

    # 2) Metas (fonte da verdade)
    metas_df = load_metas_df(ARQUIVO_EXCEL_METAS)
    # DEBUG opcional:
    # print("\n--- Preview METAS (normalizado) ---"); print(metas_df.head(20))

    # 3) MATCH por número com validações auxiliares
    base_join = emails_df_inmem.merge(
        metas_df, how="inner", left_on="N_NUM", right_on="N_NUM", suffixes=("_EMAIL", "_META")
    )

    # Se por algum motivo as colunas de e-mail se sobrepuserem, garantimos que existam:
    for col in ["EMAIL_ENTRY_ID", "EMAIL_SUBJECT", "EMAIL_RECEIVED"]:
        if col not in base_join.columns and f"{col}_EMAIL" in base_join.columns:
            base_join[col] = base_join[f"{col}_EMAIL"]

    ok_mask = []
    for _, row in base_join.iterrows():
        ok_mask.append(
            row_matches_target(
                email_row=row[["N_NUM","N_ORGAO_EMAIL","N_TIPO_EMAIL"]].rename(
                    {"N_ORGAO_EMAIL":"N_ORGAO","N_TIPO_EMAIL":"N_TIPO"}
                ),
                meta_row=row[["N_NUM","N_ORGAO_META","N_TIPO_META"]].rename(
                    {"N_ORGAO_META":"N_ORGAO","N_TIPO_META":"N_TIPO"}
                )
            )
        )
    matched = base_join[ok_mask].copy()

    # 4) Download – evita duplicidade por (email, órgão, tipo, número)
    saved_mail_key = set()  # (EMAIL_ENTRY_ID, N(orgao), token(tipo), nnum)
    results = []

    iter_df = matched
    if "EMAIL_RECEIVED" in iter_df.columns:
        try:
            iter_df = iter_df.sort_values("EMAIL_RECEIVED")
        except Exception:
            pass  # se a coluna estiver com formato estranho, segue sem ordenar

    for _, r in iter_df.iterrows():
        entry_id = r.get("EMAIL_ENTRY_ID", "")
        mail = mail_index.get(entry_id)
        if mail is None:
            continue

        orgao = str(r.get("Órgão_META") or "")
        tipo = str(r.get("Tipo_META") or "")
        nnum = str(r.get("N_NUM") or "")
        numero_vis = str(r.get("Número_META") or nnum)

        dedup_key = (entry_id, N(orgao), tipo_main_token(tipo), nnum)
        if dedup_key in saved_mail_key:
            continue

        base_name = build_output_name_orgao_tipo_num(orgao, tipo, numero_vis)
        fs_hit = None
        try:
            fs_hit = file_already_present(PASTA_DESTINO, base_name)
        except Exception:
            fs_hit = None

        if state_has_triplet(state, orgao, tipo, nnum) or fs_hit:
            saved_mail_key.add(dedup_key)
            results.append({
                "email_entry_id": entry_id,
                "email_subject": r.get("EMAIL_SUBJECT",""),
                "email_received": r.get("EMAIL_RECEIVED",""),
                "orgao": orgao, "tipo": tipo, "numero": nnum,
                "arquivos_salvos": fs_hit or "",
                "status": "JA_EXISTIA"
            })
            continue

        all_atts = find_all_pdf_attachments(mail)

        # 1º: com número obrigatório
        scored = []
        for name, att in all_atts:
            sc = score_attachment_for_meta(name, orgao, tipo, nnum, require_number=True)
            if sc is not None:
                scored.append((sc, name, att))
        scored.sort(reverse=True)

        saved_paths = []
        if scored:
            _, name, att = scored[0]
            dest = os.path.join(PASTA_DESTINO, base_name)
            k = 1
            while os.path.exists(dest):
                dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                k += 1
            att.SaveAsFile(dest)
            saved_paths.append(dest)
        else:
            # 2º: sem número (casos especiais)
            scored2 = []
            for name, att in all_atts:
                sc = score_attachment_for_meta(name, orgao, tipo, nnum, require_number=False)
                if sc is not None:
                    scored2.append((sc, name, att))
            scored2.sort(reverse=True)
            if scored2:
                _, name, att = scored2[0]
                dest = os.path.join(PASTA_DESTINO, base_name)
                k = 1
                while os.path.exists(dest):
                    dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                    k += 1
                att.SaveAsFile(dest)
                saved_paths.append(dest)
            else:
                # 3º: link com número no corpo
                html = str(getattr(mail, "HTMLBody", "") or "")
                for url in find_pdf_links_in_html(html, nnum):
                    dest = os.path.join(PASTA_DESTINO, base_name)
                    k = 1
                    while os.path.exists(dest):
                        dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                        k += 1
                    if download_link(url, dest):
                        saved_paths.append(dest)
                        break

        saved_mail_key.add(dedup_key)
        if saved_paths:
            state_add_triplet(state, orgao, tipo, nnum)

        results.append({
            "email_entry_id": entry_id,
            "email_subject": r.get("EMAIL_SUBJECT",""),
            "email_received": r.get("EMAIL_RECEIVED",""),
            "orgao": orgao, "tipo": tipo, "numero": nnum,
            "arquivos_salvos": ";".join(saved_paths),
            "status": "BAIXADO" if saved_paths else "NAO_ENCONTRADO_PDF"
        })

    # 5) Atualiza METAS com PDF/Caminho
    metas_df = load_metas_df(ARQUIVO_EXCEL_METAS)  # recarrega limpo para escrever o final
    if results:
        df_results = pd.DataFrame(results)
        metas_df["__KEY__"] = metas_df.apply(lambda r: triplet_key(r["Órgão"], r["Tipo"], r["N_NUM"]), axis=1)
        df_results["__KEY__"] = df_results.apply(lambda r: triplet_key(r["orgao"], r["tipo"], r["numero"]), axis=1)

        agg = (df_results[df_results["status"].isin(["BAIXADO","JA_EXISTIA"])]
               .groupby("__KEY__")["arquivos_salvos"].apply(lambda s: ";".join([x for x in s if x])).to_dict())

        metas_df.loc[metas_df["__KEY__"].isin(agg.keys()), "PDF"] = "SIM"
        metas_df.loc[metas_df["__KEY__"].isin(agg.keys()), "Caminho"] = metas_df["__KEY__"].map(agg)

        metas_df.drop(columns=["__KEY__"], inplace=True, errors="ignore")

    # 6) Log CSV (permanece; apenas não geramos outros Excel)
    try:
        if results:
            df_log = pd.DataFrame(results)
            if os.path.exists(LOG_CSV):
                old = pd.read_csv(LOG_CSV, encoding="utf-8-sig")
                df_log = pd.concat([old, df_log], ignore_index=True)
            if not df_log.empty:
                df_log.drop_duplicates(subset=["email_entry_id","orgao","tipo","numero","arquivos_salvos"], keep="last", inplace=True)
            df_log.to_csv(LOG_CSV, index=False, encoding="utf-8-sig")
            logging.info(f"Log salvo em: {LOG_CSV}")
    except Exception as e:
        logging.warning(f"Falha ao salvar log: {e}")

    save_state(state)

    # 7) Gera APENAS o Excel final (metas + PDF + Caminho)
    try:
        metas_df[["Órgão","Tipo","Número","PDF","Caminho"]].to_excel(EXCEL_FINAL, index=False)
        logging.info(f"Planilha final gerada: {EXCEL_FINAL}")
    except Exception as e:
        logging.error(f"Falha ao salvar Excel final: {e}")

    logging.info("Processo concluído.")


# =========================
# RUN
# =========================
if __name__ == "__main__":
    main()
