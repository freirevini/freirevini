import os
import re
import json
import logging
import pathlib
import datetime as dt
from typing import List, Dict, Optional, Tuple

import win32com.client as win32
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

# =========================
# CONFIGURAÇÕES
# =========================
# --> Datas de TESTE (edite livremente)
DATA_INICIO = "01/03/2025"
DATA_FIM    = "07/03/2025"

# --> PRODUÇÃO: comente as duas linhas acima e descomente a linha abaixo
# _hoje = dt.datetime.today()
# DATA_INICIO, DATA_FIM = (_hoje - dt.timedelta(days=3)).strftime("%d/%m/%Y"), _hoje.strftime("%d/%m/%Y")

REMETENTE_FILTRO = "informe@mkcompliance.com.br"
ASSUNTO_FIXO = "[EXT] - Normativos Divulgados em "  # assunto termina com dd/mm/yyyy
OUTLOOK_FOLDER_PATH = r"Caixa de Entrada\Normas MK Compliance"  # use Inbox\... caso seu Outlook esteja em inglês

# Caminhos (ajuste conforme necessário)
PASTA_DESTINO = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Normativo_Download_Outlook"
ARQUIVO_EXCEL_EMAILS = os.path.join(PASTA_DESTINO, "Normativos_Email_Consolidado.xlsx")
ARQUIVO_EXCEL_METAS  = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\3_BaseMK\BaseMkNormasParaAvaliar.xlsx"

# Log
LOG_LEVEL = logging.INFO
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")

HTTP_TIMEOUT = 45

# =========================
# HELPERS GERAIS
# =========================
def ensure_dir(path: str | pathlib.Path) -> None:
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)

def N(txt: str) -> str:
    """Normaliza: remove acentos, upper, colapsa espaços."""
    import unicodedata
    txt = str(txt or "")
    txt = "".join(c for c in unicodedata.normalize("NFKD", txt) if not unicodedata.combining(c))
    return re.sub(r"\s+", " ", txt.upper().strip())

def only_digits(s: str) -> str:
    return re.sub(r"\D", "", str(s or ""))

def parse_date_flexible(s: str) -> Optional[dt.date]:
    if not s:
        return None
    s = str(s).strip()
    try:
        return dtparser.parse(s, dayfirst=True).date()
    except Exception:
        pass
    m = re.search(r"(\d{1,2})\s+de\s+([a-zç\.]+)\s+de\s+(\d{4})", s, flags=re.IGNORECASE)
    if m:
        d = int(m.group(1))
        mes_raw = m.group(2).replace(".", "").lower()
        mapa = {
            "jan":1,"janeiro":1,"fev":2,"fevereiro":2,"mar":3,"marco":3,"março":3,"abr":4,"abril":4,
            "mai":5,"maio":5,"jun":6,"junho":6,"jul":7,"julho":7,"ago":8,"agosto":8,"set":9,"setembro":9,
            "out":10,"outubro":10,"nov":11,"novembro":11,"dez":12,"dezembro":12
        }
        mes = mapa.get(mes_raw, 0)
        y = int(m.group(3))
        if 1 <= int(mes) <= 12:
            return dt.date(y, int(mes), d)
    return None

def sanitize_filename(s: str) -> str:
    s = re.sub(r"[^\w\-. ]", "_", s, flags=re.UNICODE)
    s = re.sub(r"[_ ]{2,}", "_", s).strip("._ ")
    return s[:180]

def build_output_name(orgao: str, tipo: str, numero: str, data_ato: Optional[dt.date], fallback: Optional[str]) -> str:
    d = data_ato.strftime("%Y-%m-%d") if isinstance(data_ato, dt.date) else (fallback or dt.date.today().isoformat())
    base = f"{d}_{orgao}_{tipo}_{only_digits(numero)}.pdf"
    return sanitize_filename(base)

# =========================
# OUTLOOK
# =========================
def open_outlook_folder_by_path(path_str: str):
    """Abre uma pasta do Outlook a partir de um caminho com '\'. Suporta mailbox compartilhado."""
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    parts = [p for p in path_str.split("\\") if p.strip()]
    if not parts:
        return ns.GetDefaultFolder(6)  # Inbox

    # tenta identificar mailbox como primeiro nó
    root = None
    first = parts[0]
    for i in range(1, ns.Folders.Count + 1):
        store = ns.Folders.Item(i)
        if str(store.Name).strip().lower() == first.strip().lower():
            root = store
            parts = parts[1:]
            break

    if root is None:
        root = ns.GetDefaultFolder(6)
        if first.strip().lower() in ["inbox", "caixa de entrada"]:
            parts = parts[1:]

    folder = root
    for p in parts:
        folder = folder.Folders.Item(p)
    return folder

def restrict_by_date_and_subject(items, data_inicio_dt: dt.datetime, data_fim_dt: dt.datetime) -> list:
    """Usa Restrict por ReceivedTime + validação de assunto prefixado com data."""
    # Filtro MAPI por data
    di = data_inicio_dt.strftime("%m/%d/%Y")
    df = data_fim_dt.strftime("%m/%d/%Y")
    filtro_data = f"[ReceivedTime] >= '{di} 00:00 AM' AND [ReceivedTime] <= '{df} 11:59 PM'"
    candidates = list(items.Restrict(filtro_data))

    # Pós-filtro: remetente + assunto com o padrão + data no intervalo
    out = []
    for mail in candidates:
        try:
            subj = str(getattr(mail, "Subject", "") or "")
            from_addr = str(getattr(mail, "SenderEmailAddress", "") or "")
            if REMETENTE_FILTRO.lower() not in from_addr.lower():
                continue
            if not subj.startswith(ASSUNTO_FIXO):
                continue
            data_str = subj.replace(ASSUNTO_FIXO, "").strip()
            try:
                data_no_assunto = dt.datetime.strptime(data_str, "%d/%m/%Y")
            except Exception:
                continue
            if data_inicio_dt <= data_no_assunto <= data_fim_dt:
                out.append(mail)
        except Exception:
            continue
    return out

# =========================
# TABELA DO E-MAIL -> DATAFRAME CANÔNICO
# =========================
COLS_CANON = ["Tipo", "Número", "Data do Ato", "Órgão", "Ementa", "Vigência"]

def map_headers_to_canonic(headers_norm: list[str]) -> Dict[int, str]:
    """Mapeia cabeçalhos normalizados para nomes canônicos."""
    wanted = {
        "Órgão": {N("Órgão"), N("Órgão regulador"), N("Orgao"), N("Orgao Regulador"), N("Orgão Regulador"), N("Orgão Responsável")},
        "Tipo": {N("Tipo"), N("Tipo da Norma"), N("Espécie"), N("Especie")},
        "Número": {N("Número"), N("Numero"), N("Nº"), N("No"), N("N."), N("N")},
        "Data do Ato": {N("Data do Ato"), N("Data do Normativo"), N("Data"), N("Publicação"), N("Data Publicação")},
        "Ementa": {N("Ementa"), N("Assunto"), N("Resumo"), N("Descrição"), N("Descricao")},
        "Vigência": {N("Vigência"), N("Vigencia")}
    }
    idx_to_key = {}
    for i, h in enumerate(headers_norm):
        for canon, variants in wanted.items():
            if h in variants:
                idx_to_key[i] = canon
                break
    return idx_to_key

def extract_df_from_email_html(html: str) -> pd.DataFrame:
    """
    Extrai a melhor tabela do HTML e a entrega já padronizada:
    colunas: Tipo, Número, Data do Ato, Órgão, Ementa, Vigência
    """
    if not html:
        return pd.DataFrame(columns=COLS_CANON)

    soup = BeautifulSoup(html, "lxml")
    candidates: list[tuple[tuple[int,int], pd.DataFrame]] = []

    for table in soup.find_all("table"):
        trs = table.find_all("tr")
        if not trs:
            continue

        # 1) Se houver <th>, use-os como cabeçalho; senão, 1ª linha é header
        ths = table.find_all("th")
        if ths:
            headers = [N(th.get_text(" ", strip=True)) for th in ths]
            data_trs = [tr for tr in trs if tr.find_all("td")]
        else:
            first_tds = trs[0].find_all(["td", "th"])
            if not first_tds:
                continue
            headers = [N(td.get_text(" ", strip=True)) for td in first_tds]
            data_trs = trs[1:]

        idx_to_key = map_headers_to_canonic(headers)
        # Precisa pelo menos Número + (Tipo ou Órgão)
        keys_present = set(idx_to_key.values())
        if not (("Número" in keys_present) and (("Tipo" in keys_present) or ("Órgão" in keys_present))):
            continue

        # Constrói linhas
        records = []
        for tr in data_trs:
            tds = tr.find_all("td")
            if not tds:
                continue
            row = {k: "" for k in COLS_CANON}
            for idx, canon in idx_to_key.items():
                if idx < len(tds):
                    row[canon] = tds[idx].get_text(" ", strip=True)
            # exige Número preenchido
            if row["Número"]:
                records.append(row)

        if not records:
            continue

        df_tmp = pd.DataFrame.from_records(records, columns=COLS_CANON)
        score = (len(keys_present & set(COLS_CANON)), len(df_tmp))
        candidates.append((score, df_tmp))

    if not candidates:
        return pd.DataFrame(columns=COLS_CANON)

    # escolhe a melhor tabela
    candidates.sort(key=lambda t: t[0], reverse=True)
    best_df = candidates[0][1].copy()

    # limpa espaços/NaN
    for c in COLS_CANON:
        if c in best_df.columns:
            best_df[c] = best_df[c].fillna("").astype(str).str.strip()
        else:
            best_df[c] = ""

    return best_df[COLS_CANON]

def add_normalized_cols(df: pd.DataFrame) -> pd.DataFrame:
    """Adiciona colunas de apoio para matching."""
    out = df.copy()
    out["N_TIPO"] = out["Tipo"].map(N)
    out["N_NUM"] = out["Número"].map(only_digits)
    out["N_ORGAO"] = out["Órgão"].map(N)
    out["DATA_ATO_DT"] = out["Data do Ato"].map(parse_date_flexible)
    return out

# =========================
# METAS (EXCEL) -> DATAFRAME NORMALIZADO
# =========================
def load_metas_df(path: str) -> pd.DataFrame:
    df = pd.read_excel(path)
    # mapear nomes prováveis
    colmap = {N(c): c for c in df.columns}
    def find(*cands):
        for c in cands:
            if N(c) in colmap:
                return colmap[N(c)]
        return None

    c_org = find("Órgão regulador","Órgão","Orgao","Orgão Regulador","Orgao Regulador")
    c_tipo= find("Tipo","Tipo da Norma","Espécie","Especie")
    c_num = find("Número","Numero","Nº","No","N.","N")
    c_data= find("Data do Normativo","Data do Ato","Data","Publicação","Data Publicação")

    miss = [n for n,v in [("Órgão",c_org),("Tipo",c_tipo),("Número",c_num),("Data",c_data)] if v is None]
    if miss:
        raise RuntimeError(f"Colunas ausentes na planilha de metas: {miss}")

    metas = pd.DataFrame({
        "Órgão": df[c_org].astype(str),
        "Tipo":  df[c_tipo].astype(str),
        "Número":df[c_num].astype(str),
        "Data do Ato": df[c_data].astype(str),
    })
    metas = add_normalized_cols(metas)
    return metas

# =========================
# MATCHING
# =========================
def row_matches_target(email_row: pd.Series, meta_row: pd.Series) -> bool:
    """Regra: número igual (dígitos) + órgão/tipo com tolerância."""
    if str(email_row["N_NUM"]) != str(meta_row["N_NUM"]):
        return False

    e_org = email_row["N_ORGAO"]; t_org = meta_row["N_ORGAO"]
    e_tipo= email_row["N_TIPO"];  t_tipo= meta_row["N_TIPO"]

    org_ok = any(alias in e_org for alias in [t_org, "BANCO CENTRAL", "BANCO CENTRAL DO BRASIL", "BCB", "BACEN", "CVM", "ANBIMA", "CMN"])
    if not org_ok:
        return False

    tipo_ok = (t_tipo in e_tipo) or (e_tipo in t_tipo) or (t_tipo.split()[0] in e_tipo if t_tipo else False)
    if not tipo_ok:
        return False

    return True

# =========================
# PDF (ANEXOS/LINKS) – DOWNLOAD
# =========================
def find_pdf_attachments(mail, number_digits: str, tipo_norm: str) -> List[Tuple[str, object]]:
    found = []
    try:
        cnt = int(getattr(mail.Attachments, "Count", 0))
        for i in range(1, cnt + 1):
            att = mail.Attachments.Item(i)
            name = str(getattr(att, "FileName", "") or "")
            if not name.lower().endswith(".pdf"):
                continue
            if number_digits in N(name):
                found.append((name, att))
    except Exception as e:
        logging.warning(f"Erro ao ler anexos: {e}")
    return found

def find_pdf_links_in_html(html: str, number_digits: str) -> List[str]:
    links = []
    if not html:
        return links
    soup = BeautifulSoup(html, "lxml")
    for a in soup.find_all("a"):
        href = (a.get("href") or "").strip()
        if href.lower().endswith(".pdf") and number_digits in N(href):
            links.append(href)
    return links

def download_link(url: str, dest_path: str) -> bool:
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return False
        with open(dest_path, "wb") as f:
            f.write(r.content)
        return True
    except Exception as e:
        logging.warning(f"Falha no download {url}: {e}")
        return False

# =========================
# PIPELINE
# =========================
def main():
    ensure_dir(PASTA_DESTINO)

    # 0) Conectar no Outlook e abrir pasta
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    folder = open_outlook_folder_by_path(OUTLOOK_FOLDER_PATH)
    items = folder.Items
    items.Sort("[ReceivedTime]", True)

    # 1) Filtrar e-mails por data/remetente/assunto
    di = dt.datetime.strptime(DATA_INICIO, "%d/%m/%Y")
    df = dt.datetime.strptime(DATA_FIM, "%d/%m/%Y")
    mails = restrict_by_date_and_subject(items, di, df)
    logging.info(f"E-mails elegíveis na pasta '{OUTLOOK_FOLDER_PATH}': {len(mails)}")

    # 2) Extrair tabelas dos e-mails e consolidar em DF canônico
    email_rows_all: list[pd.DataFrame] = []
    mail_index: dict[str, object] = {}  # EntryID -> mail (para baixar anexos depois)

    for mail in mails:
        entry_id = str(getattr(mail, "EntryID", ""))
        subj = str(getattr(mail, "Subject", "") or "")
        recd = getattr(mail, "ReceivedTime", None)
        html = str(getattr(mail, "HTMLBody", "") or "")

        df_email = extract_df_from_email_html(html)
        if df_email.empty:
            logging.info(f"[SKIP] Sem tabela reconhecida em: {subj}")
            continue

        # normaliza e anota metadados do e-mail
        df_email = add_normalized_cols(df_email)
        df_email["EMAIL_ENTRY_ID"] = entry_id
        df_email["EMAIL_SUBJECT"] = subj
        df_email["EMAIL_RECEIVED"] = recd.strftime("%Y-%m-%d %H:%M:%S") if isinstance(recd, dt.datetime) else ""

        email_rows_all.append(df_email)
        mail_index[entry_id] = mail
        logging.info(f"[OK] Tabela extraída do e-mail: {subj}  (linhas: {len(df_email)})")

    if not email_rows_all:
        logging.warning("Nenhuma tabela de e-mail extraída. Encerrando.")
        return

    emails_df = pd.concat(email_rows_all, ignore_index=True)
    # Salva uma cópia da consolidação (útil para auditoria/depuração)
    try:
        emails_df.to_excel(ARQUIVO_EXCEL_EMAILS, index=False)
        logging.info(f"Planilha consolidada dos e-mails salva em: {ARQUIVO_EXCEL_EMAILS}")
    except Exception as e:
        logging.warning(f"Falha ao salvar Excel consolidado: {e}")

    # 3) Carrega metas e normaliza
    metas_df = load_metas_df(ARQUIVO_EXCEL_METAS)
    logging.info(f"Metas carregadas: {len(metas_df)} linhas")

    # 4) Match por número (inner join) e depois checagem fina por órgão/tipo
    base_join = emails_df.merge(
        metas_df,
        how="inner",
        left_on="N_NUM",
        right_on="N_NUM",
        suffixes=("_EMAIL", "_META")
    )

    if base_join.empty:
        logging.info("Nenhum número coincidente entre e-mails e metas.")
        return

    # aplica a validação de órgão/tipo
    mask = []
    for _, row in base_join.iterrows():
        mask.append(row_matches_target(
            email_row=row[["N_NUM","N_ORGAO_EMAIL","N_TIPO_EMAIL"]].rename({
                "N_ORGAO_EMAIL":"N_ORGAO","N_TIPO_EMAIL":"N_TIPO"
            }),
            meta_row=row[["N_NUM","N_ORGAO_META","N_TIPO_META"]].rename({
                "N_ORGAO_META":"N_ORGAO","N_TIPO_META":"N_TIPO"
            })
        ))
    base_join["MATCH_OK"] = mask
    matched = base_join[base_join["MATCH_OK"]].copy()

    if matched.empty:
        logging.info("Nenhum match validado por órgão/tipo.")
        return

    logging.info(f"Matches validados: {len(matched)}")

    # 5) Download dos PDFs corretos por e-mail/numero
    ensure_dir(PASTA_DESTINO)
    results: list[dict] = []

    for _, r in matched.iterrows():
        entry_id = r["EMAIL_ENTRY_ID"]
        mail = mail_index.get(entry_id)
        if mail is None:
            continue

        number_digits = str(r["N_NUM"])
        tipo_meta = str(r["Tipo_META"])
        orgao_meta = str(r["Órgão_META"])
        data_ato = r.get("DATA_ATO_DT_META") if "DATA_ATO_DT_META" in r else parse_date_flexible(r.get("Data do Ato_META", ""))

        # anexos primeiro
        saved = []
        atts = find_pdf_attachments(mail, number_digits, tipo_meta)
        if atts:
            for name, att in atts:
                outname = build_output_name(orgao_meta, tipo_meta, number_digits, data_ato, None)
                dest = os.path.join(PASTA_DESTINO, outname)
                k = 1
                while os.path.exists(dest):
                    dest = os.path.join(PASTA_DESTINO, sanitize_filename(outname.replace(".pdf", f"_{k}.pdf")))
                    k += 1
                att.SaveAsFile(dest)
                saved.append(dest)

        # se não houver anexo, tenta links .pdf no corpo
        if not saved:
            html = str(getattr(mail, "HTMLBody", "") or "")
            links = find_pdf_links_in_html(html, number_digits)
            for url in links:
                outname = build_output_name(orgao_meta, tipo_meta, number_digits, data_ato, None)
                dest = os.path.join(PASTA_DESTINO, outname)
                k = 1
                while os.path.exists(dest):
                    dest = os.path.join(PASTA_DESTINO, sanitize_filename(outname.replace(".pdf", f"_{k}.pdf")))
                    k += 1
                if download_link(url, dest):
                    saved.append(dest)

        results.append({
            "email_entry_id": entry_id,
            "email_subject": r["EMAIL_SUBJECT"],
            "email_received": r["EMAIL_RECEIVED"],
            "orgao_meta": orgao_meta,
            "tipo_meta": tipo_meta,
            "numero": number_digits,
            "arquivos_salvos": ";".join(saved),
            "status": "BAIXADO" if saved else "NAO_ENCONTRADO_PDF"
        })

    # 6) Log final (CSV ao lado do consolidado)
    log_path = os.path.join(PASTA_DESTINO, "log_execucao.csv")
    try:
        df_log = pd.DataFrame(results)
        if os.path.exists(log_path):
            df_exist = pd.read_csv(log_path, encoding="utf-8-sig")
            df_log = pd.concat([df_exist, df_log], ignore_index=True)
        df_log.to_csv(log_path, index=False, encoding="utf-8-sig")
        logging.info(f"Log salvo em: {log_path}")
    except Exception as e:
        logging.warning(f"Falha ao salvar log: {e}")

    logging.info("Processo concluído.")

# =========================
# RUN
# =========================
if __name__ == "__main__":
    main()
