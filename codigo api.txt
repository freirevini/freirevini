# -*- coding: utf-8 -*-
import os
import re
import json
import logging
import pathlib
import datetime as dt
from typing import List, Dict, Optional, Tuple

import win32com.client as win32
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

# =========================
# CONFIG
# =========================
# TESTE – ajuste o intervalo:
DATA_INICIO = "01/03/2025"
DATA_FIM    = "07/03/2025"
# PRODUÇÃO (últimos 3 dias):
# _hoje = dt.datetime.today()
# DATA_INICIO, DATA_FIM = (_hoje - dt.timedelta(days=3)).strftime("%d/%m/%Y"), _hoje.strftime("%d/%m/%Y")

REMETENTE_FILTRO = "informe@mkcompliance.com.br"
ASSUNTO_FIXO = "[EXT] - Normativos Divulgados em "
OUTLOOK_FOLDER_PATH = r"Caixa de Entrada\Normas MK Compliance"

PASTA_DESTINO = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Normativo_Download_Outlook"
ARQUIVO_EXCEL_EMAILS = os.path.join(PASTA_DESTINO, "Normativos_Email_Consolidado.xlsx")
ARQUIVO_EXCEL_METAS  = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\3_BaseMK\BaseMkNormasParaAvaliar.xlsx"
STATE_JSON           = os.path.join(PASTA_DESTINO, "state.json")
LOG_CSV              = os.path.join(PASTA_DESTINO, "log_execucao.csv")

HTTP_TIMEOUT = 45
LOG_LEVEL = logging.INFO
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")

# =========================
# HELPERS
# =========================
def ensure_dir(path: str | pathlib.Path) -> None:
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)

def N(txt: str) -> str:
    import unicodedata
    txt = str(txt or "")
    txt = "".join(c for c in unicodedata.normalize("NFKD", txt) if not unicodedata.combining(c))
    return re.sub(r"\s+", " ", txt.upper().strip())

def only_digits(s: str) -> str:
    return re.sub(r"\D", "", str(s or ""))

def parse_date_flexible(s: str) -> Optional[dt.date]:
    if not s:
        return None
    s = str(s).strip()
    try:
        return dtparser.parse(s, dayfirst=True).date()
    except Exception:
        pass
    m = re.search(r"(\d{1,2})\s+de\s+([a-zç\.]+)\s+de\s+(\d{4})", s, flags=re.IGNORECASE)
    if m:
        d = int(m.group(1))
        mes_raw = m.group(2).replace(".", "").lower()
        mapa = {
            "jan":1,"janeiro":1,"fev":2,"fevereiro":2,"mar":3,"marco":3,"março":3,"abr":4,"abril":4,
            "mai":5,"maio":5,"jun":6,"junho":6,"jul":7,"julho":7,"ago":8,"agosto":8,"set":9,"setembro":9,
            "out":10,"outubro":10,"nov":11,"novembro":11,"dez":12,"dezembro":12
        }
        mes = mapa.get(mes_raw, 0)
        y = int(m.group(3))
        if 1 <= int(mes) <= 12:
            return dt.date(y, int(mes), d)
    return None

def sanitize_filename(s: str) -> str:
    s = re.sub(r"[^\w\-. ]", "_", s, flags=re.UNICODE)
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"_+", "_", s)
    return s.strip("._")[:180]

def build_output_name_orgao_tipo_num(orgao: str, tipo: str, numero: str) -> str:
    base = f"{orgao}_{tipo}_{only_digits(numero)}"
    return sanitize_filename(base) + ".pdf"

def load_state() -> Dict:
    try:
        if os.path.exists(STATE_JSON):
            with open(STATE_JSON, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {}

def save_state(state: Dict) -> None:
    ensure_dir(os.path.dirname(STATE_JSON))
    with open(STATE_JSON, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def state_has_triplet(state: Dict, orgao: str, tipo: str, nnum: str) -> bool:
    key = f"{N(orgao)}|{N(tipo)}|{nnum}"
    return key in state.setdefault("downloaded_triplets", [])

def state_add_triplet(state: Dict, orgao: str, tipo: str, nnum: str) -> None:
    key = f"{N(orgao)}|{N(tipo)}|{nnum}"
    arr = state.setdefault("downloaded_triplets", [])
    if key not in arr:
        arr.append(key)

def file_already_present(dest_dir: str, base_filename: str) -> Optional[str]:
    base_stem = os.path.splitext(base_filename)[0]
    for f in os.listdir(dest_dir):
        if f.lower().endswith(".pdf") and f.lower().startswith(base_stem.lower()):
            return os.path.join(dest_dir, f)
    return None

# =========================
# ALIASES & ACRÔNIMOS (no-ops de quem ainda não casava: BNDES, MCID)
# =========================
ALIASES_ORGAO_RAW = {
    "BANCO NACIONAL DE DESENVOLVIMENTO ECONOMICO E SOCIAL": ["BNDES"],
    "MINISTERIO DAS CIDADES": ["MCID", "MINISTERIO DA CIDADE"],
    "MINISTERIO DA FAZENDA": ["MF", "FAZENDA"],
    "MINISTERIO DA ECONOMIA": ["ME", "ECONOMIA"],
    "BANCO CENTRAL DO BRASIL": ["BCB", "BACEN", "BANCO CENTRAL"],
    "CONSELHO MONETARIO NACIONAL": ["CMN"]
}
# normaliza keys e valores
ALIASES_ORGAO = {N(k): [N(v) for v in vs] for k, vs in ALIASES_ORGAO_RAW.items()}

STOPWORDS_SIGLA = {"DE","DA","DO","DAS","DOS","E","DOU","DOU-SE"}

def acronym_norm(s: str) -> str:
    toks = [t for t in re.split(r"[^\w]+", N(s)) if t and t not in STOPWORDS_SIGLA]
    return "".join(t[0] for t in toks)[:8]

def org_aliases(org: str) -> List[str]:
    orgN = N(org)
    out = {orgN, acronym_norm(orgN)}
    # pega aliases cadastrados
    for key, vals in ALIASES_ORGAO.items():
        if key in orgN or orgN in key:
            out.add(key)
            out.update(vals)
    return [a for a in out if a]

def org_match_loose(email_org: str, meta_org: str) -> bool:
    e, m = N(email_org), N(meta_org)
    if m in e or e in m:
        return True
    al = set(org_aliases(meta_org))
    # também tenta o acrônimo do email vs meta
    al.add(acronym_norm(email_org))
    return any(a and a in e for a in al)

def tipo_main_token(tipo: str) -> str:
    t = N(tipo)
    # primeira palavra útil
    return re.split(r"[^\w]+", t)[0] if t else ""

# =========================
# OUTLOOK
# =========================
def open_outlook_folder_by_path(path_str: str):
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    parts = [p for p in path_str.split("\\") if p.strip()]
    if not parts:
        return ns.GetDefaultFolder(6)  # Inbox

    root = None
    first = parts[0]
    for i in range(1, ns.Folders.Count + 1):
        store = ns.Folders.Item(i)
        if str(store.Name).strip().lower() == first.strip().lower():
            root = store
            parts = parts[1:]
            break

    if root is None:
        root = ns.GetDefaultFolder(6)
        if first.strip().lower() in ["inbox", "caixa de entrada"]:
            parts = parts[1:]

    folder = root
    for p in parts:
        folder = folder.Folders.Item(p)
    return folder

def restrict_by_date_and_subject(items, data_inicio_dt: dt.datetime, data_fim_dt: dt.datetime) -> list:
    di = data_inicio_dt.strftime("%m/%d/%Y")
    df = data_fim_dt.strftime("%m/%d/%Y")
    filtro_data = f"[ReceivedTime] >= '{di} 00:00 AM' AND [ReceivedTime] <= '{df} 11:59 PM'"
    candidates = list(items.Restrict(filtro_data))

    out = []
    for mail in candidates:
        try:
            subj = str(getattr(mail, "Subject", "") or "")
            from_addr = str(getattr(mail, "SenderEmailAddress", "") or "")
            if REMETENTE_FILTRO.lower() not in from_addr.lower():
                continue
            if not subj.startswith(ASSUNTO_FIXO):
                continue
            data_str = subj.replace(ASSUNTO_FIXO, "").strip()
            try:
                data_no_assunto = dt.datetime.strptime(data_str, "%d/%m/%Y")
            except Exception:
                continue
            if data_inicio_dt <= data_no_assunto <= data_fim_dt:
                out.append(mail)
        except Exception:
            continue
    return out

# =========================
# EMAIL HTML -> DATAFRAME CANÔNICO
# =========================
COLS_CANON = ["Tipo", "Número", "Data do Ato", "Órgão", "Ementa", "Vigência"]

HEADER_VARIANTS = {
    "Órgão": {N("Órgão"), N("Órgão regulador"), N("Orgao"), N("Orgao Regulador"), N("Orgão Regulador"), N("Órgão Responsável"), N("Orgão Responsável")},
    "Tipo": {N("Tipo"), N("Tipo da Norma"), N("Espécie"), N("Especie")},
    "Número": {N("Número"), N("Numero"), N("Nº"), N("No"), N("N."), N("N")},
    "Data do Ato": {N("Data do Ato"), N("Data do Normativo"), N("Data"), N("Publicação"), N("Data Publicação")},
    "Ementa": {N("Ementa"), N("Assunto"), N("Resumo"), N("Descrição"), N("Descricao")},
    "Vigência": {N("Vigência"), N("Vigencia")},
}

FORWARD_MARKERS = re.compile(
    r"(mensagem encaminhada|encaminhada|enc:|fw:|forwarded message|-----+ original message -----+)",
    re.I
)

def _row_cells(tr) -> list[str]:
    tds = tr.find_all(["td", "th"])
    return [td.get_text(" ", strip=True) for td in tds]

def _header_mapping(cells_norm: list[str]) -> Dict[int, str]:
    idx_to_key = {}
    for i, txt in enumerate(cells_norm):
        for canon, variants in HEADER_VARIANTS.items():
            if txt in variants and canon not in idx_to_key.values():
                idx_to_key[i] = canon
                break
    return idx_to_key

def _scan_table_for_header(table) -> Optional[Tuple[Dict[int, str], int, list]]:
    trs = table.find_all("tr")
    if not trs:
        return None
    best = None
    for idx, tr in enumerate(trs):
        cells = _row_cells(tr)
        if not cells:
            continue
        cells_norm = [N(c) for c in cells]
        mapping = _header_mapping(cells_norm)
        keys = set(mapping.values())
        if ("Número" in keys) and (("Tipo" in keys) or ("Órgão" in keys)) and len(keys) >= 4:
            score = (len(keys),)
            if (best is None) or (score > best[0]):
                best = (score, mapping, idx, trs)
    if best is None:
        return None
    _, mapping, idx, trs = best
    return mapping, idx, trs

def extract_df_from_email_html(html: str) -> pd.DataFrame:
    if not html:
        return pd.DataFrame(columns=COLS_CANON)

    cut_html = html
    m = FORWARD_MARKERS.search(html)
    if m:
        cut_html = html[:m.start()]

    soup = BeautifulSoup(cut_html, "lxml")
    candidates: list[tuple[Tuple[int, int], pd.DataFrame]] = []

    for table in soup.find_all("table"):
        header_scan = _scan_table_for_header(table)
        if not header_scan:
            continue
        idx_to_key, idx_header, trs = header_scan

        records = []
        for tr in trs[idx_header+1:]:
            cells = _row_cells(tr)
            if not cells:
                continue
            row = {k: "" for k in COLS_CANON}
            for i, canon in idx_to_key.items():
                if i < len(cells):
                    row[canon] = cells[i]
            if only_digits(row.get("Número", "")):
                records.append(row)

        if not records:
            continue

        df_tmp = pd.DataFrame.from_records(records, columns=COLS_CANON)
        for c in COLS_CANON:
            df_tmp[c] = df_tmp[c].fillna("").astype(str).str.replace("\xa0", " ", regex=False).str.strip()

        mask_headerish = df_tmp["Número"].map(lambda x: N(x) in HEADER_VARIANTS["Número"])
        df_tmp = df_tmp[~mask_headerish].copy()

        df_tmp["__N_NUM__"] = df_tmp["Número"].map(only_digits)
        df_tmp = df_tmp.drop_duplicates(subset=["__N_NUM__"], keep="first").drop(columns=["__N_NUM__"])

        useful_cols = sum(df_tmp[c].notna().any() for c in ["Tipo","Número","Data do Ato","Órgão"])
        score = (useful_cols, len(df_tmp))
        candidates.append((score, df_tmp))

    if not candidates:
        return pd.DataFrame(columns=COLS_CANON)

    candidates.sort(key=lambda t: t[0], reverse=True)
    best_df = candidates[0][1].copy()
    return best_df[COLS_CANON].reset_index(drop=True)

def add_normalized_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out["N_TIPO"] = out["Tipo"].map(N)
    out["N_NUM"] = out["Número"].map(only_digits)
    out["N_ORGAO"] = out["Órgão"].map(N)
    out["DATA_ATO_DT"] = out["Data do Ato"].map(parse_date_flexible)
    return out

# =========================
# METAS (EXCEL)
# =========================
def load_metas_df(path: str) -> pd.DataFrame:
    df = pd.read_excel(path)
    colmap = {N(c): c for c in df.columns}
    def find(*cands):
        for c in cands:
            if N(c) in colmap:
                return colmap[N(c)]
        return None

    c_org = find("Órgão regulador","Órgão","Orgao","Orgão Regulador","Orgao Regulador")
    c_tipo= find("Tipo","Tipo da Norma","Espécie","Especie")
    c_num = find("Número","Numero","Nº","No","N.","N")
    c_data= find("Data do Normativo","Data do Ato","Data","Publicação","Data Publicação")

    miss = [n for n,v in [("Órgão",c_org),("Tipo",c_tipo),("Número",c_num),("Data",c_data)] if v is None]
    if miss:
        raise RuntimeError(f"Colunas ausentes na planilha de metas: {miss}")

    metas = pd.DataFrame({
        "Órgão": df[c_org].astype(str),
        "Tipo":  df[c_tipo].astype(str),
        "Número":df[c_num].astype(str),
        "Data do Ato": df[c_data].astype(str),
    })
    metas = add_normalized_cols(metas)
    return metas

# =========================
# MATCHING
# =========================
def row_matches_target(email_row: pd.Series, meta_row: pd.Series) -> bool:
    # mantém o match forte por número
    if str(email_row["N_NUM"]) != str(meta_row["N_NUM"]):
        return False
    # órgão com aliases/acrônimo e contenções
    if not org_match_loose(email_row["N_ORGAO"], meta_row["N_ORGAO"]):
        return False
    # tipo por token principal
    e_tok, t_tok = tipo_main_token(email_row["N_TIPO"]), tipo_main_token(meta_row["N_TIPO"])
    return (t_tok == e_tok) or (t_tok and t_tok in email_row["N_TIPO"]) or (e_tok and e_tok in meta_row["N_TIPO"])

# =========================
# PDF DOWNLOAD (escolha por score; fallback sem número)
# =========================
def score_attachment_for_meta(filename: str, orgao: str, tipo: str, nnum: str, require_number: bool) -> Optional[int]:
    nameN = N(filename)
    has_num = nnum in nameN if nnum else False
    if require_number and not has_num:
        return None
    score = 0
    if has_num:
        score += 100
    first_tipo = tipo_main_token(tipo)
    if first_tipo and first_tipo in nameN:
        score += 20
    for alias in org_aliases(orgao):
        if alias and alias in nameN:
            score += 10
    # bônus se o "prefixo" esperado aparecer (ao menos o número)
    if nnum and nnum in nameN:
        score += 5
    return score

def find_all_pdf_attachments(mail) -> List[Tuple[str, object]]:
    out = []
    try:
        cnt = int(getattr(mail.Attachments, "Count", 0))
        for i in range(1, cnt + 1):
            att = mail.Attachments.Item(i)
            name = str(getattr(att, "FileName", "") or "")
            if name.lower().endswith(".pdf"):
                out.append((name, att))
    except Exception as e:
        logging.warning(f"Erro ao ler anexos: {e}")
    return out

def find_pdf_links_in_html(html: str, number_digits: str) -> List[str]:
    links = []
    if not html:
        return links
    soup = BeautifulSoup(html, "lxml")
    for a in soup.find_all("a"):
        href = (a.get("href") or "").strip()
        if href.lower().endswith(".pdf") and number_digits in N(href):
            links.append(href)
    return links

def download_link(url: str, dest_path: str) -> bool:
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return False
        with open(dest_path, "wb") as f:
            f.write(r.content)
        return True
    except Exception as e:
        logging.warning(f"Falha no download {url}: {e}")
        return False

# =========================
# PIPELINE
# =========================
def main():
    ensure_dir(PASTA_DESTINO)
    state = load_state()

    # 1) Ler e-mails e consolidar (gera Excel de auditoria) + guardar DF por e-mail
    folder = open_outlook_folder_by_path(OUTLOOK_FOLDER_PATH)
    items = folder.Items
    items.Sort("[ReceivedTime]", True)

    di = dt.datetime.strptime(DATA_INICIO, "%d/%m/%Y")
    df_ = dt.datetime.strptime(DATA_FIM, "%d/%m/%Y")
    mails = restrict_by_date_and_subject(items, di, df_)
    logging.info(f"E-mails elegíveis: {len(mails)}")

    email_dfs: List[pd.DataFrame] = []
    mail_index: Dict[str, object] = {}
    email_df_map: Dict[str, pd.DataFrame] = {}

    for mail in mails:
        entry_id = str(getattr(mail, "EntryID", ""))
        subj = str(getattr(mail, "Subject", "") or "")
        recd = getattr(mail, "ReceivedTime", None)
        html = str(getattr(mail, "HTMLBody", "") or "")

        df_email = extract_df_from_email_html(html)
        if df_email.empty:
            logging.info(f"[SKIP] Sem tabela reconhecida em: {subj}")
            continue

        df_email = add_normalized_cols(df_email)
        df_email = df_email.drop_duplicates(subset=["N_NUM", "N_TIPO", "N_ORGAO"], keep="first").reset_index(drop=True)

        df_email["EMAIL_ENTRY_ID"] = entry_id
        df_email["EMAIL_SUBJECT"] = subj
        df_email["EMAIL_RECEIVED"] = recd.strftime("%Y-%m-%d %H:%M:%S") if isinstance(recd, dt.datetime) else ""

        email_dfs.append(df_email)
        email_df_map[entry_id] = df_email.copy()
        mail_index[entry_id] = mail

    if not email_dfs:
        logging.warning("Nenhuma tabela de e-mail extraída. Encerrando.")
        return

    emails_df_inmem = pd.concat(email_dfs, ignore_index=True)
    try:
        emails_df_inmem.to_excel(ARQUIVO_EXCEL_EMAILS, index=False)
        logging.info(f"Consolidado salvo em: {ARQUIVO_EXCEL_EMAILS}")
    except Exception as e:
        logging.warning(f"Falha ao salvar Excel consolidado: {e}")

    # 2) Reabre planilha do e-mail (fonte para JOIN)
    emails_df = pd.read_excel(ARQUIVO_EXCEL_EMAILS, dtype=str).fillna("")
    emails_df["N_TIPO"]   = emails_df["Tipo"].map(N)
    emails_df["N_NUM"]    = emails_df["Número"].map(only_digits)
    emails_df["N_ORGAO"]  = emails_df["Órgão"].map(N)

    # 3) Metas (fonte da verdade)
    metas_df = load_metas_df(ARQUIVO_EXCEL_METAS)
    logging.info(f"Metas carregadas: {len(metas_df)}")

    # 4) JOIN forte por número
    base_join = emails_df.merge(
        metas_df, how="inner", left_on="N_NUM", right_on="N_NUM", suffixes=("_EMAIL", "_META")
    )
    # validação órgão/tipo
    ok_mask = []
    for _, row in base_join.iterrows():
        ok_mask.append(
            row_matches_target(
                email_row=row[["N_NUM","N_ORGAO_EMAIL","N_TIPO_EMAIL"]].rename(
                    {"N_ORGAO_EMAIL":"N_ORGAO","N_TIPO_EMAIL":"N_TIPO"}
                ),
                meta_row=row[["N_NUM","N_ORGAO_META","N_TIPO_META"]].rename(
                    {"N_ORGAO_META":"N_ORGAO","N_TIPO_META":"N_TIPO"}
                )
            )
        )
    matched = base_join[ok_mask].copy()

    # Conjunto de triplas já atendidas
    done_keys = set(
        f"{N(r['Órgão_META'])}|{N(r['Tipo_META'])}|{str(r['N_NUM'])}"
        for _, r in matched.iterrows()
    )

    # 5) Fallback: metas não atendidas — tentar casar por (Órgão ~ aliases/acrônimo) + tipo (token principal)
    remaining_metas = []
    for _, m in metas_df.iterrows():
        key = f"{m['N_ORGAO']}|{m['N_TIPO']}|{str(m['N_NUM'])}"
        if key not in done_keys:
            remaining_metas.append(m)
    fallback_rows = []

    for entry_id, df_email in email_df_map.items():
        for m in remaining_metas:
            # procura linha do e-mail que combine por órgão (aliases/acrônimo) + tipo (main token)
            main_tok = tipo_main_token(m["N_TIPO"])
            candidates = df_email[
                df_email.apply(lambda r: org_match_loose(r["N_ORGAO"], m["N_ORGAO"]) and
                                         (main_tok and (main_tok in r["N_TIPO"] or main_tok == tipo_main_token(r["N_TIPO"]))),
                               axis=1)
            ]
            if not candidates.empty:
                # pegue a primeira linha (qualquer uma já garante presença)
                r = candidates.iloc[0]
                row = {
                    "EMAIL_ENTRY_ID": entry_id,
                    "EMAIL_SUBJECT": r["EMAIL_SUBJECT"],
                    "EMAIL_RECEIVED": r["EMAIL_RECEIVED"],
                    "Órgão_META": m["Órgão"],
                    "Tipo_META": m["Tipo"],
                    "Número_META": m["Número"],
                    "N_ORGAO_META": m["N_ORGAO"],
                    "N_TIPO_META": m["N_TIPO"],
                    "N_NUM": m["N_NUM"],  # pode estar vazio/estranho; usaremos só se ajudar
                }
                fallback_rows.append(row)

    matched_fallback = pd.DataFrame(fallback_rows)
    # Unifica (mantendo os já fortes)
    if not matched_fallback.empty:
        # Remove os que já estão em 'matched'
        mf_keys = set(f"{N(r['Órgão_META'])}|{N(r['Tipo_META'])}|{str(r['N_NUM'])}" for _, r in matched.iterrows())
        matched_fallback = matched_fallback[
            matched_fallback.apply(lambda r: f"{N(r['Órgão_META'])}|{N(r['Tipo_META'])}|{str(r['N_NUM'])}" not in mf_keys, axis=1)
        ]
        # Anexa como se fosse o mesmo schema de 'matched'
        matched_extra = matched_fallback.copy()
        matched_extra["EMAIL_ENTRY_ID"] = matched_fallback["EMAIL_ENTRY_ID"]
        matched = pd.concat([matched, matched_extra], ignore_index=True)

    if matched.empty:
        logging.info("Nenhum match encontrado.")
        return

    logging.info(f"Total de matches (forte + fallback): {len(matched)}")

    # 6) Download por e-mail, usando metas como referência
    saved_mail_num = set()
    results = []

    for entry_id, grp in matched.groupby("EMAIL_ENTRY_ID"):
        mail = mail_index.get(entry_id)
        if mail is None:
            continue
        all_atts = find_all_pdf_attachments(mail)

        for _, r in grp.iterrows():
            nnum   = str(r.get("N_NUM") or "")
            orgao  = str(r.get("Órgão_META") or "")
            tipo   = str(r.get("Tipo_META")  or "")
            numero_vis = str(r.get("Número_META") or nnum)

            # trava por (email, numero) quando há número. Se não houver, trava por (email, orgao|tipo)
            key_mail_num = (entry_id, nnum if nnum else f"{N(orgao)}|{tipo_main_token(tipo)}")
            if key_mail_num in saved_mail_num:
                continue

            base_name = build_output_name_orgao_tipo_num(orgao, tipo, numero_vis)
            fs_hit = file_already_present(PASTA_DESTINO, base_name)
            if state_has_triplet(state, orgao, tipo, nnum) or fs_hit:
                saved_mail_num.add(key_mail_num)
                results.append({
                    "email_entry_id": entry_id,
                    "email_subject": r.get("EMAIL_SUBJECT",""),
                    "email_received": r.get("EMAIL_RECEIVED",""),
                    "orgao": orgao, "tipo": tipo, "numero": nnum,
                    "arquivos_salvos": fs_hit or "",
                    "status": "JA_EXISTIA"
                })
                continue

            # 1º: tenta com número obrigatório
            scored = []
            for name, att in all_atts:
                sc = score_attachment_for_meta(name, orgao, tipo, nnum, require_number=True)
                if sc is not None:
                    scored.append((sc, name, att))
            scored.sort(reverse=True)

            saved_paths = []
            if scored:
                _, name, att = scored[0]
                dest = os.path.join(PASTA_DESTINO, base_name)
                k = 1
                while os.path.exists(dest):
                    dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                    k += 1
                att.SaveAsFile(dest)
                saved_paths.append(dest)
            else:
                # 2º: fallback sem número (para casos como “Portaria Inter MCID_MF 2025-04.pdf”)
                scored2 = []
                for name, att in all_atts:
                    sc = score_attachment_for_meta(name, orgao, tipo, nnum, require_number=False)
                    if sc is not None:
                        scored2.append((sc, name, att))
                scored2.sort(reverse=True)
                if scored2:
                    _, name, att = scored2[0]
                    dest = os.path.join(PASTA_DESTINO, base_name)
                    k = 1
                    while os.path.exists(dest):
                        dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                        k += 1
                    att.SaveAsFile(dest)
                    saved_paths.append(dest)
                else:
                    # último recurso: link com número (se houver)
                    html = str(getattr(mail, "HTMLBody", "") or "")
                    for url in find_pdf_links_in_html(html, nnum):
                        dest = os.path.join(PASTA_DESTINO, base_name)
                        k = 1
                        while os.path.exists(dest):
                            dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                            k += 1
                        if download_link(url, dest):
                            saved_paths.append(dest)
                            break

            saved_mail_num.add(key_mail_num)
            if saved_paths:
                state_add_triplet(state, orgao, tipo, nnum)

            results.append({
                "email_entry_id": entry_id,
                "email_subject": r.get("EMAIL_SUBJECT",""),
                "email_received": r.get("EMAIL_RECEIVED",""),
                "orgao": orgao, "tipo": tipo, "numero": nnum,
                "arquivos_salvos": ";".join(saved_paths),
                "status": "BAIXADO" if saved_paths else "NAO_ENCONTRADO_PDF"
            })

    # 7) Log + persistência
    try:
        df_log = pd.DataFrame(results)
        if os.path.exists(LOG_CSV):
            old = pd.read_csv(LOG_CSV, encoding="utf-8-sig")
            df_log = pd.concat([old, df_log], ignore_index=True)
        if not df_log.empty:
            df_log.drop_duplicates(subset=["orgao","tipo","numero","arquivos_salvos"], keep="last", inplace=True)
        df_log.to_csv(LOG_CSV, index=False, encoding="utf-8-sig")
        logging.info(f"Log salvo em: {LOG_CSV}")
    except Exception as e:
        logging.warning(f"Falha ao salvar log: {e}")

    save_state(state)
    logging.info("Processo concluído.")

# =========================
# RUN
# =========================
if __name__ == "__main__":
    main()
