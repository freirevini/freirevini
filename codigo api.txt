# -*- coding: utf-8 -*-
import os
import re
import logging
import pathlib
import datetime as dt
from typing import List, Dict, Optional, Tuple

import win32com.client as win32
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

# =========================
# CONFIG
# =========================
# TESTE – ajuste o intervalo:
DATA_INICIO = "01/03/2025"
DATA_FIM    = "07/03/2025"
# PRODUÇÃO (últimos 3 dias):
# _hoje = dt.datetime.today()
# DATA_INICIO, DATA_FIM = (_hoje - dt.timedelta(days=3)).strftime("%d/%m/%Y"), _hoje.strftime("%d/%m/%Y")

REMETENTE_FILTRO = "informe@mkcompliance.com.br"
ASSUNTO_FIXO = "[EXT] - Normativos Divulgados em "
OUTLOOK_FOLDER_PATH = r"Caixa de Entrada\Normas MK Compliance"

# Pasta onde os PDFs serão salvos
PASTA_DESTINO = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Normativo_Download_Outlook"

# Caminho da planilha de Metas (fonte e local da planilha de saída)
ARQUIVO_EXCEL_METAS  = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\3_BaseMK\BaseMkNormasParaAvaliar.xlsx"

# Arquivo de saída: será criado na MESMA PASTA do arquivo de metas
NOME_ARQ_SAIDA = "Normas_Status_Download.xlsx"
ARQ_SAIDA = os.path.join(os.path.dirname(ARQUIVO_EXCEL_METAS), NOME_ARQ_SAIDA)

HTTP_TIMEOUT = 45
LOG_LEVEL = logging.INFO
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")

# =========================
# HELPERS
# =========================
def ensure_dir(path: str | pathlib.Path) -> None:
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)

def N(txt: str) -> str:
    import unicodedata
    txt = str(txt or "")
    txt = "".join(c for c in unicodedata.normalize("NFKD", txt) if not unicodedata.combining(c))
    return re.sub(r"\s+", " ", txt.upper().strip())

def only_digits(s: str) -> str:
    return re.sub(r"\D", "", str(s or ""))

def parse_date_flexible(s: str) -> Optional[dt.date]:
    if not s:
        return None
    s = str(s).strip()
    try:
        return dtparser.parse(s, dayfirst=True).date()
    except Exception:
        pass
    m = re.search(r"(\d{1,2})\s+de\s+([a-zç\.]+)\s+de\s+(\d{4})", s, flags=re.IGNORECASE)
    if m:
        d = int(m.group(1))
        mes_raw = m.group(2).replace(".", "").lower()
        mapa = {
            "jan":1,"janeiro":1,"fev":2,"fevereiro":2,"mar":3,"marco":3,"março":3,"abr":4,"abril":4,
            "mai":5,"maio":5,"jun":6,"junho":6,"jul":7,"julho":7,"ago":8,"agosto":8,"set":9,"setembro":9,
            "out":10,"outubro":10,"nov":11,"novembro":11,"dez":12,"dezembro":12
        }
        mes = mapa.get(mes_raw, 0)
        y = int(m.group(3))
        if 1 <= int(mes) <= 12:
            return dt.date(y, int(mes), d)
    return None

def sanitize_filename(s: str) -> str:
    # Mantém underscore como separador; remove caracteres inválidos
    s = re.sub(r"[^\w\-. ]", "_", s, flags=re.UNICODE)
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"_+", "_", s)
    return s.strip("._")[:180]

def build_output_name_orgao_tipo_num(orgao: str, tipo: str, numero: str) -> str:
    base = f"{orgao}_{tipo}_{only_digits(numero)}"
    return sanitize_filename(base) + ".pdf"

def file_already_present(dest_dir: str, base_filename: str) -> Optional[str]:
    """
    Se já existir qualquer arquivo iniciando com base_filename (sem sufixos),
    retorna o caminho encontrado; senão, None.
    """
    base_stem = os.path.splitext(base_filename)[0]
    for f in os.listdir(dest_dir):
        if f.lower().endswith(".pdf") and f.lower().startswith(base_stem.lower()):
            return os.path.join(dest_dir, f)
    return None

# =========================
# OUTLOOK
# =========================
def open_outlook_folder_by_path(path_str: str):
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    parts = [p for p in path_str.split("\\") if p.strip()]
    if not parts:
        return ns.GetDefaultFolder(6)  # Inbox

    root = None
    first = parts[0]
    for i in range(1, ns.Folders.Count + 1):
        store = ns.Folders.Item(i)
        if str(store.Name).strip().lower() == first.strip().lower():
            root = store
            parts = parts[1:]
            break

    if root is None:
        root = ns.GetDefaultFolder(6)
        if first.strip().lower() in ["inbox", "caixa de entrada"]:
            parts = parts[1:]

    folder = root
    for p in parts:
        folder = folder.Folders.Item(p)
    return folder

def restrict_by_date_and_subject(items, data_inicio_dt: dt.datetime, data_fim_dt: dt.datetime) -> list:
    di = data_inicio_dt.strftime("%m/%d/%Y")
    df = data_fim_dt.strftime("%m/%d/%Y")
    filtro_data = f"[ReceivedTime] >= '{di} 00:00 AM' AND [ReceivedTime] <= '{df} 11:59 PM'"
    candidates = list(items.Restrict(filtro_data))

    out = []
    for mail in candidates:
        try:
            subj = str(getattr(mail, "Subject", "") or "")
            from_addr = str(getattr(mail, "SenderEmailAddress", "") or "")
            if REMETENTE_FILTRO.lower() not in from_addr.lower():
                continue
            if not subj.startswith(ASSUNTO_FIXO):
                continue
            data_str = subj.replace(ASSUNTO_FIXO, "").strip()
            try:
                data_no_assunto = dt.datetime.strptime(data_str, "%d/%m/%Y")
            except Exception:
                continue
            if data_inicio_dt <= data_no_assunto <= data_fim_dt:
                out.append(mail)
        except Exception:
            continue
    return out

# =========================
# EMAIL HTML -> DATAFRAME CANÔNICO
# =========================
COLS_CANON = ["Tipo", "Número", "Data do Ato", "Órgão", "Ementa", "Vigência"]

HEADER_VARIANTS = {
    "Órgão": {N("Órgão"), N("Órgão regulador"), N("Orgao"), N("Orgao Regulador"), N("Orgão Regulador"), N("Órgão Responsável"), N("Orgão Responsável")},
    "Tipo": {N("Tipo"), N("Tipo da Norma"), N("Espécie"), N("Especie")},
    "Número": {N("Número"), N("Numero"), N("Nº"), N("No"), N("N."), N("N")},
    "Data do Ato": {N("Data do Ato"), N("Data do Normativo"), N("Data"), N("Publicação"), N("Data Publicação")},
    "Ementa": {N("Ementa"), N("Assunto"), N("Resumo"), N("Descrição"), N("Descricao")},
    "Vigência": {N("Vigência"), N("Vigencia")},
}

FORWARD_MARKERS = re.compile(
    r"(mensagem encaminhada|encaminhada|enc:|fw:|forwarded message|-----+ original message -----+)",
    re.I
)

def _row_cells(tr) -> list[str]:
    tds = tr.find_all(["td", "th"])
    return [td.get_text(" ", strip=True) for td in tds]

def _header_mapping(cells_norm: list[str]) -> Dict[int, str]:
    idx_to_key = {}
    for i, txt in enumerate(cells_norm):
        for canon, variants in HEADER_VARIANTS.items():
            if txt in variants and canon not in idx_to_key.values():
                idx_to_key[i] = canon
                break
    return idx_to_key

def _scan_table_for_header(table) -> Optional[Tuple[Dict[int, str], int, list]]:
    trs = table.find_all("tr")
    if not trs:
        return None
    best = None
    for idx, tr in enumerate(trs):
        cells = _row_cells(tr)
        if not cells:
            continue
        cells_norm = [N(c) for c in cells]
        mapping = _header_mapping(cells_norm)
        keys = set(mapping.values())
        if ("Número" in keys) and (("Tipo" in keys) or ("Órgão" in keys)) and len(keys) >= 4:
            score = (len(keys),)
            if (best is None) or (score > best[0]):
                best = (score, mapping, idx, trs)
    if best is None:
        return None
    _, mapping, idx, trs = best
    return mapping, idx, trs

def extract_df_from_email_html(html: str) -> pd.DataFrame:
    if not html:
        return pd.DataFrame(columns=COLS_CANON)

    cut_html = html
    m = FORWARD_MARKERS.search(html)
    if m:
        cut_html = html[:m.start()]

    soup = BeautifulSoup(cut_html, "lxml")
    candidates: list[tuple[Tuple[int, int], pd.DataFrame]] = []

    for table in soup.find_all("table"):
        header_scan = _scan_table_for_header(table)
        if not header_scan:
            continue
        idx_to_key, idx_header, trs = header_scan

        records = []
        for tr in trs[idx_header+1:]:
            cells = _row_cells(tr)
            if not cells:
                continue
            row = {k: "" for k in COLS_CANON}
            for i, canon in idx_to_key.items():
                if i < len(cells):
                    row[canon] = cells[i]
            if only_digits(row.get("Número", "")):
                records.append(row)

        if not records:
            continue

        df_tmp = pd.DataFrame.from_records(records, columns=COLS_CANON)
        # limpeza
        for c in COLS_CANON:
            df_tmp[c] = df_tmp[c].fillna("").astype(str).str.replace("\xa0", " ", regex=False).str.strip()

        # remove re-cabeçalhos ocultos
        mask_headerish = df_tmp["Número"].map(lambda x: N(x) in HEADER_VARIANTS["Número"])
        df_tmp = df_tmp[~mask_headerish].copy()

        # dedupe por Número (dentro da tabela)
        df_tmp["__N_NUM__"] = df_tmp["Número"].map(only_digits)
        df_tmp = df_tmp.drop_duplicates(subset=["__N_NUM__"], keep="first").drop(columns=["__N_NUM__"])

        useful_cols = sum(df_tmp[c].notna().any() for c in ["Tipo","Número","Data do Ato","Órgão"])
        score = (useful_cols, len(df_tmp))
        candidates.append((score, df_tmp))

    if not candidates:
        return pd.DataFrame(columns=COLS_CANON)

    candidates.sort(key=lambda t: t[0], reverse=True)
    best_df = candidates[0][1].copy()
    return best_df[COLS_CANON].reset_index(drop=True)

def add_normalized_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out["N_TIPO"] = out["Tipo"].map(N)
    out["N_NUM"] = out["Número"].map(only_digits)
    out["N_ORGAO"] = out["Órgão"].map(N)
    out["DATA_ATO_DT"] = out["Data do Ato"].map(parse_date_flexible)
    return out

# =========================
# METAS (EXCEL) — mapeamento amplo
# =========================
def load_metas_df(path: str) -> pd.DataFrame:
    df = pd.read_excel(path).fillna("")
    # mapa das possíveis variantes -> nome canônico
    variants = {
        "Órgão regulador": {"Órgão regulador","Órgão","Orgao","Orgão Regulador","Orgao Regulador","Órgão Regulador"},
        "Tipo": {"Tipo","Tipo da Norma","Espécie","Especie"},
        "Número": {"Número","Numero","Nº","No","N.","N"},
        "Data do Normativo": {"Data do Normativo","Data do Ato","Data","Publicação","Data Publicação"},
        "Ementa": {"Ementa","Assunto","Resumo","Descrição","Descricao"},
        "Data de inicio de vigência": {"Data de inicio de vigência","Data de início de vigência","Início de Vigência","Data início de vigência","Data Inicio Vigencia"},
        "Vigência": {"Vigência","Vigencia"},
        "Data de inclusão no MKCompliance": {"Data de inclusão no MKCompliance","Inclusão MKCompliance","Data Inclusão MK"}
    }

    # cria um mapa N(coluna)->coluna original
    colnorm = {N(c): c for c in df.columns}

    def pick(canon: str) -> Optional[str]:
        for v in variants.get(canon, {canon}):
            if N(v) in colnorm:
                return colnorm[N(v)]
        return None

    c_org  = pick("Órgão regulador")
    c_tipo = pick("Tipo")
    c_num  = pick("Número")
    c_data = pick("Data do Normativo")
    # outros opcionais
    c_ementa   = pick("Ementa")
    c_vig_ini  = pick("Data de inicio de vigência")
    c_vig      = pick("Vigência")
    c_inc_mk   = pick("Data de inclusão no MKCompliance")

    miss = [n for n,v in [("Órgão regulador",c_org),("Tipo",c_tipo),("Número",c_num),("Data do Normativo",c_data)] if v is None]
    if miss:
        raise RuntimeError(f"Colunas ausentes na planilha de metas: {miss}")

    metas = pd.DataFrame({
        "Órgão regulador": df[c_org].astype(str),
        "Tipo":            df[c_tipo].astype(str),
        "Número":          df[c_num].astype(str),
        "Data do Normativo": df[c_data].astype(str),
        "Ementa": df[c_ementa].astype(str) if c_ementa else "",
        "Data de inicio de vigência": df[c_vig_ini].astype(str) if c_vig_ini else "",
        "Vigência": df[c_vig].astype(str) if c_vig else "",
        "Data de inclusão no MKCompliance": df[c_inc_mk].astype(str) if c_inc_mk else "",
    })

    # colunas normalizadas para matching
    metas["N_TIPO_META"]   = metas["Tipo"].map(N)
    metas["N_NUM"]         = metas["Número"].map(only_digits)
    metas["N_ORGAO_META"]  = metas["Órgão regulador"].map(N)

    # remove duplicidades explícitas na própria planilha (por segurança)
    metas = metas.drop_duplicates(subset=["N_NUM","N_ORGAO_META","N_TIPO_META"], keep="first").reset_index(drop=True)
    return metas

# =========================
# MATCHING
# =========================
def row_matches_target(email_row: pd.Series, meta_row: pd.Series) -> bool:
    # 1) mesmo número
    if str(email_row["N_NUM"]) != str(meta_row["N_NUM"]):
        return False
    # 2) órgão: aceita equivalências comuns
    e_org = email_row["N_ORGAO"]; t_org = meta_row["N_ORGAO_META"]
    org_ok = any(alias in e_org for alias in [t_org, "BANCO CENTRAL", "BANCO CENTRAL DO BRASIL", "BCB", "BACEN", "CVM", "ANBIMA", "CMN"])
    if not org_ok:
        return False
    # 3) tipo flexível (contém/é prefixo)
    e_tipo= email_row["N_TIPO"];  t_tipo= meta_row["N_TIPO_META"]
    tipo_ok = (t_tipo in e_tipo) or (e_tipo in t_tipo) or (t_tipo.split()[0] in e_tipo if t_tipo else False)
    return tipo_ok

# =========================
# PDF DOWNLOAD
# =========================
def find_pdf_attachments(mail, number_digits: str, tipo_norm: str | None = None) -> List[Tuple[str, object]]:
    found = []
    try:
        cnt = int(getattr(mail.Attachments, "Count", 0))
        for i in range(1, cnt + 1):
            att = mail.Attachments.Item(i)
            name = str(getattr(att, "FileName", "") or "")
            if not name.lower().endswith(".pdf"):
                continue
            n_ok = number_digits in N(name)
            if not n_ok:
                continue
            if tipo_norm:
                if N(tipo_norm).split()[0] not in N(name):
                    found.append((name, att))   # prioridade menor
                else:
                    found.insert(0, (name, att))  # prioridade maior
            else:
                found.append((name, att))
    except Exception as e:
        logging.warning(f"Erro ao ler anexos: {e}")
    return found

def find_pdf_links_in_html(html: str, number_digits: str) -> List[str]:
    links = []
    if not html:
        return links
    soup = BeautifulSoup(html, "lxml")
    for a in soup.find_all("a"):
        href = (a.get("href") or "").strip()
        if href.lower().endswith(".pdf") and number_digits in N(href):
            links.append(href)
    return links

def download_link(url: str, dest_path: str) -> bool:
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return False
        with open(dest_path, "wb") as f:
            f.write(r.content)
        return True
    except Exception as e:
        logging.warning(f"Falha no download {url}: {e}")
        return False

# =========================
# PIPELINE
# =========================
def main():
    ensure_dir(PASTA_DESTINO)

    # 1) Ler e-mails do Outlook (apenas em memória; não gera Excel)
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    folder = open_outlook_folder_by_path(OUTLOOK_FOLDER_PATH)
    items = folder.Items
    items.Sort("[ReceivedTime]", True)

    di = dt.datetime.strptime(DATA_INICIO, "%d/%m/%Y")
    df_ = dt.datetime.strptime(DATA_FIM, "%d/%m/%Y")
    mails = restrict_by_date_and_subject(items, di, df_)
    logging.info(f"E-mails elegíveis: {len(mails)}")

    email_dfs = []
    mail_index: Dict[str, object] = {}

    for mail in mails:
        entry_id = str(getattr(mail, "EntryID", ""))
        subj = str(getattr(mail, "Subject", "") or "")
        recd = getattr(mail, "ReceivedTime", None)
        html = str(getattr(mail, "HTMLBody", "") or "")

        df_email = extract_df_from_email_html(html)
        if df_email.empty:
            logging.info(f"[SKIP] Sem tabela reconhecida em: {subj}")
            continue

        df_email = add_normalized_cols(df_email)

        # DEDUPE dentro do e-mail por (N_NUM, N_TIPO, N_ORGAO)
        df_email = df_email.drop_duplicates(subset=["N_NUM", "N_TIPO", "N_ORGAO"], keep="first").reset_index(drop=True)

        df_email["EMAIL_ENTRY_ID"] = entry_id
        df_email["EMAIL_SUBJECT"] = subj
        df_email["EMAIL_RECEIVED"] = recd.strftime("%Y-%m-%d %H:%M:%S") if isinstance(recd, dt.datetime) else ""

        email_dfs.append(df_email)
        mail_index[entry_id] = mail

    if not email_dfs:
        logging.warning("Nenhuma tabela de e-mail extraída. Encerrando.")
        # Mesmo assim, ainda geramos arquivo de saída marcando todos como Não
        metas_df = load_metas_df(ARQUIVO_EXCEL_METAS)
        saida = metas_df.copy()
        saida["PDF"] = "Não"
        saida["Caminho"] = ""
        saida.to_excel(ARQ_SAIDA, index=False)
        logging.info(f"Saída gerada (sem e-mails): {ARQ_SAIDA}")
        return

    emails_df = pd.concat(email_dfs, ignore_index=True)

    # 2) Carregar METAS (fonte de verdade para seleção do que baixar)
    metas_df = load_metas_df(ARQUIVO_EXCEL_METAS)
    logging.info(f"Metas carregadas: {len(metas_df)}")

    # 3) JOIN por número (N_NUM) e depois validação por órgão/tipo
    base_join = emails_df.merge(
        metas_df,
        how="inner",
        left_on="N_NUM",
        right_on="N_NUM",
        suffixes=("_EMAIL", "_META")
    )
    if base_join.empty:
        logging.info("Nenhum número coincidente entre e-mails e metas.")
        # Gera saída com todos Não
        saida = metas_df.copy()
        saida["PDF"] = "Não"
        saida["Caminho"] = ""
        saida.to_excel(ARQ_SAIDA, index=False)
        logging.info(f"Saída gerada (sem matches): {ARQ_SAIDA}")
        return

    # Validação por órgão/tipo
    ok_mask = []
    for _, row in base_join.iterrows():
        e_row = pd.Series({
            "N_NUM": row["N_NUM"],
            "N_ORGAO": row["N_ORGAO_EMAIL"],
            "N_TIPO": row["N_TIPO_EMAIL"]
        })
        m_row = pd.Series({
            "N_NUM": row["N_NUM"],
            "N_ORGAO_META": row["N_ORGAO_META"],
            "N_TIPO_META": row["N_TIPO_META"]
        })
        ok_mask.append(row_matches_target(e_row, m_row))

    matched = base_join[ok_mask].copy()
    if matched.empty:
        logging.info("Nenhum match validado por órgão/tipo.")
        saida = metas_df.copy()
        saida["PDF"] = "Não"
        saida["Caminho"] = ""
        saida.to_excel(ARQ_SAIDA, index=False)
        logging.info(f"Saída gerada (sem matches válidos): {ARQ_SAIDA}")
        return

    logging.info(f"Matches validados (linhas): {len(matched)}")

    # 4) Desambiguação: baixar no máximo 1 PDF por chave (N_NUM, N_ORGAO_META)
    #    Se houver múltiplos e-mails para a mesma chave, prioriza onde o tipo casa melhor.
    matched["__score_tipo__"] = matched.apply(
        lambda r: 2 if r["N_TIPO_META"] == r["N_TIPO_EMAIL"]
        else (1 if (r["N_TIPO_META"].split()[0] in r["N_TIPO_EMAIL"]) else 0),
        axis=1
    )

    # Ordena por: (N_NUM, N_ORGAO_META) e melhor score_tipo, e mais recente email
    matched.sort_values(
        by=["N_NUM", "N_ORGAO_META", "__score_tipo__", "EMAIL_RECEIVED"],
        ascending=[True, True, False, False],
        inplace=True
    )

    # Mantém apenas o primeiro por (N_NUM, N_ORGAO_META)
    matched_unique = matched.drop_duplicates(subset=["N_NUM", "N_ORGAO_META"], keep="first").copy()

    # 5) Download
    ensure_dir(PASTA_DESTINO)

    # Mapearemos (N_NUM, N_ORGAO_META) -> caminho (se baixado/encontrado)
    dl_map: Dict[Tuple[str, str], str] = {}

    for _, r in matched_unique.iterrows():
        entry_id = str(r["EMAIL_ENTRY_ID"])
        mail = mail_index.get(entry_id)
        if mail is None:
            continue

        nnum   = str(r["N_NUM"])
        orgao_meta = str(r["Órgão regulador"])
        tipo_meta  = str(r["Tipo"])
        numero_vis = str(r["Número"] or nnum)

        base_name = build_output_name_orgao_tipo_num(orgao_meta, tipo_meta, numero_vis)

        # Se já existe no filesystem, só registra e segue (não duplica)
        fs_hit = file_already_present(PASTA_DESTINO, base_name)
        if fs_hit:
            dl_map[(nnum, N(orgao_meta))] = fs_hit
            continue

        saved_paths = []

        # 1) anexos
        for name, att in find_pdf_attachments(mail, nnum, tipo_norm=tipo_meta):
            dest = os.path.join(PASTA_DESTINO, base_name)
            k = 1
            while os.path.exists(dest):
                dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                k += 1
            att.SaveAsFile(dest)
            saved_paths.append(dest)
            break  # apenas um por chave

        # 2) fallback por link
        if not saved_paths:
            html = str(getattr(mail, "HTMLBody", "") or "")
            for url in find_pdf_links_in_html(html, nnum):
                dest = os.path.join(PASTA_DESTINO, base_name)
                k = 1
                while os.path.exists(dest):
                    dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                    k += 1
                if download_link(url, dest):
                    saved_paths.append(dest)
                    break  # apenas um por chave

        if saved_paths:
            dl_map[(nnum, N(orgao_meta))] = saved_paths[0]

    # 6) Monta a planilha de SAÍDA com base na planilha de Metas
    saida = metas_df.copy()

    # Inicialmente sem PDF
    saida["PDF"] = "Não"
    saida["Caminho"] = ""

    # Preenche para cada linha de metas
    for idx, row in saida.iterrows():
        k = (str(row["N_NUM"]), N(str(row["Órgão regulador"])))
        path = dl_map.get(k, "")
        if path and os.path.exists(path):
            saida.at[idx, "PDF"] = "Sim"
            saida.at[idx, "Caminho"] = path

    # 7) Salva APENAS o Excel de saída na pasta da planilha de metas
    try:
        saida.to_excel(ARQ_SAIDA, index=False)
        logging.info(f"Saída gerada: {ARQ_SAIDA}")
    except Exception as e:
        logging.warning(f"Falha ao salvar saída: {e}")

    logging.info("Processo concluído (v2).")

# =========================
# RUN
# =========================
if __name__ == "__main__":
    main()
