# -*- coding: utf-8 -*-
import os
import re
import json
import logging
import pathlib
import datetime as dt
from typing import List, Dict, Optional, Tuple

import win32com.client as win32
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

# =========================
# CONFIG
# =========================
# TESTE – ajuste o intervalo:
DATA_INICIO = "01/03/2025"
DATA_FIM    = "07/03/2025"
# PRODUÇÃO (últimos 3 dias):
# _hoje = dt.datetime.today()
# DATA_INICIO, DATA_FIM = (_hoje - dt.timedelta(days=3)).strftime("%d/%m/%Y"), _hoje.strftime("%d/%m/%Y")

REMETENTE_FILTRO = "informe@mkcompliance.com.br"
ASSUNTO_FIXO = "[EXT] - Normativos Divulgados em "
OUTLOOK_FOLDER_PATH = r"Caixa de Entrada\Normas MK Compliance"

PASTA_DESTINO = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Normativo_Download_Outlook"
ARQUIVO_EXCEL_METAS  = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\3_BaseMK\BaseMkNormasParaAvaliar.xlsx"
STATE_JSON           = os.path.join(PASTA_DESTINO, "state.json")

HTTP_TIMEOUT = 45
LOG_LEVEL = logging.INFO
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")

# =========================
# HELPERS
# =========================
def ensure_dir(path: str | pathlib.Path) -> None:
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)

def N(txt: str) -> str:
    import unicodedata
    txt = str(txt or "")
    txt = "".join(c for c in unicodedata.normalize("NFKD", txt) if not unicodedata.combining(c))
    return re.sub(r"\s+", " ", txt.upper().strip())

def only_digits(s: str) -> str:
    return re.sub(r"\D", "", str(s or ""))

def parse_date_flexible(s: str) -> Optional[dt.date]:
    if not s:
        return None
    s = str(s).strip()
    try:
        return dtparser.parse(s, dayfirst=True).date()
    except Exception:
        pass
    m = re.search(r"(\d{1,2})\s+de\s+([a-zç\.]+)\s+de\s+(\d{4})", s, flags=re.IGNORECASE)
    if m:
        d = int(m.group(1))
        mes_raw = m.group(2).replace(".", "").lower()
        mapa = {
            "jan":1,"janeiro":1,"fev":2,"fevereiro":2,"mar":3,"marco":3,"março":3,"abr":4,"abril":4,
            "mai":5,"maio":5,"jun":6,"junho":6,"jul":7,"julho":7,"ago":8,"agosto":8,"set":9,"setembro":9,
            "out":10,"outubro":10,"nov":11,"novembro":11,"dez":12,"dezembro":12
        }
        mes = mapa.get(mes_raw, 0)
        y = int(m.group(3))
        if 1 <= int(mes) <= 12:
            return dt.date(y, int(mes), d)
    return None

def sanitize_filename(s: str) -> str:
    s = re.sub(r"[^\w\-. ]", "_", s, flags=re.UNICODE)
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"_+", "_", s)
    return s.strip("._")[:180]

def build_output_name_orgao_tipo_num(orgao: str, tipo: str, numero: str) -> str:
    base = f"{orgao}_{tipo}_{only_digits(numero)}"
    return sanitize_filename(base) + ".pdf"

def load_state() -> Dict:
    try:
        if os.path.exists(STATE_JSON):
            with open(STATE_JSON, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {}

def save_state(state: Dict) -> None:
    ensure_dir(os.path.dirname(STATE_JSON))
    with open(STATE_JSON, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def state_has_triplet(state: Dict, key_norm: str) -> bool:
    return key_norm in state.setdefault("downloaded_keys", [])

def state_add_triplet(state: Dict, key_norm: str) -> None:
    arr = state.setdefault("downloaded_keys", [])
    if key_norm not in arr:
        arr.append(key_norm)

def file_already_present(dest_dir: str, base_filename: str) -> Optional[str]:
    base_stem = os.path.splitext(base_filename)[0]
    for f in os.listdir(dest_dir):
        if f.lower().endswith(".pdf") and f.lower().startswith(base_stem.lower()):
            return os.path.join(dest_dir, f)
    return None

# =========================
# ALIASES & CANONIZAÇÃO
# =========================
ALIASES_ORGAO_RAW = {
    "BANCO NACIONAL DE DESENVOLVIMENTO ECONOMICO E SOCIAL": ["BNDES"],
    "MINISTERIO DAS CIDADES": ["MCID", "MINISTERIO DA CIDADE"],
    "MINISTERIO DA FAZENDA": ["MF", "FAZENDA"],
    "MINISTERIO DA ECONOMIA": ["ME", "ECONOMIA"],
    "BANCO CENTRAL DO BRASIL": ["BCB", "BACEN", "BANCO CENTRAL"],
    "CONSELHO MONETARIO NACIONAL": ["CMN"]
}
ALIASES_ORGAO = {N(k): [N(v) for v in vs] for k, vs in ALIASES_ORGAO_RAW.items()}

STOPWORDS_SIGLA = {"DE","DA","DO","DAS","DOS","E"}

def acronym_norm(s: str) -> str:
    toks = [t for t in re.split(r"[^\w]+", N(s)) if t and t not in STOPWORDS_SIGLA]
    return "".join(t[0] for t in toks)[:12]

def canon_orgao(s: str) -> str:
    sN = N(s)
    # se bater com key ou alias conhecido, devolve a key canônica
    for key, aliases in ALIASES_ORGAO.items():
        if key in sN or sN in key:
            return key
        if any(a in sN for a in aliases):
            return key
    # se não reconheceu, usa o próprio normalizado (ou seu acrônimo se pequeno)
    return sN

def tipo_main_token(s: str) -> str:
    t = N(s)
    return re.split(r"[^\w]+", t)[0] if t else ""

def build_match_key(orgao: str, tipo: str, numero: str) -> str:
    org = canon_orgao(orgao)
    tip = tipo_main_token(tipo)
    num = only_digits(numero)
    # para a chave, usamos sublinhado:
    return f"{org}_{tip}_{num}"

# =========================
# OUTLOOK
# =========================
def open_outlook_folder_by_path(path_str: str):
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    parts = [p for p in path_str.split("\\") if p.strip()]
    if not parts:
        return ns.GetDefaultFolder(6)  # Inbox

    root = None
    first = parts[0]
    for i in range(1, ns.Folders.Count + 1):
        store = ns.Folders.Item(i)
        if str(store.Name).strip().lower() == first.strip().lower():
            root = store
            parts = parts[1:]
            break

    if root is None:
        root = ns.GetDefaultFolder(6)
        if first.strip().lower() in ["inbox", "caixa de entrada"]:
            parts = parts[1:]

    folder = root
    for p in parts:
        folder = folder.Folders.Item(p)
    return folder

def restrict_by_date_and_subject(items, data_inicio_dt: dt.datetime, data_fim_dt: dt.datetime) -> list:
    di = data_inicio_dt.strftime("%m/%d/%Y")
    df = data_fim_dt.strftime("%m/%d/%Y")
    filtro_data = f"[ReceivedTime] >= '{di} 00:00 AM' AND [ReceivedTime] <= '{df} 11:59 PM'"
    candidates = list(items.Restrict(filtro_data))

    out = []
    for mail in candidates:
        try:
            subj = str(getattr(mail, "Subject", "") or "")
            from_addr = str(getattr(mail, "SenderEmailAddress", "") or "")
            if REMETENTE_FILTRO.lower() not in from_addr.lower():
                continue
            if not subj.startswith(ASSUNTO_FIXO):
                continue
            data_str = subj.replace(ASSUNTO_FIXO, "").strip()
            try:
                data_no_assunto = dt.datetime.strptime(data_str, "%d/%m/%Y")
            except Exception:
                continue
            if data_inicio_dt <= data_no_assunto <= data_fim_dt:
                out.append(mail)
        except Exception:
            continue
    return out

# =========================
# EMAIL HTML -> DATAFRAME CANÔNICO
# =========================
COLS_CANON = ["Tipo", "Número", "Data do Ato", "Órgão", "Ementa", "Vigência"]

HEADER_VARIANTS = {
    "Órgão": {N("Órgão"), N("Órgão regulador"), N("Orgao"), N("Orgao Regulador"), N("Orgão Regulador")},
    "Tipo": {N("Tipo"), N("Tipo da Norma"), N("Espécie"), N("Especie")},
    "Número": {N("Número"), N("Numero"), N("Nº"), N("No"), N("N."), N("N")},
    "Data do Ato": {N("Data do Ato"), N("Data do Normativo"), N("Data"), N("Publicação"), N("Data Publicação")},
    "Ementa": {N("Ementa"), N("Assunto"), N("Resumo"), N("Descrição"), N("Descricao")},
    "Vigência": {N("Vigência"), N("Vigencia")},
}

FORWARD_MARKERS = re.compile(
    r"(mensagem encaminhada|encaminhada|enc:|fw:|forwarded message|-----+ original message -----+)",
    re.I
)

def _row_cells(tr) -> list[str]:
    tds = tr.find_all(["td", "th"])
    return [td.get_text(" ", strip=True) for td in tds]

def _header_mapping(cells_norm: list[str]) -> Dict[int, str]:
    idx_to_key = {}
    for i, txt in enumerate(cells_norm):
        for canon, variants in HEADER_VARIANTS.items():
            if txt in variants and canon not in idx_to_key.values():
                idx_to_key[i] = canon
                break
    return idx_to_key

def _scan_table_for_header(table) -> Optional[Tuple[Dict[int, str], int, list]]:
    trs = table.find_all("tr")
    if not trs:
        return None
    best = None
    for idx, tr in enumerate(trs):
        cells = _row_cells(tr)
        if not cells:
            continue
        cells_norm = [N(c) for c in cells]
        mapping = _header_mapping(cells_norm)
        keys = set(mapping.values())
        if ("Número" in keys) and (("Tipo" in keys) or ("Órgão" in keys)) and len(keys) >= 4:
            score = (len(keys),)
            if (best is None) or (score > best[0]):
                best = (score, mapping, idx, trs)
    if best is None:
        return None
    _, mapping, idx, trs = best
    return mapping, idx, trs

def extract_df_from_email_html(html: str) -> pd.DataFrame:
    if not html:
        return pd.DataFrame(columns=COLS_CANON)

    cut_html = html
    m = FORWARD_MARKERS.search(html)
    if m:
        cut_html = html[:m.start()]

    soup = BeautifulSoup(cut_html, "lxml")
    candidates: list[tuple[Tuple[int, int], pd.DataFrame]] = []

    for table in soup.find_all("table"):
        header_scan = _scan_table_for_header(table)
        if not header_scan:
            continue
        idx_to_key, idx_header, trs = header_scan

        records = []
        for tr in trs[idx_header+1:]:
            cells = _row_cells(tr)
            if not cells:
                continue
            row = {k: "" for k in COLS_CANON}
            for i, canon in idx_to_key.items():
                if i < len(cells):
                    row[canon] = cells[i]
            if only_digits(row.get("Número", "")):
                records.append(row)

        if not records:
            continue

        df_tmp = pd.DataFrame.from_records(records, columns=COLS_CANON)
        for c in COLS_CANON:
            df_tmp[c] = df_tmp[c].fillna("").astype(str).str.replace("\xa0", " ", regex=False).str.strip()

        mask_headerish = df_tmp["Número"].map(lambda x: N(x) in HEADER_VARIANTS["Número"])
        df_tmp = df_tmp[~mask_headerish].copy()

        df_tmp["N_NUM"] = df_tmp["Número"].map(only_digits)
        df_tmp["N_ORGAO"] = df_tmp["Órgão"].map(canon_orgao)
        df_tmp["N_TIPO"]  = df_tmp["Tipo"].map(tipo_main_token)
        df_tmp["CHAVE"]   = df_tmp.apply(lambda r: f"{r['N_ORGAO']}_{r['N_TIPO']}_{r['N_NUM']}", axis=1)

        useful_cols = sum(df_tmp[c].notna().any() for c in ["Tipo","Número","Órgão"])
        score = (useful_cols, len(df_tmp))
        candidates.append((score, df_tmp))

    if not candidates:
        return pd.DataFrame(columns=COLS_CANON + ["N_ORGAO","N_TIPO","N_NUM","CHAVE"])

    candidates.sort(key=lambda t: t[0], reverse=True)
    best_df = candidates[0][1].copy()
    return best_df.reset_index(drop=True)

# =========================
# METAS (EXCEL)
# =========================
def load_metas_df(path: str) -> pd.DataFrame:
    df = pd.read_excel(path)
    colmap = {N(c): c for c in df.columns}
    def find(*cands):
        for c in cands:
            if N(c) in colmap:
                return colmap[N(c)]
        return None

    c_org = find("Órgão regulador","Órgão","Orgao","Orgão Regulador","Orgao Regulador")
    c_tipo= find("Tipo","Tipo da Norma","Espécie","Especie")
    c_num = find("Número","Numero","Nº","No","N.","N")
    c_data= find("Data do Normativo","Data do Ato","Data","Publicação","Data Publicação")

    miss = [n for n,v in [("Órgão",c_org),("Tipo",c_tipo),("Número",c_num)] if v is None]
    if miss:
        raise RuntimeError(f"Colunas ausentes na planilha de metas: {miss}")

    metas = pd.DataFrame({
        "Órgão": df[c_org].astype(str),
        "Tipo":  df[c_tipo].astype(str),
        "Número":df[c_num].astype(str),
        "Data do Ato": (df[c_data].astype(str) if c_data else ""),
    })
    metas["N_NUM"]   = metas["Número"].map(only_digits)
    metas["N_ORGAO"] = metas["Órgão"].map(canon_orgao)
    metas["N_TIPO"]  = metas["Tipo"].map(tipo_main_token)
    metas["CHAVE"]   = metas.apply(lambda r: f"{r['N_ORGAO']}_{r['N_TIPO']}_{r['N_NUM']}", axis=1)

    # colunas de saída
    metas["PDF"] = ""
    metas["Caminho"] = ""

    return metas

# =========================
# ANEXOS / DOWNLOAD
# =========================
def score_attachment_for_meta(filename: str, orgao: str, tipo: str, nnum: str) -> int:
    nameN = N(filename)
    score = 0
    if nnum and nnum in nameN:
        score += 100
    first_tipo = tipo_main_token(tipo)
    if first_tipo and first_tipo in nameN:
        score += 20
    canon = canon_orgao(orgao)
    if canon in nameN:
        score += 10
    for a in ALIASES_ORGAO.get(canon, []):
        if a in nameN:
            score += 10
    return score

def find_all_pdf_attachments(mail) -> List[Tuple[str, object]]:
    out = []
    try:
        cnt = int(getattr(mail.Attachments, "Count", 0))
        for i in range(1, cnt + 1):
            att = mail.Attachments.Item(i)
            name = str(getattr(att, "FileName", "") or "")
            if name.lower().endswith(".pdf"):
                out.append((name, att))
    except Exception as e:
        logging.warning(f"Erro ao ler anexos: {e}")
    return out

def find_pdf_links_in_html(html: str, number_digits: str) -> List[str]:
    links = []
    if not html:
        return links
    soup = BeautifulSoup(html, "lxml")
    for a in soup.find_all("a"):
        href = (a.get("href") or "").strip()
        if href.lower().endswith(".pdf") and number_digits in N(href):
            links.append(href)
    return links

def download_link(url: str, dest_path: str) -> bool:
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return False
        with open(dest_path, "wb") as f:
            f.write(r.content)
        return True
    except Exception as e:
        logging.warning(f"Falha no download {url}: {e}")
        return False

# =========================
# PIPELINE
# =========================
def main():
    ensure_dir(PASTA_DESTINO)
    state = load_state()

    # 1) Abrir e-mails elegíveis
    folder = open_outlook_folder_by_path(OUTLOOK_FOLDER_PATH)
    items = folder.Items
    items.Sort("[ReceivedTime]", True)

    di = dt.datetime.strptime(DATA_INICIO, "%d/%m/%Y")
    df_ = dt.datetime.strptime(DATA_FIM, "%d/%m/%Y")
    mails = restrict_by_date_and_subject(items, di, df_)
    logging.info(f"E-mails elegíveis: {len(mails)}")

    # 2) Extrair tabelas dos e-mails (com CHAVE) e manter por e-mail
    email_rows: List[pd.DataFrame] = []
    mail_index: Dict[str, object] = {}
    for mail in mails:
        entry_id = str(getattr(mail, "EntryID", ""))
        subj = str(getattr(mail, "Subject", "") or "")
        recd = getattr(mail, "ReceivedTime", None)
        html = str(getattr(mail, "HTMLBody", "") or "")

        df_email = extract_df_from_email_html(html)
        if df_email.empty:
            logging.info(f"[SKIP] Sem tabela reconhecida em: {subj}")
            continue

        df_email["EMAIL_ENTRY_ID"] = entry_id
        df_email["EMAIL_SUBJECT"] = subj
        df_email["EMAIL_RECEIVED"] = recd.strftime("%Y-%m-%d %H:%M:%S") if isinstance(recd, dt.datetime) else ""

        email_rows.append(df_email)
        mail_index[entry_id] = mail

    if not email_rows:
        logging.warning("Nenhuma tabela extraída dos e-mails.")
        return

    emails_df = pd.concat(email_rows, ignore_index=True)

    # 3) Carregar metas (com CHAVE) – será nossa planilha final
    metas_df = load_metas_df(ARQUIVO_EXCEL_METAS)

    # 4) JOIN por CHAVE (Órgão + Tipo + Número)
    base_join = emails_df.merge(
        metas_df[["CHAVE","Órgão","Tipo","Número"]],
        how="inner",
        on="CHAVE",
        suffixes=("", "_META")
    )

    if base_join.empty:
        logging.info("Nenhum match por CHAVE (Órgão+Tipo+Número).")
        # Mesmo sem match, ainda assim geramos a planilha de metas enriquecida (sem PDFs)
        out_path = os.path.join(os.path.dirname(ARQUIVO_EXCEL_METAS),
                                pathlib.Path(ARQUIVO_EXCEL_METAS).stem + "_com_PDF.xlsx")
        metas_df.to_excel(out_path, index=False)
        logging.info(f"Planilha gerada: {out_path}")
        return

    logging.info(f"Matches por CHAVE: {len(base_join)}")

    # 5) Para cada match, fazer o download (apenas 1 PDF por CHAVE)
    #    Nome do arquivo: Orgao_Tipo_Numero.pdf
    for key, grp in base_join.groupby("CHAVE"):
        # marcar na metas_df (PDF/Caminho)
        metas_idx = metas_df.index[metas_df["CHAVE"] == key]
        if metas_idx.empty:
            continue

        orgao = grp.iloc[0]["Órgão"]
        tipo  = grp.iloc[0]["Tipo"]
        num   = grp.iloc[0]["Número"]
        base_name = build_output_name_orgao_tipo_num(orgao, tipo, num)
        key_norm = N(key)

        # trava: já baixado (state) ou já existe arquivo com esse prefixo
        path_hit = file_already_present(PASTA_DESTINO, base_name)
        if state_has_triplet(state, key_norm) or path_hit:
            metas_df.loc[metas_idx, "PDF"] = "SIM"
            metas_df.loc[metas_idx, "Caminho"] = path_hit or os.path.join(PASTA_DESTINO, base_name)
            continue

        # escolhe e-mail mais novo para tentar baixar
        grp_sorted = grp.sort_values(by="EMAIL_RECEIVED", ascending=False)

        saved_path = ""
        for _, row in grp_sorted.iterrows():
            mail = mail_index.get(row["EMAIL_ENTRY_ID"])
            if mail is None:
                continue
            all_atts = find_all_pdf_attachments(mail)

            # 1º: tentar por anexo com número/tipo/órgão
            scored = []
            nnum = only_digits(num)
            for name, att in all_atts:
                sc = score_attachment_for_meta(name, orgao, tipo, nnum)
                if sc >= 100:  # exige número no nome
                    scored.append((sc, name, att))
            scored.sort(reverse=True)

            if scored:
                _, name, att = scored[0]
                dest = os.path.join(PASTA_DESTINO, base_name)
                k = 1
                while os.path.exists(dest):
                    dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                    k += 1
                att.SaveAsFile(dest)
                saved_path = dest
                break

            # 2º: fallback – tentar link com número
            html = str(getattr(mail, "HTMLBody", "") or "")
            for url in find_pdf_links_in_html(html, nnum):
                dest = os.path.join(PASTA_DESTINO, base_name)
                k = 1
                while os.path.exists(dest):
                    dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                    k += 1
                if download_link(url, dest):
                    saved_path = dest
                    break

            if saved_path:
                break

        if saved_path:
            metas_df.loc[metas_idx, "PDF"] = "SIM"
            metas_df.loc[metas_idx, "Caminho"] = saved_path
            state_add_triplet(state, key_norm)

    # 6) Gerar somente a planilha final (no mesmo caminho da metas)
    out_path = os.path.join(os.path.dirname(ARQUIVO_EXCEL_METAS),
                            pathlib.Path(ARQUIVO_EXCEL_METAS).stem + "_com_PDF.xlsx")
    # Ordena colunas: metas originais + PDF + Caminho
    cols_first = [c for c in ["Órgão","Tipo","Número","Data do Ato"] if c in metas_df.columns]
    cols_final = cols_first + [c for c in metas_df.columns if c not in cols_first and c not in {"N_NUM","N_ORGAO","N_TIPO","CHAVE"}]
    metas_df[cols_final].to_excel(out_path, index=False)
    save_state(state)
    logging.info(f"Planilha gerada: {out_path}")

# =========================
# RUN
# =========================
if __name__ == "__main__":
    main()
