# -*- coding: utf-8 -*-
import os
import re
import io
import csv
import json
import time
import glob
import hashlib
import logging
import pathlib
import gc
import datetime as dt
from typing import List, Dict, Optional, Tuple

import win32com.client as win32  # Outlook Desktop
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

# =========================
# CONFIG
# =========================
SENDER_EMAIL = "informe@mkcompliance.com.br"
SUBJECT_PREFIX = "[EXT] - Normativos Divulgados em "  # o assunto termina com dd/mm/yyyy

# >>>>> TESTE: edite a(s) data(s) abaixo (dd/mm/yyyy) e deixe ESTA linha ativa nos testes
SUBJECT_DATES = ["25/09/2025"]  # coloque a(s) data(s) que quer testar

# >>>>> PRODUÇÃO: comente a linha de cima e DESCOMENTE esta para pegar HOJE e os 2 dias anteriores
# SUBJECT_DATES = [(dt.datetime.today() - dt.timedelta(days=i)).strftime("%d/%m/%Y") for i in range(0, 3)]

# Caminhos
EXCEL_METAS = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\3_BaseMK\BaseMkNormasParaAvaliar.xlsx"           # planilha com as metas (Órgão, Tipo, Número, Data do Ato)
SAVE_DIR = pathlib.Path(r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Normativo_Download_Outlook")   # pasta de rede de destino dos PDFs
LOG_CSV = pathlib.Path(r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\7_LogExecucao\log_baixas.csv")
LOG_XLSX = pathlib.Path(r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\7_LogExecucao\log_baixas.xlsx")
STATE_JSON = pathlib.Path(r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\7_LogExecucao\state.json")

# Opcional: filtros adicionais por órgão/tipo (todos normalizados/sem acento/upper).
# Deixe vazios para aceitar qualquer órgão/tipo da planilha.
ORGAOS_ALLOW = {"BACEN", "BCB", "BANCO CENTRAL DO BRASIL", "CVM", "ANBIMA", "CMN"}
TIPOS_ALLOW = {"INSTRUCAO NORMATIVA", "RESOLUCAO", "CIRCULAR", "CARTA CIRCULAR", "COMUNICADO", "PORTARIA"}

HTTP_TIMEOUT = 45  # s para download de PDF por link
LOG_LEVEL = logging.DEBUG # Alterado para DEBUG para exibir mais detalhes

# =========================
# LOG
# =========================
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")


# =========================
# HELPERS
# =========================
def ensure_dir(p: pathlib.Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def N(txt: str) -> str:
    """Normaliza: sem acentos + upper + collapse espaços."""
    import unicodedata
    txt = str(txt or "")
    txt = "".join(c for c in unicodedata.normalize("NFKD", txt) if not unicodedata.combining(c))
    return re.sub(r"\s+", " ", txt.upper().strip())

def only_digits(s: str) -> str:
    return re.sub(r"\D", "", str(s or ""))

def sanitize_filename(s: str) -> str:
    s = re.sub(r"[^\w\-. ]", "_", s, flags=re.UNICODE)
    s = re.sub(r"[_ ]{2,}", "_", s).strip("._ ")
    return s[:180]

def parse_date_flexible(s: str) -> Optional[dt.date]:
    if not s:
        return None
    s = str(s).strip()
    # dd/mm/yyyy ou yyyy-mm-dd resolvem direto:
    try:
        # tenta dayfirst
        return dtparser.parse(s, dayfirst=True).date()
    except Exception:
        pass
    # "26 de set de 2025" / "30 de agosto de 2024"
    m = re.search(r"(\d{1,2})\s+de\s+([a-zç\.]+)\s+de\s+(\d{4})", s, flags=re.IGNORECASE)
    if m:
        d = int(m.group(1))
        mes_raw = m.group(2).replace(".", "").lower()
        mapa = {
            "jan":1,"janeiro":1,"fev":2,"fevereiro":2,"mar":3,"marco":3,"março":3,"abr":4,"abril":4,
            "mai":5,"maio":5,"jun":6,"junho":6,"jul":7,"julho":7,"ago":8,"agosto":8,"set":9,"setembro":9,
            "out":10,"outubro":10,"nov":11,"novembro":11,"dez":12,"dezembro":12
        }
        mes = mapa.get(mes_raw)
        y = int(m.group(3))
        if 1 <= mes <= 12:
            return dt.date(y, mes, d)
    return None

def date_to_ddmmyyyy(d: dt.date) -> str:
    return d.strftime("%d/%m/%Y")

def build_expected_subjects() -> List[str]:
    return [f"{SUBJECT_PREFIX}{d}" for d in SUBJECT_DATES]

def load_state() -> Dict:
    if STATE_JSON.exists():
        try:
            return json.loads(STATE_JSON.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def save_state(state: Dict) -> None:
    ensure_dir(STATE_JSON.parent)
    STATE_JSON.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def load_targets_from_excel(path: str) -> pd.DataFrame:
    """
    Lê a planilha de metas e normaliza colunas:
      Órgão | Tipo | Número | Data do Ato
    Aceita variantes de cabeçalho.
    """
    # Define as colunas que precisamos ler da planilha.
    # Isso otimiza a leitura, evitando carregar dados desnecessários.
    cols_to_read = [
        "Órgão regulador",
        "Tipo",
        "Número",
        "Data do Normativo"
    ]
    df = pd.read_excel(path, usecols=cols_to_read)

    # normaliza nomes das colunas
    colmap = {N(c): c for c in df.columns}
    def colfind(cands: List[str]) -> Optional[str]:
        """Encontra o nome real da coluna a partir de uma lista de candidatos."""
        for c in cands:
            if N(c) in colmap:
                return colmap[N(c)]
        return None

    col_org = colfind(["Órgão regulador", "Órgão", "Orgao", "Orgão", "Orgão Regulador", "Orgao Regulador"])
    col_tipo = colfind(["Tipo", "Tipo da Norma", "Espécie", "Especie"])
    col_num  = colfind(["Número", "Numero", "Nº", "No", "N."])
    col_data = colfind(["Data do Normativo", "Data do Ato", "Data", "Publicação", "Data Publicação"])

    missing = [("Órgão", col_org), ("Tipo", col_tipo), ("Número", col_num), ("Data do Ato", col_data)]
    miss = [n for n, v in missing if v is None]
    if miss:
        raise RuntimeError(f"Colunas ausentes na planilha: {miss}")

    df2 = pd.DataFrame({
        "ORGAO": df[col_org].astype(str),
        "TIPO": df[col_tipo].astype(str),
        "NUMERO": df[col_num].astype(str),
        "DATA_ATO": df[col_data].astype(str),
    })

    # normalizados
    df2["N_ORGAO"] = df2["ORGAO"].map(N)
    df2["N_TIPO"]  = df2["TIPO"].map(N)
    df2["N_NUM"]   = df2["NUMERO"].map(lambda x: only_digits(x))
    df2["DATA_ATO_DT"] = df2["DATA_ATO"].map(parse_date_flexible)

    # filtros (opcionais)
    if ORGAOS_ALLOW:
        df2 = df2[df2["N_ORGAO"].apply(lambda x: any(k in x for k in ORGAOS_ALLOW))].copy()
    if TIPOS_ALLOW:
        df2 = df2[df2["N_TIPO"].apply(lambda x: any(k in x for k in TIPOS_ALLOW))].copy()

    df2.reset_index(drop=True, inplace=True)
    return df2

def html_tables_to_rows(html: str) -> List[Dict]:
    """Extrai linhas de todas as tabelas do HTML."""
    rows: List[Dict] = []
    if not html:
        return rows

    # 1) pandas.read_html
    try:
        dfs = pd.read_html(html)
    except ValueError:
        dfs = []

    def normalize_headers(df: pd.DataFrame) -> pd.DataFrame:
        hm = {
            "ORGAO": {"ÓRGÃO","ORGAO","ORGÃO","ÓRGÃO REGULADOR","ORGAO REGULADOR","ORG"},
            "TIPO": {"TIPO","TIPO DA NORMA","ESPÉCIE","ESPECIE"},
            "NUMERO": {"NÚMERO","NUMERO","Nº","N.","NO","N"},
            "DATA_ATO": {"DATA DO ATO","DATA","PUBLICAÇÃO","PUBLICACAO","DATA DA PUBLICAÇÃO"},
            "EMENTA": {"EMENTA","ASSUNTO","RESUMO","DESCRIÇÃO","DESCRICAO"},
            "LINK": {"LINK","URL","PDF","ARQUIVO"}
        }
        newcols = []
        for c in df.columns:
            cn = N(str(c))
            mapped = None
            for k, variants in hm.items():
                if cn in {N(v) for v in variants}:
                    mapped = k; break
            newcols.append(mapped or cn)
        df.columns = newcols
        return df

    for df in dfs:
        df = normalize_headers(df)
        for _, r in df.iterrows():
            rows.append({k: (None if pd.isna(v) else str(v)) for k, v in r.to_dict().items()})

    # 2) fallback BS4 <table>
    if not rows:
        soup = BeautifulSoup(html, "lxml")
        for table in soup.select("table"):
            # cabeçalho
            headers = []
            ths = table.select("thead th")
            if not ths:
                ths = table.select("tr th")
            if ths:
                headers = [N(th.get_text(" ", strip=True)) for th in ths]
            for tr in table.select("tr"):
                tds = tr.find_all("td")
                if not tds: continue
                vals = [td.get_text(" ", strip=True) for td in tds]
                if headers and len(headers) == len(vals):
                    row = {headers[i]: vals[i] for i in range(len(vals))}
                else:
                    row = {f"C{i+1}": v for i, v in enumerate(vals)}
                rows.append(row)

    return rows

def norm_row(row: Dict) -> Dict:
    """Normaliza uma linha vinda do HTML do e-mail."""
    def g(*keys):
        for k in keys:
            if k in row and row[k] is not None:
                return str(row[k])
        return ""
    org = g("ORGAO","ÓRGÃO","ORGÃO","ORG")
    tipo = g("TIPO","ESPÉCIE")
    num  = g("NUMERO","NÚMERO","Nº","NO","N")
    data = g("DATA_ATO","DATA","PUBLICAÇÃO")
    return {
        "ORGAO": org, "TIPO": tipo, "NUMERO": num, "DATA_ATO": data,
        "N_ORGAO": N(org), "N_TIPO": N(tipo), "N_NUM": only_digits(num),
        "DATA_ATO_DT": parse_date_flexible(data)
    }

def rows_from_message_body(html: str, plain: str) -> List[Dict]:
    rows = html_tables_to_rows(html or plain or "")
    return [norm_row(r) for r in rows if r]

def target_matches_email_row(tgt: pd.Series, r: Dict) -> bool:
    """Compara meta (planilha) com linha do e-mail (normalizada)."""
    tgt_num = str(tgt["N_NUM"])
    email_num = str(r["N_NUM"])
    tgt_orgao = tgt["N_ORGAO"]
    email_orgao = r["N_ORGAO"]
    tgt_tipo = tgt["N_TIPO"]
    email_tipo = r["N_TIPO"]

    # Número: exato em dígitos
    if tgt_num != email_num:
        logging.debug(f"  [MATCH FAIL] Número diferente: Meta='{tgt_num}' vs Email='{email_num}'")
        return False
    # Órgão: contém (permite “BANCO CENTRAL DO BRASIL” vs “BACEN”)
    if not any(k in email_orgao for k in [tgt_orgao, "BACEN","BCB","BANCO CENTRAL"]):
        logging.debug(f"  [MATCH FAIL] Órgão não corresponde: Meta='{tgt_orgao}' vs Email='{email_orgao}'")
        return False
    # Tipo: contém (INSTRUÇÃO NORMATIVA pode aparecer “INSTRUÇÃO NORMATIVA BCB”)
    if not (tgt_tipo in email_tipo or email_tipo in tgt_tipo):
        logging.debug(f"  [MATCH FAIL] Tipo não corresponde: Meta='{tgt_tipo}' vs Email='{email_tipo}'")
        return False
    # Data do ato: igualdade tolerante (até +/- 2 dias se parse ok)
    td, rd = tgt["DATA_ATO_DT"], r["DATA_ATO_DT"]
    if td and rd and abs((td - rd).days) > 2:
        logging.debug(f"  [MATCH FAIL] Data do ato fora da tolerância: Meta='{td}' vs Email='{rd}'")
        return False
    return True

def find_pdf_attachments(mail, number_digits: str, tipo_norm: str) -> List[Tuple[str, bytes]]:
    """Seleciona anexos PDF cujo nome bate com o número e (preferencial) com o tipo."""
    found = []
    for att in list(mail.Attachments):
        name = str(att.FileName or "")
        if not name.lower().endswith(".pdf"):
            continue
        n_ok = number_digits in N(name)
        t_ok = N(tipo_norm).split()[0] in N(name)  # primeira palavra do tipo
        if n_ok and t_ok:
            found.append((name, att))
    # fallback: só por número
    if not found:
        for att in list(mail.Attachments):
            name = str(att.FileName or "")
            if name.lower().endswith(".pdf") and number_digits in N(name):
                found.append((name, att))
    # baixa bytes só quando for salvar (aqui retorna o objeto)
    return found

def find_pdf_links_in_body(html: str, number_digits: str) -> List[str]:
    links = []
    if not html:
        return links
    soup = BeautifulSoup(html, "lxml")
    for a in soup.find_all("a"):
        href = a.get("href") or ""
        if href.lower().endswith(".pdf") and number_digits in N(href):
            links.append(href)
    return links

def download_link(url: str, dest: pathlib.Path) -> bool:
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code >= 400: return False
        dest.write_bytes(r.content)
        return True
    except Exception as e:
        logging.warning(f"Falha download {url}: {e}")
        return False

def build_output_name(orgao: str, tipo: str, numero: str, data_ato: Optional[dt.date], fallback: str) -> str:
    d = data_ato.strftime("%Y-%m-%d") if isinstance(data_ato, dt.date) else (fallback or dt.date.today().isoformat())
    base = f"{d}_{orgao}_{tipo}_{only_digits(numero)}.pdf"
    return sanitize_filename(base)

def already_logged(state: Dict, entry_id: str, number_digits: str) -> bool:
    keys = state.setdefault("downloaded", [])
    key = f"{entry_id}:{number_digits}"
    return key in keys

def mark_logged(state: Dict, entry_id: str, number_digits: str):
    keys = state.setdefault("downloaded", [])
    key = f"{entry_id}:{number_digits}"
    if key not in keys:
        keys.append(key)

# =========================
# OUTLOOK
# =========================
def open_inbox():
    outlook = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    inbox = outlook.GetDefaultFolder(6)  # 6 = Inbox
    return inbox

def iter_candidate_messages(inbox, subjects: List[str]) -> List:
    """Percorre do mais novo ao mais antigo e retorna só os que batem remetente + assunto esperado."""
    items = inbox.Items
    items.Sort("[ReceivedTime]", True)
    # Normaliza os assuntos esperados para uma busca mais flexível
    norm_subjects = {N(s) for s in subjects}
    out = []
    for mail in items:
        try:
            subj = str(getattr(mail, "Subject", ""))
            from_addr = str(getattr(mail, "SenderEmailAddress", "") or "")
            norm_subj = N(subj)
            norm_from = N(from_addr)

            # Verifica se o remetente é o correto e se o assunto contém um dos assuntos esperados
            if norm_from.endswith(N(SENDER_EMAIL)) and any(expected_subj in norm_subj for expected_subj in norm_subjects):
                    out.append(mail)

            # early stop (se passou de 15 dias atrás, pare)
            rec = getattr(mail, "ReceivedTime", None)
            if isinstance(rec, dt.datetime) and (dt.datetime.now() - rec).days > 15:
                break
        except Exception:
            continue
    return out

# =========================
# PIPELINE
# =========================
def main():
    logging.info("="*20 + " INICIANDO EXECUÇÃO " + "="*20)
    ensure_dir(SAVE_DIR)
    ensure_dir(LOG_CSV.parent)

    # 1) metas
    logging.info(f"1. Carregando metas da planilha: {EXCEL_METAS}")
    metas = load_targets_from_excel(EXCEL_METAS)
    if metas.empty:
        logging.info("Planilha de metas vazia após filtros.")
        return

    # 2) Outlook: localizar mensagens candidatas
    inbox = open_inbox()
    subjects = build_expected_subjects()
    mails = iter_candidate_messages(inbox, subjects)
    logging.info(f"E-mails candidatos: {len(mails)}")

    state = load_state()
    log_rows: List[Dict] = []

    # 3) para cada e-mail, extrair tabela e cruzar
    for mail in mails:
        entry_id = str(getattr(mail, "EntryID", ""))
        mail_identifier = f"ID:{entry_id[:15]}..."
        subj = str(getattr(mail, "Subject", ""))
        recd = getattr(mail, "ReceivedTime", None)
        recd_iso = recd.strftime("%Y-%m-%d %H:%M:%S") if isinstance(recd, dt.datetime) else ""
        html = str(getattr(mail, "HTMLBody", "") or "")
        plain = str(getattr(mail, "Body", "") or "")

        email_rows = rows_from_message_body(html, plain)
        logging.info(f"\n--- Processando e-mail: '{subj}' (Recebido: {recd_iso}) ---")
        if not email_rows:
            logging.warning(f"Nenhuma tabela de normativos encontrada no corpo do e-mail.")
            continue
        logging.debug(f"Encontradas {len(email_rows)} linhas na tabela do e-mail.")

        # 4) tentar cada meta contra as linhas do e-mail
        for i, tgt in metas.iterrows():
            number_digits = str(tgt["N_NUM"])
            meta_identifier = f"Meta(Órgão='{tgt['ORGAO']}', Tipo='{tgt['TIPO']}', Num='{tgt['NUMERO']}')"
            logging.debug(f"Verificando {meta_identifier}")
            # pular se já baixado este número para este e-mail
            if already_logged(state, entry_id, number_digits):
                continue

            # match em uma das linhas do e-mail
            matched_row = None
            for r in email_rows:
                if target_matches_email_row(tgt, r):
                    matched_row = r
                    break
            if not matched_row:
                logging.debug(f"  -> Meta não encontrada neste e-mail.")
                continue  # esta meta não aparece neste e-mail

            logging.info(f"  [MATCH SUCESSO] {meta_identifier} corresponde a uma linha no e-mail.")
            # 5) procurar PDF — anexos primeiro
            logging.debug("  Procurando por anexos PDF correspondentes...")
            attachments = find_pdf_attachments(mail, number_digits, str(tgt["TIPO"]))
            saved_paths = []
            if attachments:
                logging.debug(f"    -> Encontrados {len(attachments)} anexos PDF.")
                for name, att in attachments:
                    outname = build_output_name(
                        orgao=str(tgt["ORGAO"]),
                        tipo=str(tgt["TIPO"]),
                        numero=str(tgt["NUMERO"]),
                        data_ato=tgt["DATA_ATO_DT"],
                        fallback=recd.date().isoformat() if isinstance(recd, dt.datetime) else None
                    )
                    dest = SAVE_DIR / outname
                    k = 1
                    while dest.exists():
                        dest = SAVE_DIR / sanitize_filename(outname.replace(".pdf", f"_{k}.pdf"))
                        k += 1
                    att.SaveAsFile(str(dest))
                    logging.info(f"    -> Anexo salvo em: {dest}")
                    saved_paths.append(str(dest))

            # 6) se não teve anexo, procurar link .pdf no corpo
            if not saved_paths:
                logging.debug("  Nenhum anexo encontrado. Procurando por links PDF no corpo do e-mail...")
                links = find_pdf_links_in_body(html, number_digits)
                logging.debug(f"    -> Encontrados {len(links)} links PDF.")
                for url in links:
                    outname = build_output_name(
                        orgao=str(tgt["ORGAO"]),
                        tipo=str(tgt["TIPO"]),
                        numero=str(tgt["NUMERO"]),
                        data_ato=tgt["DATA_ATO_DT"],
                        fallback=recd.date().isoformat() if isinstance(recd, dt.datetime) else None
                    )
                    dest = SAVE_DIR / outname
                    k = 1
                    while dest.exists():
                        dest = SAVE_DIR / sanitize_filename(outname.replace(".pdf", f"_{k}.pdf"))
                        k += 1
                    if download_link(url, dest):
                        logging.info(f"    -> Link baixado e salvo em: {dest}")
                        saved_paths.append(str(dest))

            status = "BAIXADO" if saved_paths else "NAO_ENCONTRADO_PDF"

            if saved_paths:
                mark_logged(state, entry_id, number_digits)

            # 7) log
            logging.debug("  Adicionando resultado ao log.")
            log_rows.append({
                "email_entry_id": entry_id,
                "email_subject": subj,
                "email_received": recd_iso,
                "meta_orgao": str(tgt["ORGAO"]),
                "meta_tipo": str(tgt["TIPO"]),
                "meta_numero": str(tgt["NUMERO"]),
                "meta_data_ato": str(tgt["DATA_ATO"]),
                "match_row_orgao": matched_row.get("ORGAO") if matched_row else None,
                "match_row_tipo": matched_row.get("TIPO") if matched_row else None,
                "match_row_numero": matched_row.get("NUMERO") if matched_row else None,
                "match_row_data_ato": matched_row.get("DATA_ATO") if matched_row else None,
                "arquivos_salvos": ";".join(saved_paths) if saved_paths else "",
                "status": status,
            })

    # 8) persistir logs e estado
    logging.info("\n--- Finalizando e salvando resultados ---")
    if log_rows:
        ensure_dir(LOG_CSV.parent)
        df_log = pd.DataFrame(log_rows)

        # 1. Salva em CSV (modo append)
        header = not LOG_CSV.exists()
        df_log.to_csv(LOG_CSV, mode='a', header=header, index=False, encoding="utf-8-sig")

        # 2. Salva em Excel (lendo o existente e concatenando)
        try:
            if LOG_XLSX.exists():
                # Se o Excel já existe, lê, adiciona as novas linhas e salva
                df_existente = pd.read_excel(LOG_XLSX)
                df_final_excel = pd.concat([df_existente, df_log], ignore_index=True)
            else:
                # Se não existe, o DataFrame final é apenas o log atual
                df_final_excel = df_log

            # Remove duplicatas baseadas em um conjunto de colunas chave para garantir a integridade
            colunas_chave = ["email_entry_id", "meta_orgao", "meta_tipo", "meta_numero"]
            df_final_excel.drop_duplicates(subset=colunas_chave, keep='last', inplace=True)

            df_final_excel.to_excel(LOG_XLSX, index=False)
        except Exception as e:
            logging.error(f"Falha ao escrever o log em Excel ({LOG_XLSX}): {e}")

        logging.info(f"-> Log salvo em {LOG_CSV} e {LOG_XLSX} ({len(log_rows)} novas entradas).")
    else:
        logging.info("-> Nenhuma nova meta foi encontrada e baixada nos e-mails candidatos.")

    # Libera objetos COM e força a coleta de lixo
    gc.collect()
    save_state(state)
    logging.info("="*20 + " EXECUÇÃO CONCLUÍDA " + "="*20)

# =========================
# EXECUÇÃO
# =========================
if __name__ == "__main__":
    main()
