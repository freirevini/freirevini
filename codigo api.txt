# -*- coding: utf-8 -*-
import os
import re
import json
import logging
import pathlib
import gc
import datetime as dt
from typing import List, Dict, Optional, Tuple

import win32com.client as win32  # Outlook Desktop
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

# =========================
# CONFIG
# =========================
SENDER_EMAIL = "informe@mkcompliance.com.br"
SUBJECT_PREFIX = "[EXT] - Normativos Divulgados em "  # o assunto termina com dd/mm/yyyy

# >>>>> TESTE: edite a(s) data(s) abaixo (dd/mm/yyyy) e deixe ESTA linha ativa nos testes
SUBJECT_DATES = ["25/09/2025"]  # coloque a(s) data(s) que quer testar

# >>>>> PRODUÇÃO: comente a linha acima e DESCOMENTE esta para pegar HOJE e os 2 dias anteriores
# SUBJECT_DATES = [(dt.datetime.today() - dt.timedelta(days=i)).strftime("%d/%m/%Y") for i in range(0, 3)]

# PASTA NO OUTLOOK (regra move os e-mails para cá)
OUTLOOK_FOLDER_PATH = r"Caixa de Entrada\Normas MK Compliance"
# Exemplos alternativos:
# OUTLOOK_FOLDER_PATH = r"Inbox\Normas MK Compliance"
# OUTLOOK_FOLDER_PATH = r"seu.email@empresa.com.br\Inbox\Normas MK Compliance"

# Caminhos de arquivos/pastas locais/rede
EXCEL_METAS = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\3_BaseMK\BaseMkNormasParaAvaliar.xlsx"
SAVE_DIR = pathlib.Path(r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Normativo_Download_Outlook")
LOG_CSV = pathlib.Path(r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\7_LogExecucao\log_baixas.csv")
LOG_XLSX = pathlib.Path(r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\7_LogExecucao\log_baixas.xlsx")
STATE_JSON = pathlib.Path(r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\7_LogExecucao\state.json")

# Filtros opcionais (pode deixar set() para não filtrar por aqui)
ORGAOS_ALLOW = {"BACEN", "BCB", "BANCO CENTRAL DO BRASIL", "CVM", "ANBIMA", "CMN"}
TIPOS_ALLOW = {"INSTRUCAO NORMATIVA", "RESOLUCAO", "CIRCULAR", "CARTA CIRCULAR", "COMUNICADO", "PORTARIA"}

HTTP_TIMEOUT = 45
LOG_LEVEL = logging.DEBUG  # DEBUG para ver o que está acontecendo

# =========================
# LOG
# =========================
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")

# =========================
# HELPERS
# =========================
def ensure_dir(p: pathlib.Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def N(txt: str) -> str:
    """Normaliza: sem acentos + upper + collapse espaços."""
    import unicodedata
    txt = str(txt or "")
    txt = "".join(c for c in unicodedata.normalize("NFKD", txt) if not unicodedata.combining(c))
    return re.sub(r"\s+", " ", txt.upper().strip())

def only_digits(s: str) -> str:
    return re.sub(r"\D", "", str(s or ""))

def sanitize_filename(s: str) -> str:
    s = re.sub(r"[^\w\-. ]", "_", s, flags=re.UNICODE)
    s = re.sub(r"[_ ]{2,}", "_", s).strip("._ ")
    return s[:180]

def parse_date_flexible(s: str) -> Optional[dt.date]:
    """Robusto para dd/mm/yyyy, yyyy-mm-dd, '26 de set de 2025', etc."""
    if not s:
        return None
    s = str(s).strip()
    try:
        return dtparser.parse(s, dayfirst=True).date()
    except Exception:
        pass
    m = re.search(r"(\d{1,2})\s+de\s+([a-zç\.]+)\s+de\s+(\d{4})", s, flags=re.IGNORECASE)
    if m:
        d = int(m.group(1))
        mes_raw = m.group(2).replace(".", "").lower()
        mapa = {
            "jan":1,"janeiro":1,"fev":2,"fevereiro":2,"mar":3,"marco":3,"março":3,"abr":4,"abril":4,
            "mai":5,"maio":5,"jun":6,"junho":6,"jul":7,"julho":7,"ago":8,"agosto":8,"set":9,"setembro":9,
            "out":10,"outubro":10,"nov":11,"novembro":11,"dez":12,"dezembro":12
        }
        mes = mapa.get(mes_raw)
        try:
            mes = int(mes)
        except Exception:
            mes = 0
        y = int(m.group(3))
        if 1 <= mes <= 12:
            return dt.date(y, mes, d)
    return None

def build_expected_subjects() -> List[str]:
    return [f"{SUBJECT_PREFIX}{d}" for d in SUBJECT_DATES]

def load_state() -> Dict:
    if STATE_JSON.exists():
        try:
            return json.loads(STATE_JSON.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def save_state(state: Dict) -> None:
    ensure_dir(STATE_JSON.parent)
    STATE_JSON.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def _read_excel_any(path: str) -> pd.DataFrame:
    """Tenta ler com usecols; se falhar, faz fallback p/ ler tudo."""
    try:
        cols_to_read = ["Órgão regulador", "Tipo", "Número", "Data do Normativo"]
        logging.debug(f"Lendo Excel com usecols={cols_to_read}")
        return pd.read_excel(path, usecols=cols_to_read)
    except Exception as e:
        logging.warning(f"usecols não bateu ({e}). Lendo planilha completa…")
        return pd.read_excel(path)

def load_targets_from_excel(path: str) -> pd.DataFrame:
    df = _read_excel_any(path)
    logging.info(f"Planilha carregada: {df.shape[0]} linhas, colunas: {list(df.columns)}")

    colmap = {N(c): c for c in df.columns}
    def colfind(cands: List[str]) -> Optional[str]:
        for c in cands:
            if N(c) in colmap:
                return colmap[N(c)]
        return None

    col_org = colfind(["Órgão regulador","Órgão","Orgao","Orgão","Orgão Regulador","Orgao Regulador"])
    col_tipo = colfind(["Tipo","Tipo da Norma","Espécie","Especie"])
    col_num  = colfind(["Número","Numero","Nº","No","N.","N"])
    col_data = colfind(["Data do Normativo","Data do Ato","Data","Publicação","Data Publicação"])

    missing = [("Órgão", col_org), ("Tipo", col_tipo), ("Número", col_num), ("Data do Ato", col_data)]
    miss = [n for n, v in missing if v is None]
    if miss:
        raise RuntimeError(f"Colunas ausentes na planilha: {miss}")

    df2 = pd.DataFrame({
        "ORGAO": df[col_org].astype(str),
        "TIPO": df[col_tipo].astype(str),
        "NUMERO": df[col_num].astype(str),
        "DATA_ATO": df[col_data].astype(str),
    })

    df2["N_ORGAO"] = df2["ORGAO"].map(N)
    df2["N_TIPO"]  = df2["TIPO"].map(N)
    df2["N_NUM"]   = df2["NUMERO"].map(only_digits)
    df2["DATA_ATO_DT"] = df2["DATA_ATO"].map(parse_date_flexible)

    before = len(df2)
    if ORGAOS_ALLOW:
        df2 = df2[df2["N_ORGAO"].apply(lambda x: any(k in x for k in ORGAOS_ALLOW))].copy()
    if TIPOS_ALLOW:
        df2 = df2[df2["N_TIPO"].apply(lambda x: any(k in x for k in TIPOS_ALLOW))].copy()
    logging.info(f"Metas após filtros: {len(df2)} (antes: {before})")

    df2.reset_index(drop=True, inplace=True)
    logging.debug(f"Exemplo metas:\n{df2.head(3)}")
    return df2

def html_tables_to_rows(html: str) -> List[Dict]:
    rows: List[Dict] = []
    if not html:
        return rows
    try:
        dfs = pd.read_html(html)
    except ValueError:
        dfs = []

    def normalize_headers(df: pd.DataFrame) -> pd.DataFrame:
        hm = {
            "ORGAO": {"ÓRGÃO","ORGAO","ORGÃO","ÓRGÃO REGULADOR","ORGAO REGULADOR","ORG"},
            "TIPO": {"TIPO","TIPO DA NORMA","ESPÉCIE","ESPECIE"},
            "NUMERO": {"NÚMERO","NUMERO","Nº","N.","NO","N"},
            "DATA_ATO": {"DATA DO ATO","DATA","PUBLICAÇÃO","PUBLICACAO","DATA DA PUBLICAÇÃO"},
            "EMENTA": {"EMENTA","ASSUNTO","RESUMO","DESCRIÇÃO","DESCRICAO"},
            "LINK": {"LINK","URL","PDF","ARQUIVO"}
        }
        newcols = []
        for c in df.columns:
            cn = N(str(c))
            mapped = None
            for k, variants in hm.items():
                if cn in {N(v) for v in variants}:
                    mapped = k; break
            newcols.append(mapped or cn)
        df.columns = newcols
        return df

    for df in dfs:
        df = normalize_headers(df)
        for _, r in df.iterrows():
            rows.append({k: (None if pd.isna(v) else str(v)) for k, v in r.to_dict().items()})

    if not rows:
        soup = BeautifulSoup(html, "lxml")
        for table in soup.select("table"):
            headers = []
            ths = table.select("thead th") or table.select("tr th")
            if ths:
                headers = [N(th.get_text(" ", strip=True)) for th in ths]
            for tr in table.select("tr"):
                tds = tr.find_all("td")
                if not tds:
                    continue
                vals = [td.get_text(" ", strip=True) for td in tds]
                if headers and len(headers) == len(vals):
                    row = {headers[i]: vals[i] for i in range(len(vals))}
                else:
                    row = {f"C{i+1}": v for i, v in enumerate(vals)}
                rows.append(row)
    return rows

def norm_row(row: Dict) -> Dict:
    def g(*keys):
        for k in keys:
            if k in row and row[k] is not None:
                return str(row[k])
        return ""
    org = g("ORGAO","ÓRGÃO","ORGÃO","ORG")
    tipo = g("TIPO","ESPÉCIE")
    num  = g("NUMERO","NÚMERO","Nº","NO","N")
    data = g("DATA_ATO","DATA","PUBLICAÇÃO")
    return {
        "ORGAO": org, "TIPO": tipo, "NUMERO": num, "DATA_ATO": data,
        "N_ORGAO": N(org), "N_TIPO": N(tipo), "N_NUM": only_digits(num),
        "DATA_ATO_DT": parse_date_flexible(data)
    }

def rows_from_message_body(html: str, plain: str) -> List[Dict]:
    rows = html_tables_to_rows(html or plain or "")
    return [norm_row(r) for r in rows if r]

def target_matches_email_row(tgt: pd.Series, r: Dict) -> bool:
    tgt_num = str(tgt["N_NUM"])
    email_num = str(r["N_NUM"])
    tgt_orgao = tgt["N_ORGAO"]
    email_orgao = r["N_ORGAO"]
    tgt_tipo = tgt["N_TIPO"]
    email_tipo = r["N_TIPO"]

    if tgt_num != email_num:
        logging.debug(f"  [MATCH FAIL] Número: Meta='{tgt_num}' vs Email='{email_num}'")
        return False
    if not any(k in email_orgao for k in [tgt_orgao, "BACEN","BCB","BANCO CENTRAL"]):
        logging.debug(f"  [MATCH FAIL] Órgão: Meta='{tgt_orgao}' vs Email='{email_orgao}'")
        return False
    if not (tgt_tipo in email_tipo or email_tipo in tgt_tipo):
        logging.debug(f"  [MATCH FAIL] Tipo: Meta='{tgt_tipo}' vs Email='{email_tipo}'")
        return False
    td, rd = tgt["DATA_ATO_DT"], r["DATA_ATO_DT"]
    if td and rd and abs((td - rd).days) > 2:
        logging.debug(f"  [MATCH FAIL] Data do ato: Meta='{td}' vs Email='{rd}'")
        return False
    return True

def find_pdf_attachments(mail, number_digits: str, tipo_norm: str) -> List[Tuple[str, object]]:
    found = []
    try:
        cnt = int(getattr(mail.Attachments, "Count", 0))
        for i in range(1, cnt + 1):
            att = mail.Attachments.Item(i)
            name = str(getattr(att, "FileName", "") or "")
            if not name.lower().endswith(".pdf"):
                continue
            n_ok = number_digits in N(name)
            t_ok = N(tipo_norm).split()[0] in N(name) if tipo_norm else False
            if n_ok and t_ok:
                found.append((name, att))
        if not found:
            for i in range(1, cnt + 1):
                att = mail.Attachments.Item(i)
                name = str(getattr(att, "FileName", "") or "")
                if name.lower().endswith(".pdf") and number_digits in N(name):
                    found.append((name, att))
    except Exception as e:
        logging.error(f"Erro ao ler anexos: {e}")
    return found

def find_pdf_links_in_body(html: str, number_digits: str) -> List[str]:
    links = []
    if not html:
        return links
    soup = BeautifulSoup(html, "lxml")
    for a in soup.find_all("a"):
        href = a.get("href") or ""
        if href.lower().endswith(".pdf") and number_digits in N(href):
            links.append(href)
    return links

def download_link(url: str, dest: pathlib.Path) -> bool:
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return False
        dest.write_bytes(r.content)
        return True
    except Exception as e:
        logging.warning(f"Falha download {url}: {e}")
        return False

def build_output_name(orgao: str, tipo: str, numero: str, data_ato: Optional[dt.date], fallback: str) -> str:
    d = data_ato.strftime("%Y-%m-%d") if isinstance(data_ato, dt.date) else (fallback or dt.date.today().isoformat())
    base = f"{d}_{orgao}_{tipo}_{only_digits(numero)}.pdf"
    return sanitize_filename(base)

def already_logged(state: Dict, entry_id: str, number_digits: str) -> bool:
    keys = state.setdefault("downloaded", [])
    key = f"{entry_id}:{number_digits}"
    return key in keys

def mark_logged(state: Dict, entry_id: str, number_digits: str):
    keys = state.setdefault("downloaded", [])
    key = f"{entry_id}:{number_digits}"
    if key not in keys:
        keys.append(key)

# =========================
# OUTLOOK: abrir pasta por caminho
# =========================
def open_outlook_folder_by_path(path_str: str):
    """Abre uma pasta do Outlook a partir de um caminho com '\'.
    Suporta: 'Inbox\Subpasta', 'Caixa de Entrada\Subpasta',
             ou 'Mailbox\Inbox\Subpasta' (quando há múltiplos stores)."""
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    parts = [p for p in path_str.split("\\") if p.strip()]
    if not parts:
        return ns.GetDefaultFolder(6)  # Inbox

    # Verifica se o primeiro segmento é um mailbox (store)
    root = None
    first = parts[0]
    for i in range(1, ns.Folders.Count + 1):
        store = ns.Folders.Item(i)
        if str(store.Name).strip().lower() == first.strip().lower():
            root = store
            parts = parts[1:]
            break

    if root is None:
        # começa na Inbox padrão
        root = ns.GetDefaultFolder(6)
        # se o primeiro segmento for "Inbox" ou "Caixa de Entrada", consome
        if first.strip().lower() in ["inbox", "caixa de entrada"]:
            parts = parts[1:]

    folder = root
    for p in parts:
        folder = folder.Folders.Item(p)
    return folder

def open_inbox():
    logging.info(f"Abrindo pasta do Outlook: {OUTLOOK_FOLDER_PATH}")
    return open_outlook_folder_by_path(OUTLOOK_FOLDER_PATH)

def iter_candidate_messages(folder, subjects: List[str]) -> List:
    items = folder.Items
    items.Sort("[ReceivedTime]", True)
    norm_subjects = {N(s) for s in subjects}
    logging.debug(f"Assuntos esperados: {norm_subjects}")
    out = []
    for mail in items:
        try:
            subj = str(getattr(mail, "Subject", ""))
            from_addr = str(getattr(mail, "SenderEmailAddress", "") or "")
            norm_subj = N(subj)
            norm_from = N(from_addr)
            if norm_from.endswith(N(SENDER_EMAIL)) and any(ns in norm_subj for ns in norm_subjects):
                out.append(mail)
            rec = getattr(mail, "ReceivedTime", None)
            if isinstance(rec, dt.datetime) and (dt.datetime.now() - rec).days > 15:
                break
        except Exception as e:
            logging.debug(f"Ignorando item não-mensagem: {e}")
            continue
    return out

# =========================
# PIPELINE
# =========================
def main():
    logging.info("="*20 + " INICIANDO EXECUÇÃO " + "="*20)
    ensure_dir(SAVE_DIR)
    ensure_dir(LOG_CSV.parent)

    # 1) metas
    logging.info(f"1. Carregando metas da planilha: {EXCEL_METAS}")
    try:
        metas = load_targets_from_excel(EXCEL_METAS)
    except Exception as e:
        logging.error(f"Falha ao ler planilha: {e}")
        return
    if metas.empty:
        logging.info("Planilha de metas vazia após filtros.")
        return

    # 2) localizar mensagens
    folder = open_inbox()
    subjects = build_expected_subjects()
    mails = iter_candidate_messages(folder, subjects)
    logging.info(f"E-mails candidatos na pasta '{OUTLOOK_FOLDER_PATH}': {len(mails)}")
    if not mails:
        logging.warning("Nenhum e-mail candidato encontrado (verifique data de teste/assunto/pasta).")

    state = load_state()
    log_rows: List[Dict] = []

    # 3) processar e-mails
    for mail in mails:
        try:
            entry_id = str(getattr(mail, "EntryID", ""))
            subj = str(getattr(mail, "Subject", ""))
            recd = getattr(mail, "ReceivedTime", None)
            recd_iso = recd.strftime("%Y-%m-%d %H:%M:%S") if isinstance(recd, dt.datetime) else ""
            html = str(getattr(mail, "HTMLBody", "") or "")
            plain = str(getattr(mail, "Body", "") or "")

            email_rows = rows_from_message_body(html, plain)
            logging.info(f"\n--- Processando e-mail: '{subj}' (Recebido: {recd_iso}) ---")
            if not email_rows:
                logging.warning("Nenhuma tabela de normativos encontrada no corpo do e-mail.")
                continue
            logging.debug(f"Linhas na tabela do e-mail: {len(email_rows)}")

            for _, tgt in metas.iterrows():
                try:
                    number_digits = str(tgt["N_NUM"])
                    if already_logged(state, entry_id, number_digits):
                        continue

                    matched_row = None
                    for r in email_rows:
                        if target_matches_email_row(tgt, r):
                            matched_row = r
                            break
                    if not matched_row:
                        continue

                    logging.info(f"  [MATCH] {tgt['ORGAO']} | {tgt['TIPO']} | {tgt['NUMERO']}")

                    saved_paths = []
                    # anexos
                    attachments = find_pdf_attachments(mail, number_digits, str(tgt["TIPO"]))
                    if attachments:
                        logging.debug(f"    Anexos candidatos: {len(attachments)}")
                        for name, att in attachments:
                            outname = build_output_name(
                                orgao=str(tgt["ORGAO"]),
                                tipo=str(tgt["TIPO"]),
                                numero=str(tgt["NUMERO"]),
                                data_ato=tgt["DATA_ATO_DT"],
                                fallback=recd.date().isoformat() if isinstance(recd, dt.datetime) else None
                            )
                            dest = SAVE_DIR / outname
                            k = 1
                            while dest.exists():
                                dest = SAVE_DIR / sanitize_filename(outname.replace(".pdf", f"_{k}.pdf"))
                                k += 1
                            att.SaveAsFile(str(dest))
                            logging.info(f"    -> Anexo salvo: {dest}")
                            saved_paths.append(str(dest))

                    # links no corpo
                    if not saved_paths:
                        links = find_pdf_links_in_body(html, number_digits)
                        logging.debug(f"    Links PDF: {len(links)}")
                        for url in links:
                            outname = build_output_name(
                                orgao=str(tgt["ORGAO"]),
                                tipo=str(tgt["TIPO"]),
                                numero=str(tgt["NUMERO"]),
                                data_ato=tgt["DATA_ATO_DT"],
                                fallback=recd.date().isoformat() if isinstance(recd, dt.datetime) else None
                            )
                            dest = SAVE_DIR / outname
                            k = 1
                            while dest.exists():
                                dest = SAVE_DIR / sanitize_filename(outname.replace(".pdf", f"_{k}.pdf"))
                                k += 1
                            if download_link(url, dest):
                                logging.info(f"    -> Link salvo: {dest}")
                                saved_paths.append(str(dest))

                    status = "BAIXADO" if saved_paths else "NAO_ENCONTRADO_PDF"
                    if saved_paths:
                        mark_logged(state, entry_id, number_digits)

                    log_rows.append({
                        "email_entry_id": entry_id,
                        "email_subject": subj,
                        "email_received": recd_iso,
                        "meta_orgao": str(tgt["ORGAO"]),
                        "meta_tipo": str(tgt["TIPO"]),
                        "meta_numero": str(tgt["NUMERO"]),
                        "meta_data_ato": str(tgt["DATA_ATO"]),
                        "match_row_orgao": matched_row.get("ORGAO") if matched_row else None,
                        "match_row_tipo": matched_row.get("TIPO") if matched_row else None,
                        "match_row_numero": matched_row.get("NUMERO") if matched_row else None,
                        "match_row_data_ato": matched_row.get("DATA_ATO") if matched_row else None,
                        "arquivos_salvos": ";".join(saved_paths) if saved_paths else "",
                        "status": status,
                    })
                except Exception as e_row:
                    logging.error(f"Erro processando meta '{tgt}': {e_row}")
                    continue
        except Exception as e_mail:
            logging.error(f"Erro processando e-mail: {e_mail}")
            continue

    # 4) persistir logs/estado
    logging.info("\n--- Finalizando e salvando resultados ---")
    if log_rows:
        ensure_dir(LOG_CSV.parent)
        df_log = pd.DataFrame(log_rows)

        header = not LOG_CSV.exists()
        df_log.to_csv(LOG_CSV, mode='a', header=header, index=False, encoding="utf-8-sig")

        try:
            if LOG_XLSX.exists():
                df_existente = pd.read_excel(LOG_XLSX)
                df_final_excel = pd.concat([df_existente, df_log], ignore_index=True)
            else:
                df_final_excel = df_log
            df_final_excel.drop_duplicates(
                subset=["email_entry_id","meta_orgao","meta_tipo","meta_numero"],
                keep='last', inplace=True
            )
            df_final_excel.to_excel(LOG_XLSX, index=False)
        except Exception as e:
            logging.error(f"Falha ao escrever o log em Excel ({LOG_XLSX}): {e}")

        logging.info(f"-> Log salvo em {LOG_CSV} e {LOG_XLSX} ({len(log_rows)} novas entradas).")
    else:
        logging.info("-> Nenhuma nova meta foi encontrada/baixada.")

    gc.collect()
    save_state(state)
    logging.info("="*20 + " EXECUÇÃO CONCLUÍDA " + "="*20)

# =========================
# EXECUÇÃO
# =========================
if __name__ == "__main__":
    main()
