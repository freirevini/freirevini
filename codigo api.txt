# -*- coding: utf-8 -*-
import os
import re
import json
import logging
import pathlib
import datetime as dt
from typing import List, Dict, Optional, Tuple

import win32com.client as win32
import pandas as pd
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparser

# =========================
# CONFIG
# =========================
# TESTE – ajuste o intervalo:
DATA_INICIO = "01/03/2025"
DATA_FIM    = "07/03/2025"
# PRODUÇÃO (últimos 3 dias):
# _hoje = dt.datetime.today()
# DATA_INICIO, DATA_FIM = (_hoje - dt.timedelta(days=3)).strftime("%d/%m/%Y"), _hoje.strftime("%d/%m/%Y")

REMETENTE_FILTRO = "informe@mkcompliance.com.br"
ASSUNTO_FIXO = "[EXT] - Normativos Divulgados em "
OUTLOOK_FOLDER_PATH = r"Caixa de Entrada\Normas MK Compliance"

PASTA_DESTINO = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Normativo_Download_Outlook"
ARQUIVO_EXCEL_EMAILS = os.path.join(PASTA_DESTINO, "Normativos_Email_Consolidado.xlsx")
ARQUIVO_EXCEL_METAS  = r"N:\DCGV\Compliance\Compliance RO Varejo\Analytics\Projetos\AgendaNormativa\Melhorias_Agenda\3_BaseMK\BaseMkNormasParaAvaliar.xlsx"
STATE_JSON           = os.path.join(PASTA_DESTINO, "state.json")
LOG_CSV              = os.path.join(PASTA_DESTINO, "log_execucao.csv")

HTTP_TIMEOUT = 45
LOG_LEVEL = logging.INFO
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")

# =========================
# HELPERS
# =========================
def ensure_dir(path: str | pathlib.Path) -> None:
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)

def N(txt: str) -> str:
    import unicodedata
    txt = str(txt or "")
    txt = "".join(c for c in unicodedata.normalize("NFKD", txt) if not unicodedata.combining(c))
    return re.sub(r"\s+", " ", txt.upper().strip())

def only_digits(s: str) -> str:
    return re.sub(r"\D", "", str(s or ""))

def parse_date_flexible(s: str) -> Optional[dt.date]:
    if not s:
        return None
    s = str(s).strip()
    try:
        return dtparser.parse(s, dayfirst=True).date()
    except Exception:
        pass
    m = re.search(r"(\d{1,2})\s+de\s+([a-zç\.]+)\s+de\s+(\d{4})", s, flags=re.IGNORECASE)
    if m:
        d = int(m.group(1))
        mes_raw = m.group(2).replace(".", "").lower()
        mapa = {
            "jan":1,"janeiro":1,"fev":2,"fevereiro":2,"mar":3,"marco":3,"março":3,"abr":4,"abril":4,
            "mai":5,"maio":5,"jun":6,"junho":6,"jul":7,"julho":7,"ago":8,"agosto":8,"set":9,"setembro":9,
            "out":10,"outubro":10,"nov":11,"novembro":11,"dez":12,"dezembro":12
        }
        mes = mapa.get(mes_raw, 0)
        y = int(m.group(3))
        if 1 <= int(mes) <= 12:
            return dt.date(y, int(mes), d)
    return None

def sanitize_filename(s: str) -> str:
    s = re.sub(r"[^\w\-. ]", "_", s, flags=re.UNICODE)
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"_+", "_", s)
    return s.strip("._")[:180]

def build_output_name_orgao_tipo_num(orgao: str, tipo: str, numero: str) -> str:
    base = f"{orgao}_{tipo}_{only_digits(numero)}"
    return sanitize_filename(base) + ".pdf"

def load_state() -> Dict:
    try:
        if os.path.exists(STATE_JSON):
            with open(STATE_JSON, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {}

def save_state(state: Dict) -> None:
    ensure_dir(os.path.dirname(STATE_JSON))
    with open(STATE_JSON, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def state_has_triplet(state: Dict, orgao: str, tipo: str, nnum: str) -> bool:
    key = f"{N(orgao)}|{N(tipo)}|{nnum}"
    return key in state.setdefault("downloaded_triplets", [])

def state_add_triplet(state: Dict, orgao: str, tipo: str, nnum: str) -> None:
    key = f"{N(orgao)}|{N(tipo)}|{nnum}"
    arr = state.setdefault("downloaded_triplets", [])
    if key not in arr:
        arr.append(key)

def file_already_present(dest_dir: str, base_filename: str) -> Optional[str]:
    base_stem = os.path.splitext(base_filename)[0]
    for f in os.listdir(dest_dir):
        if f.lower().endswith(".pdf") and f.lower().startswith(base_stem.lower()):
            return os.path.join(dest_dir, f)
    return None

# =========================
# OUTLOOK
# =========================
def open_outlook_folder_by_path(path_str: str):
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    parts = [p for p in path_str.split("\\") if p.strip()]
    if not parts:
        return ns.GetDefaultFolder(6)  # Inbox

    root = None
    first = parts[0]
    for i in range(1, ns.Folders.Count + 1):
        store = ns.Folders.Item(i)
        if str(store.Name).strip().lower() == first.strip().lower():
            root = store
            parts = parts[1:]
            break

    if root is None:
        root = ns.GetDefaultFolder(6)
        if first.strip().lower() in ["inbox", "caixa de entrada"]:
            parts = parts[1:]

    folder = root
    for p in parts:
        folder = folder.Folders.Item(p)
    return folder

def restrict_by_date_and_subject(items, data_inicio_dt: dt.datetime, data_fim_dt: dt.datetime) -> list:
    di = data_inicio_dt.strftime("%m/%d/%Y")
    df = data_fim_dt.strftime("%m/%d/%Y")
    filtro_data = f"[ReceivedTime] >= '{di} 00:00 AM' AND [ReceivedTime] <= '{df} 11:59 PM'"
    candidates = list(items.Restrict(filtro_data))

    out = []
    for mail in candidates:
        try:
            subj = str(getattr(mail, "Subject", "") or "")
            from_addr = str(getattr(mail, "SenderEmailAddress", "") or "")
            if REMETENTE_FILTRO.lower() not in from_addr.lower():
                continue
            if not subj.startswith(ASSUNTO_FIXO):
                continue
            data_str = subj.replace(ASSUNTO_FIXO, "").strip()
            try:
                data_no_assunto = dt.datetime.strptime(data_str, "%d/%m/%Y")
            except Exception:
                continue
            if data_inicio_dt <= data_no_assunto <= data_fim_dt:
                out.append(mail)
        except Exception:
            continue
    return out

# =========================
# EMAIL HTML -> DATAFRAME CANÔNICO
# =========================
COLS_CANON = ["Tipo", "Número", "Data do Ato", "Órgão", "Ementa", "Vigência"]

HEADER_VARIANTS = {
    "Órgão": {N("Órgão"), N("Órgão regulador"), N("Orgao"), N("Orgao Regulador"), N("Orgão Regulador"), N("Órgão Responsável"), N("Orgão Responsável")},
    "Tipo": {N("Tipo"), N("Tipo da Norma"), N("Espécie"), N("Especie")},
    "Número": {N("Número"), N("Numero"), N("Nº"), N("No"), N("N."), N("N")},
    "Data do Ato": {N("Data do Ato"), N("Data do Normativo"), N("Data"), N("Publicação"), N("Data Publicação")},
    "Ementa": {N("Ementa"), N("Assunto"), N("Resumo"), N("Descrição"), N("Descricao")},
    "Vigência": {N("Vigência"), N("Vigencia")},
}

FORWARD_MARKERS = re.compile(
    r"(mensagem encaminhada|encaminhada|enc:|fw:|forwarded message|-----+ original message -----+)",
    re.I
)

def _row_cells(tr) -> list[str]:
    tds = tr.find_all(["td", "th"])
    return [td.get_text(" ", strip=True) for td in tds]

def _header_mapping(cells_norm: list[str]) -> Dict[int, str]:
    idx_to_key = {}
    for i, txt in enumerate(cells_norm):
        for canon, variants in HEADER_VARIANTS.items():
            if txt in variants and canon not in idx_to_key.values():
                idx_to_key[i] = canon
                break
    return idx_to_key

def _scan_table_for_header(table) -> Optional[Tuple[Dict[int, str], int, list]]:
    trs = table.find_all("tr")
    if not trs:
        return None
    best = None
    for idx, tr in enumerate(trs):
        cells = _row_cells(tr)
        if not cells:
            continue
        cells_norm = [N(c) for c in cells]
        mapping = _header_mapping(cells_norm)
        keys = set(mapping.values())
        if ("Número" in keys) and (("Tipo" in keys) or ("Órgão" in keys)) and len(keys) >= 4:
            score = (len(keys),)
            if (best is None) or (score > best[0]):
                best = (score, mapping, idx, trs)
    if best is None:
        return None
    _, mapping, idx, trs = best
    return mapping, idx, trs

def extract_df_from_email_html(html: str) -> pd.DataFrame:
    if not html:
        return pd.DataFrame(columns=COLS_CANON)

    cut_html = html
    m = FORWARD_MARKERS.search(html)
    if m:
        cut_html = html[:m.start()]

    soup = BeautifulSoup(cut_html, "lxml")
    candidates: list[tuple[Tuple[int, int], pd.DataFrame]] = []

    for table in soup.find_all("table"):
        header_scan = _scan_table_for_header(table)
        if not header_scan:
            continue
        idx_to_key, idx_header, trs = header_scan

        records = []
        for tr in trs[idx_header+1:]:
            cells = _row_cells(tr)
            if not cells:
                continue
            row = {k: "" for k in COLS_CANON}
            for i, canon in idx_to_key.items():
                if i < len(cells):
                    row[canon] = cells[i]
            if only_digits(row.get("Número", "")):
                records.append(row)

        if not records:
            continue

        df_tmp = pd.DataFrame.from_records(records, columns=COLS_CANON)
        for c in COLS_CANON:
            df_tmp[c] = df_tmp[c].fillna("").astype(str).str.replace("\xa0", " ", regex=False).str.strip()

        mask_headerish = df_tmp["Número"].map(lambda x: N(x) in HEADER_VARIANTS["Número"])
        df_tmp = df_tmp[~mask_headerish].copy()

        df_tmp["__N_NUM__"] = df_tmp["Número"].map(only_digits)
        df_tmp = df_tmp.drop_duplicates(subset=["__N_NUM__"], keep="first").drop(columns=["__N_NUM__"])

        useful_cols = sum(df_tmp[c].notna().any() for c in ["Tipo","Número","Data do Ato","Órgão"])
        score = (useful_cols, len(df_tmp))
        candidates.append((score, df_tmp))

    if not candidates:
        return pd.DataFrame(columns=COLS_CANON)

    candidates.sort(key=lambda t: t[0], reverse=True)
    best_df = candidates[0][1].copy()
    return best_df[COLS_CANON].reset_index(drop=True)

def add_normalized_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out["N_TIPO"] = out["Tipo"].map(N)
    out["N_NUM"] = out["Número"].map(only_digits)
    out["N_ORGAO"] = out["Órgão"].map(N)
    out["DATA_ATO_DT"] = out["Data do Ato"].map(parse_date_flexible)
    return out

# =========================
# METAS (EXCEL)
# =========================
def load_metas_df(path: str) -> pd.DataFrame:
    df = pd.read_excel(path)
    colmap = {N(c): c for c in df.columns}
    def find(*cands):
        for c in cands:
            if N(c) in colmap:
                return colmap[N(c)]
        return None

    c_org = find("Órgão regulador","Órgão","Orgao","Orgão Regulador","Orgao Regulador")
    c_tipo= find("Tipo","Tipo da Norma","Espécie","Especie")
    c_num = find("Número","Numero","Nº","No","N.","N")
    c_data= find("Data do Normativo","Data do Ato","Data","Publicação","Data Publicação")

    miss = [n for n,v in [("Órgão",c_org),("Tipo",c_tipo),("Número",c_num),("Data",c_data)] if v is None]
    if miss:
        raise RuntimeError(f"Colunas ausentes na planilha de metas: {miss}")

    metas = pd.DataFrame({
        "Órgão": df[c_org].astype(str),
        "Tipo":  df[c_tipo].astype(str),
        "Número":df[c_num].astype(str),
        "Data do Ato": df[c_data].astype(str),
    })
    metas = add_normalized_cols(metas)
    return metas

# =========================
# MATCHING
# =========================
def row_matches_target(email_row: pd.Series, meta_row: pd.Series) -> bool:
    if str(email_row["N_NUM"]) != str(meta_row["N_NUM"]):
        return False
    e_org = email_row["N_ORGAO"]; t_org = meta_row["N_ORGAO"]
    e_tipo= email_row["N_TIPO"];  t_tipo= meta_row["N_TIPO"]
    org_ok = any(alias in e_org for alias in [t_org, "BANCO CENTRAL", "BANCO CENTRAL DO BRASIL", "BCB", "BACEN", "CVM", "ANBIMA", "CMN"])
    if not org_ok:
        return False
    tipo_ok = (t_tipo in e_tipo) or (e_tipo in t_tipo) or (t_tipo.split()[0] in e_tipo if t_tipo else False)
    return tipo_ok

# =========================
# PDF DOWNLOAD (anexos escolhidos com base nas METAS)
# =========================
ALIASES_ORGAO = {
    "BANCO CENTRAL DO BRASIL": ["BANCO CENTRAL", "BCB", "BACEN"],
    "CVM": ["CVM"],
    "ANBIMA": ["ANBIMA"],
    "CMN": ["CMN", "CONSELHO MONETÁRIO NACIONAL", "CONSELHO MONETARIO NACIONAL"]
}

def score_attachment_for_meta(filename: str, orgao: str, tipo: str, nnum: str) -> int:
    """Score alto = melhor candidato para a meta."""
    nameN = N(filename)
    score = 0
    if nnum in nameN:
        score += 100
    first_tipo = N(tipo).split()[0] if tipo else ""
    if first_tipo and first_tipo in nameN:
        score += 20
    # aliases do órgão
    org_key = N(orgao)
    aliases = set([org_key])
    for k, arr in ALIASES_ORGAO.items():
        if N(k) in org_key:
            aliases.update([N(a) for a in arr])
            break
    for a in aliases:
        if a and a in nameN:
            score += 10
    # bônus se bater com prefixo esperado:
    prefix = N(os.path.splitext(build_output_name_orgao_tipo_num(orgao, tipo, nnum))[0])
    if prefix.split("_")[-1] in nameN:  # pelo menos o número final
        score += 5
    return score

def find_all_pdf_attachments(mail) -> List[Tuple[str, object]]:
    out = []
    try:
        cnt = int(getattr(mail.Attachments, "Count", 0))
        for i in range(1, cnt + 1):
            att = mail.Attachments.Item(i)
            name = str(getattr(att, "FileName", "") or "")
            if name.lower().endswith(".pdf"):
                out.append((name, att))
    except Exception as e:
        logging.warning(f"Erro ao ler anexos: {e}")
    return out

def find_pdf_links_in_html(html: str, number_digits: str) -> List[str]:
    links = []
    if not html:
        return links
    soup = BeautifulSoup(html, "lxml")
    for a in soup.find_all("a"):
        href = (a.get("href") or "").strip()
        if href.lower().endswith(".pdf") and number_digits in N(href):
            links.append(href)
    return links

def download_link(url: str, dest_path: str) -> bool:
    try:
        r = requests.get(url, timeout=HTTP_TIMEOUT, allow_redirects=True)
        if r.status_code >= 400:
            return False
        with open(dest_path, "wb") as f:
            f.write(r.content)
        return True
    except Exception as e:
        logging.warning(f"Falha no download {url}: {e}")
        return False

# =========================
# PIPELINE
# =========================
def main():
    ensure_dir(PASTA_DESTINO)
    state = load_state()

    # 1) Ler e-mails e consolidar tabela (gera Excel de auditoria)
    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")
    folder = open_outlook_folder_by_path(OUTLOOK_FOLDER_PATH)
    items = folder.Items
    items.Sort("[ReceivedTime]", True)

    di = dt.datetime.strptime(DATA_INICIO, "%d/%m/%Y")
    df_ = dt.datetime.strptime(DATA_FIM, "%d/%m/%Y")
    mails = restrict_by_date_and_subject(items, di, df_)
    logging.info(f"E-mails elegíveis: {len(mails)}")

    email_dfs: List[pd.DataFrame] = []
    mail_index: Dict[str, object] = {}

    for mail in mails:
        entry_id = str(getattr(mail, "EntryID", ""))
        subj = str(getattr(mail, "Subject", "") or "")
        recd = getattr(mail, "ReceivedTime", None)
        html = str(getattr(mail, "HTMLBody", "") or "")

        df_email = extract_df_from_email_html(html)
        if df_email.empty:
            logging.info(f"[SKIP] Sem tabela reconhecida em: {subj}")
            continue

        df_email = add_normalized_cols(df_email)
        df_email = df_email.drop_duplicates(subset=["N_NUM", "N_TIPO", "N_ORGAO"], keep="first").reset_index(drop=True)

        df_email["EMAIL_ENTRY_ID"] = entry_id
        df_email["EMAIL_SUBJECT"] = subj
        df_email["EMAIL_RECEIVED"] = recd.strftime("%Y-%m-%d %H:%M:%S") if isinstance(recd, dt.datetime) else ""

        email_dfs.append(df_email)
        mail_index[entry_id] = mail

    if not email_dfs:
        logging.warning("Nenhuma tabela de e-mail extraída. Encerrando.")
        return

    emails_df_inmem = pd.concat(email_dfs, ignore_index=True)
    try:
        emails_df_inmem.to_excel(ARQUIVO_EXCEL_EMAILS, index=False)
        logging.info(f"Consolidado salvo em: {ARQUIVO_EXCEL_EMAILS}")
    except Exception as e:
        logging.warning(f"Falha ao salvar Excel consolidado: {e}")

    # 2) Reabre a planilha do e-mail (fonte para JOIN)
    emails_df = pd.read_excel(ARQUIVO_EXCEL_EMAILS, dtype=str).fillna("")
    emails_df["N_TIPO"]   = emails_df["Tipo"].map(N)
    emails_df["N_NUM"]    = emails_df["Número"].map(only_digits)
    emails_df["N_ORGAO"]  = emails_df["Órgão"].map(N)

    # 3) Metas (fonte de verdade para confronto de anexos)
    metas_df = load_metas_df(ARQUIVO_EXCEL_METAS)
    logging.info(f"Metas carregadas: {len(metas_df)}")

    # 4) JOIN por N_NUM
    base_join = emails_df.merge(
        metas_df,
        how="inner",
        left_on="N_NUM",
        right_on="N_NUM",
        suffixes=("_EMAIL", "_META")
    )
    if base_join.empty:
        logging.info("Nenhum número coincidente entre e-mails e metas.")
        return

    # 5) validação órgão/tipo
    ok_mask = []
    for _, row in base_join.iterrows():
        ok_mask.append(
            row_matches_target(
                email_row=row[["N_NUM","N_ORGAO_EMAIL","N_TIPO_EMAIL"]].rename(
                    {"N_ORGAO_EMAIL":"N_ORGAO","N_TIPO_EMAIL":"N_TIPO"}
                ),
                meta_row=row[["N_NUM","N_ORGAO_META","N_TIPO_META"]].rename(
                    {"N_ORGAO_META":"N_ORGAO","N_TIPO_META":"N_TIPO"}
                )
            )
        )
    matched = base_join[ok_mask].copy()
    if matched.empty:
        logging.info("Nenhum match validado por órgão/tipo.")
        return

    logging.info(f"Matches validados: {len(matched)}")

    # 6) PARA CADA E-MAIL, ESCOLHER ANEXOS USANDO *APENAS* AS LINHAS DA PLANILHA DE METAS
    #    (que foram validadas no match)
    saved_mail_num = set()
    results = []

    for entry_id, grp in matched.groupby("EMAIL_ENTRY_ID"):
        mail = mail_index.get(entry_id)
        if mail is None:
            continue

        # anexa uma única vez a lista completa de anexos .pdf do e-mail
        all_atts = find_all_pdf_attachments(mail)

        for _, r in grp.iterrows():
            nnum   = str(r["N_NUM"])
            orgao  = str(r.get("Órgão_META") or "")
            tipo   = str(r.get("Tipo_META")  or "")
            numero_vis = str(r.get("Número_META") or nnum)

            # trava por (email, numero) no mesmo run
            key_mail_num = (entry_id, nnum)
            if key_mail_num in saved_mail_num:
                continue

            # persistência e filesystem
            base_name = build_output_name_orgao_tipo_num(orgao, tipo, numero_vis)
            fs_hit = file_already_present(PASTA_DESTINO, base_name)
            if state_has_triplet(state, orgao, tipo, nnum) or fs_hit:
                saved_mail_num.add(key_mail_num)
                results.append({
                    "email_entry_id": entry_id,
                    "email_subject": r["EMAIL_SUBJECT"],
                    "email_received": r.get("EMAIL_RECEIVED", ""),
                    "orgao": orgao, "tipo": tipo, "numero": nnum,
                    "arquivos_salvos": fs_hit or "",
                    "status": "JA_EXISTIA"
                })
                continue

            # escolhe o MELHOR anexo por score (usando *metas* como referência)
            scored = []
            for name, att in all_atts:
                if nnum not in N(name):  # número é requisito
                    continue
                scored.append((score_attachment_for_meta(name, orgao, tipo, nnum), name, att))
            scored.sort(reverse=True)

            saved_paths = []
            if scored:
                _, name, att = scored[0]  # pega só o melhor
                dest = os.path.join(PASTA_DESTINO, base_name)
                k = 1
                while os.path.exists(dest):
                    dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                    k += 1
                att.SaveAsFile(dest)
                saved_paths.append(dest)

            # fallback: link com o número
            if not saved_paths:
                html = str(getattr(mail, "HTMLBody", "") or "")
                for url in find_pdf_links_in_html(html, nnum):
                    dest = os.path.join(PASTA_DESTINO, base_name)
                    k = 1
                    while os.path.exists(dest):
                        dest = os.path.join(PASTA_DESTINO, sanitize_filename(os.path.splitext(base_name)[0] + f"_{k}.pdf"))
                        k += 1
                    if download_link(url, dest):
                        saved_paths.append(dest)
                        break  # só um arquivo

            saved_mail_num.add(key_mail_num)
            if saved_paths:
                state_add_triplet(state, orgao, tipo, nnum)

            results.append({
                "email_entry_id": entry_id,
                "email_subject": r["EMAIL_SUBJECT"],
                "email_received": r.get("EMAIL_RECEIVED", ""),
                "orgao": orgao, "tipo": tipo, "numero": nnum,
                "arquivos_salvos": ";".join(saved_paths),
                "status": "BAIXADO" if saved_paths else "NAO_ENCONTRADO_PDF"
            })

    # 7) Log + persistência
    try:
        df_log = pd.DataFrame(results)
        if os.path.exists(LOG_CSV):
            old = pd.read_csv(LOG_CSV, encoding="utf-8-sig")
            df_log = pd.concat([old, df_log], ignore_index=True)
        if not df_log.empty:
            df_log.drop_duplicates(subset=["orgao","tipo","numero","arquivos_salvos"], keep="last", inplace=True)
        df_log.to_csv(LOG_CSV, index=False, encoding="utf-8-sig")
        logging.info(f"Log salvo em: {LOG_CSV}")
    except Exception as e:
        logging.warning(f"Falha ao salvar log: {e}")

    save_state(state)
    logging.info("Processo concluído.")

# =========================
# RUN
# =========================
if __name__ == "__main__":
    main()
